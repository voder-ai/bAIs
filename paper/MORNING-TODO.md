## Morning Review TODO (2026-02-19)

1. Add Discussion paragraph citing arxiv 2501.18190 (specialization-bias paper)

**Draft paragraph:**
> Sibony-style disclosure assumes decision-makers can recognize and discount arbitrary anchors when warned. This transfers imperfectly to LLMs: RLHF-trained models (Anthropic, Hermes) show strong positive response (+35-97.5%), compliance-optimized models (GPT-4o, o3-mini) show null effect, and reasoning models (o1, GPT-5.2) show *inverse* effects (-14% to -28%). This parallels findings that specialized AI personas degrade economic rationality despite appearing more rigorous [2501.18190], suggesting intuitive 'rigor' interventions may introduce systematic errors in LLMs.

2. Update abstract to mention disclosure finding?
3. Final PDF review

---
Session: 2026-02-19 12:00-14:00 UTC
Commits: 689f6ab, 97e5bb9, 8f1f264
