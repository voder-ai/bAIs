# Related Work

## Anchoring Bias in Human Judgment

Anchoring bias—the disproportionate influence of initial information on subsequent estimates—is among the most robust findings in cognitive psychology (Tversky & Kahneman, 1974). Even experts are susceptible: Englich et al. (2006) demonstrated that experienced judges' sentencing decisions were influenced by random numbers generated by dice rolls. The effect persists even when participants are explicitly told the anchor is arbitrary.

## Cognitive Biases in LLMs

Recent work has shown that LLMs exhibit human-like cognitive biases (Binz & Schulz, 2023; Jones & Steinhardt, 2022). Anchoring effects have been documented across multiple model families (Huang et al., 2025), with susceptibility varying by model architecture and size. Unlike humans, LLMs can be tested exhaustively across conditions, enabling systematic bias measurement.

## Debiasing Techniques

Several techniques have been proposed for mitigating anchoring in LLMs:

**Outside View / Reference Class Forecasting:** Prompting models to consider what typically happens in similar cases (Sibony et al., 2016). Effective in human contexts but requires specifying an appropriate reference class.

**Self-Administered Cognitive Debiasing (SACD):** Iterative prompting that guides models through bias detection and correction (Lyu et al., 2025). Shows promise but is computationally expensive.

**Devil's Advocate:** Prompting models to argue against their initial response. Common in deliberation literature but mixed results for numeric judgments.

**Premortem Analysis:** Asking models to imagine the decision failed and explain why. Drawn from project management practice (Klein, 2007).

## Evaluation Methodology

Standard anchoring evaluation compares high-anchor and low-anchor conditions (Englich et al., 2006; Huang et al., 2025). A technique "works" if it reduces the gap between conditions. This methodology does not require ground truth—it measures susceptibility to anchors, not accuracy of outputs.

We extend this by introducing calibration to unanchored baselines. This requires collecting baseline responses but enables detection of overcorrection—a failure mode invisible to susceptibility-only evaluation.
