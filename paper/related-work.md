# Related Work

## Anchoring Bias in Human Judgment

Anchoring bias—the disproportionate influence of initial information on subsequent estimates—is among the most robust findings in cognitive psychology (Tversky & Kahneman, 1974). Even experts are susceptible: Englich et al. (2006) demonstrated that experienced judges' sentencing decisions were influenced by random numbers generated by dice rolls. Effect sizes of d = 0.6–1.2 persist regardless of anchor source or participant awareness. Our experimental paradigm adapts this judicial sentencing design.

## Cognitive Biases in LLMs

Recent work has shown that LLMs exhibit human-like cognitive biases (Binz & Schulz, 2023; Jones & Steinhardt, 2022). Anchoring effects have been documented across multiple model families (Huang et al., 2025), with susceptibility varying by model architecture and size. Song et al. (2026) survey LLM reasoning failures comprehensively, including susceptibility to anchoring and framing effects. Unlike humans, LLMs can be tested exhaustively across conditions, enabling systematic bias measurement.

## Debiasing Techniques

Several techniques have been proposed for mitigating anchoring:

**Outside View / Reference Class Forecasting:** Prompting models to consider what typically happens in similar cases (Sibony et al., 2019). Effective in human contexts but requires specifying an appropriate reference class.

**Self-Administered Cognitive Debiasing (SACD):** Iterative prompting that guides models through bias detection and correction (Lyu et al., 2025). Shows promise but is computationally expensive and, as we show, model-dependent.

**Devil's Advocate:** Prompting models to argue against their initial response. Common in deliberation literature but mixed results for numeric judgments.

**Premortem Analysis:** Asking models to imagine the decision failed and explain why. Drawn from project management practice (Klein, 2007).

Other approaches include chain-of-thought prompting (Wei et al., 2022), explicit debiasing instructions, and response aggregation. However, these methods have shown inconsistent effectiveness across models—a finding our work helps explain.

## Evaluation Methodology

Standard anchoring evaluation compares high-anchor and low-anchor conditions (Englich et al., 2006; Huang et al., 2025):

$$\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|$$

A technique "works" if it reduces this gap. This methodology does not require ground truth—it measures susceptibility to anchors, not accuracy of outputs. This is a valid and important metric.

We extend this by introducing **calibration to unanchored baselines**:

$$\text{Calibration Error} = |R_{technique} - R_{baseline}|$$

This requires collecting baseline responses but enables detection of **overcorrection**—a failure mode invisible to susceptibility-only evaluation. To our knowledge, no prior work on LLM anchoring has systematically collected unanchored baselines for calibration evaluation. As our results demonstrate, susceptibility and calibration metrics can produce completely inverted technique rankings.
