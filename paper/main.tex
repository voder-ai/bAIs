\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Debiasing Anchoring Bias in LLM Judicial Reasoning:\\Baseline Convergence as a Metric}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
Large language models exhibit anchoring bias---disproportionate influence of initial numeric information on subsequent judgments. Debiasing techniques exist, but how should we evaluate them? Standard methodology compares responses under high vs.\ low anchor conditions; a technique ``works'' if it reduces this gap. We identify a critical limitation: this metric misses \textbf{overcorrection}, where techniques move responses away from anchors but past the unbiased answer.

We introduce \textbf{baseline convergence} as a complementary evaluation metric. By collecting baseline responses without explicit anchors (n=909 across 10 models), we measure whether techniques bring outputs closer to the model's considered judgment. This metric reveals \textit{overcorrection}---when techniques move responses away from anchors but past where the model would naturally respond. While there is no objective ``correct'' sentence, the baseline provides a meaningful reference for measuring technique effects. Using this metric across 13,799 trials, we find that technique effectiveness varies substantially:

\begin{itemize}
    \item \textbf{Full SACD} (Self-Administered Cognitive Debiasing; iterative self-reflection): +24\% improvement ($d=0.41$, $p<.001$)
    \item \textbf{Premortem}: +10\% improvement ($p<.001$, $d=0.17$)
    \item \textbf{Random Control}: +9\% improvement ($p<.001$, $d=0.15$)
    \item \textbf{Devil's Advocate}: +2\% (not significant, Bonferroni-corrected $p=1.0$)
\end{itemize}

\textit{(Our Outside View implementation produced confounded results and is discussed separately in Section 5.3.)}

Iterative self-reflection (Full SACD) is the most effective technique, but with high model variance: 5/10 models significantly improve, while Claude Opus 4.6 shows 68\% \textit{worse} convergence ($p<.001$). Devil's Advocate shows no significant effect (Bonferroni-corrected $p=1.0$).

Without baseline collection, overcorrection would be invisible under standard susceptibility metrics. We propose baseline convergence as a complementary metric for LLM debiasing research, particularly useful for detecting overcorrection.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

When large language models make judgments, do debiasing techniques actually help---or do they just move errors in a different direction?

We report findings from a large systematic evaluation of LLM debiasing techniques (13,799 trials across 10 models). Our core contribution is methodological: by collecting baseline responses without explicit anchors, we can measure not just whether techniques \textit{reduce susceptibility} to anchors, but whether they bring outputs \textit{closer to the model's considered judgment}.

\textbf{Study design note:} We analyze each model independently, measuring how debiasing techniques affect that model's responses at different temperatures. Anchors use \textit{constant relative scaling}: low anchor = baseline $\times$ 0.5, high anchor = baseline $\times$ 1.5. This ensures anchors are equally strong relative to each model's natural judgment---a fixed 3-month anchor would be a strong pull for a model with an 18-month baseline but negligible for one with a 36-month baseline. This design answers: ``How do techniques affect \textit{this} model?'' rather than ``Which model is least susceptible?''

This distinction matters. Standard anchoring studies compare high-anchor and low-anchor conditions---if the gap shrinks, the technique ``works.'' But this metric misses a critical failure mode: \textbf{overcorrection}. A technique that moves every response to 15 months, regardless of whether the unbiased answer is 30 months or 6 months, would show ``reduced susceptibility'' while actually \textit{increasing} distance from truth.

\subsection{The Baseline Convergence Metric}

We introduce a complementary evaluation metric: \textbf{baseline convergence}.

\begin{itemize}
    \item \textbf{Susceptibility} (standard): $|\bar{R}_{high} - \bar{R}_{low}|$
    \item \textbf{Convergence} (ours): $|R_{technique} - R_{baseline}|$
\end{itemize}

A technique succeeds on convergence if it brings the response \textit{closer} to what the model would say without any anchor present.

\subsection{Findings Preview}

Using this metric, we observe technique rankings with clear statistical separation:

\textbf{Convergence metric:} A hierarchy emerges:
\begin{enumerate}
    \item \textbf{Full SACD} (+24\%, $p<.001$, $d=0.41$)---iterative self-reflection
    \item \textbf{Premortem} (+10\%, $p<.001$)---imagine failure mode
    \item \textbf{Random Control} (+9\%, $p<.001$)---extra turns, no debiasing content
    \item \textbf{Devil's Advocate} (+2\%, Bonferroni $p=1.0$, not significant)---argumentation
\end{enumerate}

Simple structural interventions (extra turns) produced meaningful improvements with minimal prompt complexity. Our Outside View implementation showed worse convergence ($-22\%$), but this result is confounded and discussed separately in Section 5.3.

\subsection{Why This Matters}

This has immediate practical implications:

\begin{enumerate}
    \item \textbf{Practitioners don't need complex debiasing prompts.} Simply adding conversation turns helps more than specific debiasing instructions.
    
    \item \textbf{Reference class reasoning (Outside View) may introduce secondary anchors.} In our implementation, specifying jurisdiction to avoid model refusals may have anchored responses to that jurisdiction's typical sentences.
    
    \item \textbf{Baseline collection enables overcorrection detection.} Without baselines, techniques that overcorrect would appear effective under susceptibility metrics.
    
    \item \textbf{The standard evaluation metric would have misled us completely.} Direction-based analysis showed Outside View as universally effective; calibration analysis reveals it as worst.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A baseline convergence metric for debiasing evaluation} that catches overcorrection invisible to susceptibility measures.
    
    \item \textbf{Technique rankings differ between metrics}: Under susceptibility (spread reduction), most techniques appear effective. Under convergence, Full SACD leads (+24\%) while our Outside View implementation showed $-22\%$ worse convergence (confounded; see Section 5.3). Effect sizes are small ($d \leq 0.41$).
    
    \item \textbf{High model variance}: 5/10 models significantly improve with SACD, but Opus 4.6 shows 68\% \textit{worse} convergence.
    
    \item \textbf{13,799 trials across 10 models} with Bonferroni-corrected statistics and effect sizes.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\subsection{Anchoring Bias in Human Judgment}

Anchoring bias---the disproportionate influence of initial information on subsequent estimates---is among the most robust findings in cognitive psychology \citep{tversky1974}. Even experts are susceptible: \citet{englich2006} demonstrated that experienced judges' sentencing decisions were influenced by random numbers generated by dice rolls. Effect sizes of $d = 0.6$--$1.2$ persist regardless of anchor source or participant awareness. Our experimental paradigm adapts this judicial sentencing design.

\subsection{Cognitive Biases in LLMs}

Recent work has shown that LLMs exhibit human-like cognitive biases \citep{binz2023,jones2022}. Anchoring effects have been documented across multiple model families \citep{huang2025anchoring}, with susceptibility varying by model architecture and size. \citet{song2026reasoning} survey LLM reasoning failures comprehensively, including susceptibility to anchoring and framing effects. Unlike humans, LLMs can be tested exhaustively across conditions, enabling systematic bias measurement.

\subsection{Debiasing Techniques}

Several techniques have been proposed for mitigating anchoring:

\textbf{Outside View / Reference Class Forecasting:} Prompting models to consider what typically happens in similar cases \citep{sibony2019}. Effective in human contexts but requires specifying an appropriate reference class.

\textbf{Self-Administered Cognitive Debiasing (SACD):} Iterative prompting that guides models through bias detection and correction \citep{lyu2025}. Shows promise but is computationally expensive and, as we show, model-dependent.

\textbf{Devil's Advocate:} Prompting models to argue against their initial response. Common in deliberation literature but mixed results for numeric judgments.

\textbf{Premortem Analysis:} Asking models to imagine the decision failed and explain why. Drawn from project management practice \citep{klein2007}.

\subsection{Evaluation Methodology}

Standard anchoring evaluation compares high-anchor and low-anchor conditions \citep{englich2006,huang2025anchoring}:

\[
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\]

A technique ``works'' if it reduces this gap. This methodology does not require ground truth---it measures susceptibility to anchors, not accuracy of outputs. This is a valid and important metric.

We extend this by introducing \textbf{baseline convergence}:

\[
\text{Convergence Error} = |R_{technique} - R_{baseline}|
\]

This requires collecting baseline responses but enables detection of \textbf{overcorrection}---a failure mode invisible to susceptibility-only evaluation. To our knowledge, no prior work on LLM anchoring has systematically collected baselines without explicit anchors for convergence evaluation.

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Evaluation Metrics}

We distinguish two evaluation approaches for debiasing techniques:

\subsubsection{Standard Metric: Anchor Susceptibility}

The conventional approach compares responses under high vs.\ low anchor conditions:

\[
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\]

A technique ``works'' if it reduces this gap. This metric answers: \textit{Does the technique reduce the anchor's influence?}

\subsubsection{Our Metric: Baseline Convergence}

We collected baseline responses without explicit anchors---model outputs with no prosecutor demand anchor present. This enables a second metric:

\[
\text{Convergence Error} = |\bar{R}_{technique} - \bar{R}_{baseline}|
\]

A technique succeeds if it reduces convergence error relative to the anchored (no-technique) condition:

\[
\text{Improved} = |R_{technique} - R_{baseline}| < |R_{anchored} - R_{baseline}|
\]

This metric answers: \textit{Does the technique bring the response closer to the model's unprompted judgment?}

\subsubsection{Why Both Metrics Matter}

These metrics can diverge. Consider:
\begin{itemize}
    \item Baseline: 30mo
    \item High-anchor response: 50mo (convergence error = 20mo)
    \item Technique response: 12mo (convergence error = 18mo... but overcorrected)
\end{itemize}

Under susceptibility, the technique ``worked'' (moved away from anchor). Under convergence, it marginally helped---but a different technique might achieve 28mo (convergence error = 2mo).

\subsection{Experimental Design}

\subsubsection{Models}

We evaluated 10 models across 4 providers:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Provider & Models \\
\midrule
Anthropic & Claude Haiku 4.5, Sonnet 4.6, Opus 4.6 \\
OpenAI & GPT-4.1, GPT-5.2, o3, o4-mini \\
DeepSeek & DeepSeek-v3.2 \\
Others & Kimi-k2.5 (Moonshot), GLM-5 (Zhipu) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Conditions}

\begin{enumerate}
    \item \textbf{Baseline}: Sentencing prompt with no anchor
    \item \textbf{Low anchor}: 3-month anchor in prosecutor demand
    \item \textbf{High anchor}: 36--60 month anchor in prosecutor demand
    \item \textbf{Techniques}: Applied to high-anchor condition
\end{enumerate}

\subsubsection{Techniques Evaluated}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Technique & Description \\
\midrule
Outside View & ``What typically happens in similar cases?'' (required jurisdiction) \\
Devil's Advocate & ``Argue against your initial response'' \\
Premortem & ``Imagine this sentence was overturned---why?'' \\
Random Control & Extra conversation turns with neutral content \\
Full SACD & Iterative self-administered cognitive debiasing \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Temperature Conditions}

Each technique was tested at three temperatures: t=0 (deterministic), t=0.7 (moderate variance), and t=1.0 (high variance). Baseline responses were collected at all three temperatures. Results are aggregated across temperatures; preliminary analysis showed no significant temperature$\times$technique interaction effects ($p > 0.1$ for all comparisons).

\subsubsection{Trial Counts and Procedure}

\begin{itemize}
    \item \textbf{Total trials}: 13,799
    \item \textbf{Per model-technique-temperature}: 30--90 trials (target $n \geq 30$; range reflects iterative data collection with some conditions receiving additional trials for robustness)
    \item \textbf{Baseline trials}: 909 total (approximately 90 per model across all temperatures)
    \item \textbf{Response extraction}: Final numeric response extracted via regex pattern matching for integer month values
    \item \textbf{Trial assignment}: Trials run in batches by model and technique; order randomized within batches
    \item \textbf{Anchor values}: To ensure equivalent relative anchor strength across models, we use constant proportional anchors: high anchor = baseline $\times$ 1.5 (50\% above baseline); low anchor = baseline $\times$ 0.5 (50\% below baseline). This design ensures each model experiences the same relative anchor pressure, enabling valid within-model comparisons of technique effectiveness. Fixed absolute anchors would create unequal anchor strength across models with different baselines.
\end{itemize}

\begin{table}[H]
\centering
\caption{Trial distribution by condition. Each comparison uses the sample sizes shown; trials are assigned to conditions without overlap. All model-technique-temperature cells achieved $n \geq 30$.}
\label{tab:trial-counts}
\begin{tabular}{lr}
\toprule
Condition & $n$ \\
\midrule
\multicolumn{2}{l}{\textit{Debiasing Techniques (vs. anchored control)}} \\
\quad Full SACD & 2,391 \\
\quad Outside View & 2,423 \\
\quad Random Control & 2,215 \\
\quad Premortem & 2,186 \\
\quad Devil's Advocate & 2,166 \\
\midrule
\multicolumn{2}{l}{\textit{Control Conditions}} \\
\quad Anchored (no technique) & 1,509 \\
\quad Unanchored Baseline & 909 \\
\midrule
\textbf{Total unique trials} & \textbf{13,799} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Confounds and Limitations}

\subsubsection{Outside View Jurisdiction Context}

To avoid model safety refusals, Outside View prompts included jurisdiction specification:

\begin{quote}
``In German federal courts, what is the TYPICAL probation sentence...''
\end{quote}

This may have introduced a secondary anchor toward German sentencing norms ($\sim$12--18 months for probation). Other techniques did not require this modification.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Baseline Responses}

Unanchored baseline responses varied substantially across models:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Model & Baseline Mean & SD \\
\midrule
o4-mini & 35.7mo & 4.7 \\
o3 & 33.7mo & 5.6 \\
GLM-5 & 31.9mo & 5.7 \\
GPT-5.2 & 31.8mo & 5.7 \\
Kimi-k2.5 & 30.6mo & 7.4 \\
DeepSeek-v3.2 & 29.6mo & 8.0 \\
Haiku 4.5 & 29.1mo & 11.2 \\
GPT-4.1 & 25.1mo & 3.4 \\
Sonnet 4.6 & 24.1mo & 1.3 \\
Opus 4.6 & 18.0mo & 0.0 \\
\bottomrule
\end{tabular}
\caption{Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus 4.6 shows zero variance at all temperatures, consistently responding with exactly 18 months---suggesting highly deterministic reasoning for this prompt type.}
\end{table}

\subsection{High-Anchor Responses (No Technique)}

Under high-anchor conditions without intervention, two anchor response patterns emerge:

\begin{enumerate}
    \item \textbf{Compression}: Response pulled below baseline (Anthropic models, GPT-4.1)
    \item \textbf{Inflation}: Response pulled above baseline (GPT-5.2, GLM-5, o3)
\end{enumerate}

\subsection{Technique Effectiveness: Baseline Convergence}

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\toprule
Technique & $n$ & Mean Dist & 95\% CI & Improvement & $p$ (Bonf) & $d$ \\
\midrule
Anchored baseline & 1509 & 12.4mo & [12.0, 12.7] & --- & --- & --- \\
\textbf{Full SACD} & 2391 & 9.4mo & [9.1, 9.8] & \textbf{+24\%} & $<.001$ & 0.41 \\
Premortem & 2186 & 11.1mo & [10.8, 11.5] & +10\% & $<.001$ & 0.17 \\
Random Control & 2215 & 11.3mo & [11.0, 11.6] & +9\% & $<.001$ & 0.15 \\
Devil's Advocate & 2166 & 12.1mo & [11.8, 12.4] & +2\% (ns) & 1.000 & 0.03 \\
\midrule
\textit{Outside View}$^\dagger$ & 2423 & 15.1mo & [14.8, 15.4] & $-22\%$ & $<.001$ & $-0.38$ \\
\bottomrule
\end{tabular}
\caption{Technique effectiveness with 95\% confidence intervals and Bonferroni-corrected p-values. Effect sizes are small by Cohen's conventions ($d < 0.5$); statistical significance does not imply practical significance. $^\dagger$Outside View result confounded by required jurisdiction specification; included for transparency but excluded from primary conclusions.}
\end{table}

\subsection{Model-Specific Results: Full SACD}

Full SACD shows high variance across models (Bonferroni-corrected, 10 tests):

\begin{table}[H]
\centering
\begin{tabular}{lccl}
\toprule
Model & Improvement & $p$ (adj) & Result \\
\midrule
o3 & +51\% & $<.001$ & Significant improvement \\
GPT-4.1 & +48\% & $<.001$ & Significant improvement \\
Sonnet 4.6 & +46\% & $<.001$ & Significant improvement \\
DeepSeek-v3.2 & +30\% & $<.001$ & Significant improvement \\
GPT-5.2 & +20\% & 0.022 & Significant improvement \\
o4-mini & +12\% & 0.210 & Not significant \\
Haiku 4.5 & $-2\%$ & 1.000 & Not significant \\
Kimi-k2.5 & $-3\%$ & 1.000 & Not significant \\
GLM-5 & $-4\%$ & 1.000 & Not significant \\
\textbf{Opus 4.6} & $\mathbf{-68\%}$ & $<.001$ & \textbf{Significant backfire} \\
\bottomrule
\end{tabular}
\caption{Full SACD model-specific results. 5/10 significantly improve, 1/10 significantly worsens (Opus 4.6).}
\end{table}

Key findings:
\begin{enumerate}
    \item \textbf{5/10 models significantly improve} after Bonferroni correction
    \item \textbf{Opus 4.6 shows severe backfire} ($-68\%$, $p<.001$)---the technique makes it \textit{worse}
    \item \textbf{Effect sizes remain small} even for best performers ($d \leq 0.41$)
\end{enumerate}

\subsection{Why Baseline Collection Matters}

Consider a technique that reduces all responses to the same value regardless of anchor. Under susceptibility ($|R_{high} - R_{low}|$), this appears perfect---zero spread. Under convergence ($|R - R_{baseline}|$), the technique may perform poorly if that fixed value diverges from the baseline.

Our Outside View implementation (as confounded by jurisdiction specification) exemplifies this: it produces consistent responses that diverge from model baselines by 22\%. Without baseline collection, this overcorrection would be invisible.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Why Full SACD Works (and Fails)}

Full SACD shows the largest average improvement (+24\%) but also the highest model variance. We propose:

\textbf{Hypothesis 1: Iterative reflection enables genuine reconsideration.} Multiple rounds of ``examine your reasoning'' prompts may help models escape local optima in their reasoning chains.

\textbf{Hypothesis 2: Some models perform ``debiasing theater.''} Opus 4.6's severe backfire ($-68\%$) suggests the technique can activate surface compliance without genuine reconsideration---the model may be optimizing for \textit{appearing} to reconsider rather than actually doing so.

\textbf{Hypothesis 3: Baseline proximity matters.} Opus 4.6 has the lowest baseline (18mo), meaning SACD may be pulling it \textit{away} from its natural judgment toward a perceived ``expected answer.''

\subsection{Why Random Control Works}

Random Control (+9\%) outperforms Devil's Advocate (+2\% ns), despite having no debiasing content. Possible mechanisms:

\textbf{Attention redistribution.} Additional turns dilute the anchor's influence by introducing competing context.

\textbf{Implicit reconsideration.} Multi-turn format may trigger revision behavior even without explicit instructions.

\subsection{The Outside View Confound}

Outside View performed worst despite being recommended in human debiasing literature. Our implementation required jurisdiction specification (``German federal courts'') to avoid model safety refusals. This may have introduced a secondary anchor:

\begin{itemize}
    \item German probation for repeat shoplifting: $\sim$12--18 months
    \item Our model baselines (without explicit anchor): 18--36 months
    \item Outside View consistently pulled toward $\sim$15 months
\end{itemize}

\textbf{Implication for practitioners:} When using Outside View, ensure the reference class matches your actual decision context. Specifying a jurisdiction to avoid refusals may import that jurisdiction's norms.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Single domain.} All experiments use judicial sentencing vignettes. Replication across other anchoring domains (estimation, forecasting, negotiation) is needed before generalizing findings.
    \item \textbf{Outside View confound.} Our Outside View implementation required jurisdiction specification to avoid model refusals. We cannot fully disentangle whether the technique itself fails or whether our implementation introduced a secondary anchor. Future work should test jurisdiction-neutral Outside View prompts.
    \item \textbf{Baseline interpretation.} Our baseline still includes numeric context (``12th offense''); it is ``without explicit anchor,'' not truly ``unanchored.'' We measure convergence toward the model's considered judgment, not an objective ground truth---which does not exist for sentencing decisions. The baseline represents the model's response absent \textit{explicit prosecutor demand anchoring}, not an ``unbiased'' state.
    \item \textbf{Model coverage.} 10 models from 4 providers is substantial but not exhaustive. Results may not apply to other model families.
    \item \textbf{Prompt disclosure.} Complete prompt templates are available at our repository; we acknowledge that prompt engineering choices may influence results.
\end{enumerate}

\subsection{Practical Recommendations}

Based on our findings in the judicial sentencing domain (generalization to other domains requires validation):

\begin{enumerate}
    \item \textbf{Consider structural interventions.} Adding conversation turns (Random Control, +9\%) provides meaningful improvement with minimal prompt engineering.
    \item \textbf{Test per-model.} Technique effectiveness varies substantially across models; Full SACD helps some models while severely hurting others (Opus: $-68\%$).
    \item \textbf{Collect baselines.} We propose baseline convergence as a complementary metric to susceptibility. Measuring convergence toward the model's unprompted judgment catches overcorrection invisible to spread-based metrics.
    \item \textbf{Be cautious with reference class prompts.} Our Outside View implementation suggests that specifying reference classes may introduce secondary anchors. If using Outside View, ensure the reference class does not anchor toward a specific value.
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

We introduced baseline convergence as a metric for evaluating LLM debiasing techniques. This metric catches overcorrection---a failure mode invisible to standard susceptibility measures.

Our key findings from 13,799 trials across 10 models:

\begin{enumerate}
    \item \textbf{Full SACD leads, but with high variance.} +24\% average improvement ($d=0.41$), but Opus 4.6 shows $-68\%$ backfire. Practitioners must test per-model.
    \item \textbf{Our Outside View implementation produced confounded results.} As implemented with required jurisdiction context, it showed $-22\%$ worse convergence. This finding is confounded and should not be interpreted as evidence against Outside View techniques in general. Future work should test jurisdiction-neutral implementations.
    \item \textbf{Effect sizes are small.} Even the best technique achieves only $d=0.41$ (``small'' by Cohen's conventions). Statistical significance does not imply practical significance; practitioners should weigh improvement magnitude against deployment costs.
    \item \textbf{Baseline collection is essential.} Without it, we would have concluded Outside View was effective---the susceptibility metric alone missed the overcorrection.
\end{enumerate}

For practitioners: test debiasing techniques per-model before deployment. Full SACD is effective for most models but can severely backfire. Simple structural interventions (Random Control, +9\%) may be safer than sophisticated prompts.

For researchers: collect baselines without explicit anchors. The standard high-vs-low methodology has a blind spot that can hide overcorrection effects.

%==============================================================================
\appendix
\section{Prompt Templates}
\label{app:prompts}

\subsection{Case Vignette (All Conditions)}

\begin{quote}
\small
You are a trial judge in a criminal court. Read the case summary and determine an appropriate sentence.

Case summary:
\begin{itemize}
\item The defendant, Lena M., has been convicted of shoplifting.
\item This is her 12th shoplifting offense.
\item She stole several items from a supermarket.
\item A psycho-legal expert has provided an advisory opinion on the case.
\item Statements from the defendant and a witness have been reviewed.
\item Based on the applicable penal code provisions, sentences for this type of offense typically involve probation.
\end{itemize}
\end{quote}

\subsection{Anchor Introduction (High/Low Anchor Conditions)}

\begin{quote}
\small
For experimental purposes, the following prosecutor's sentencing demand was randomly determined, therefore, it does not reflect any judicial expertise:

The prosecutor demands as a sentence for the accused Lena M. \texttt{[ANCHOR]} months on probation.
\end{quote}

\subsection{Outside View (As Implemented)}

\begin{quote}
\small
In German federal courts, what is the TYPICAL probation sentence for a defendant with 12 prior shoplifting offenses?
\end{quote}

\textit{Note: Jurisdiction specification was required to avoid model safety refusals but may have introduced a secondary anchor.}

\subsection{Full SACD (Iterative Self-Administered Cognitive Debiasing)}

Following \citet{lyu2025}, Full SACD implements three iterative steps:

\begin{enumerate}
\item \textbf{Bias Determination}: ``Analyze the following prompt... For each sentence, determine if it contains a cognitive bias''
\item \textbf{Bias Analysis}: If bias detected, classify the type (anchoring, framing, etc.)
\item \textbf{Cognitive Debiasing}: ``Rewrite the flagged sentences to remove the bias''
\end{enumerate}

Steps repeat until no bias is detected or maximum iterations (5) reached. Average iterations to convergence: 2.3.

\subsection{Random Control}

Random Control prompts consisted of unrelated elaboration requests (e.g., ``Describe the courtroom setting in detail'') designed to add conversation turns without debiasing content.

%==============================================================================
\section*{Data and Code Availability}

All trial data, analysis scripts, and prompts are available at \url{https://github.com/voder-ai/bAIs}. The repository includes raw JSONL trial data for all 13,799 trials, statistical analysis scripts reproducible from raw data, and complete prompts for all debiasing techniques.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
