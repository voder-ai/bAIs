\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Title
\title{Debiasing Anchoring Bias in LLM Judicial Reasoning:\\Why Metric Choice Determines Technique Recommendation}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
Large language models exhibit anchoring bias---disproportionate influence of initial numeric information on subsequent judgments. How should we evaluate debiasing techniques? The standard approach measures \textbf{susceptibility}: the gap between responses under high vs.\ low anchors. A technique ``works'' if it reduces this gap. We show this metric can be misleading.

We propose measuring technique responses as a \textbf{percentage of baseline}---the model's unanchored judgment. This simple metric (response $\div$ baseline $\times$ 100\%) directly answers: ``How close is the debiased response to where it should be?'' A perfect technique produces responses at 100\% of baseline.

Across 14,154 trials on 10 models, we find that \textbf{susceptibility and baseline metrics give divergent rankings}:

\begin{center}
\begin{tabular}{lcc}
\toprule
Technique & Susceptibility Rank & Baseline Rank \\
\midrule
Devil's Advocate & \#1 (best) & \#4 (worst) \\
Full SACD & \#3 & \#1 (best) \\
\bottomrule
\end{tabular}
\end{center}

Devil's Advocate reduces spread (low susceptibility) but keeps responses anchored at only 63.6\% of baseline---\textit{consistently wrong}. Full SACD achieves 93.7\% of baseline (closest to correct), but shows \textbf{bidirectional asymmetry}: from low anchors it reaches 75.7\%, from high anchors 112.0\%.

\textbf{Both baseline-aware metrics favor SACD.} By \textit{average response}, SACD is best (6.3\% from baseline vs.\ Premortem's 8.4\%). By \textit{per-trial error}, SACD is also best (18.1\% mean absolute deviation vs.\ Premortem's 22.6\%). However, model-specific variation is substantial---Haiku undershoots to 47.8\% while DeepSeek achieves 100.8\%. Without baseline collection, none of these distinctions are visible.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

When evaluating debiasing techniques for LLMs, which metric should you use? The answer determines which technique you recommend---and the standard metric can mislead.

We report findings from 14,154 trials across 10 models evaluating four debiasing techniques. Our core finding: \textbf{susceptibility and baseline-relative metrics give divergent technique rankings}. The technique that looks best under susceptibility (Devil's Advocate) looks worst when measured against baseline---and vice versa for SACD.

\subsection{Two Metrics, Opposite Conclusions}
\label{sec:two-metrics}

\textbf{Susceptibility} (standard): Measures the gap between high-anchor and low-anchor responses. Lower gap = less susceptible = ``better.''
\begin{equation}
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\end{equation}

\textbf{Susceptibility change} ($\Delta$) measures how a technique affects this gap relative to no-technique baseline:
\begin{equation}
\Delta_{\text{susceptibility}} = \frac{\text{Spread}_{\text{technique}} - \text{Spread}_{\text{no-technique}}}{\text{Spread}_{\text{no-technique}}} \times 100\%
\end{equation}
Negative $\Delta$ = reduced spread = ``less susceptible.'' Positive $\Delta$ = increased spread.

\textbf{Percentage of Baseline} (ours): Measures where the response lands relative to the model's unanchored judgment. Closer to 100\% = ``better.''
\begin{equation}
\text{\% of Baseline} = \frac{R_{technique}}{R_{baseline}} \times 100\%
\end{equation}

The baseline metric directly answers: ``Is the debiased response close to what the model would say without any anchor?''

\subsection{The Inversion}

Our key finding:

\begin{center}
\begin{tabular}{lccc}
\toprule
Technique & Susceptibility & \% of Baseline & Deviation \\
\midrule
Devil's Advocate & \textbf{$-$23.7pp} (best) & 63.6\% & 36.4\% (worst) \\
Random Control & +30.1pp & 78.3\% & 21.7\% \\
Premortem & +45.2pp & 91.6\% & 8.4\% \\
Full SACD & +36.3pp & \textbf{93.7\%} & 6.3\% (best) \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A percentage-of-baseline metric} for debiasing evaluation---simpler and more interpretable than distance-based alternatives.
    
    \item \textbf{Empirical demonstration of metric divergence} across 14,154 trials on 10 models, with model-specific breakdowns showing high variance.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\subsection{Anchoring Bias in Human Judgment}

Anchoring bias---the disproportionate influence of initial information on subsequent estimates---is among the most robust findings in cognitive psychology \citep{tversky1974}. Even experts are susceptible: \citet{englich2006} demonstrated that experienced judges' sentencing decisions were influenced by random numbers generated by dice rolls. Effect sizes of $d = 0.6$--$1.2$ persist regardless of anchor source or participant awareness. Our experimental paradigm adapts this judicial sentencing design.

\subsection{Cognitive Biases in LLMs}

Recent work has shown that LLMs exhibit human-like cognitive biases \citep{binz2023,jones2022,chen2025cognitive}. Anchoring effects have been documented across multiple model families \citep{huang2025anchoring}, with susceptibility varying by model architecture and size. \citet{song2026reasoning} survey LLM reasoning failures comprehensively, including susceptibility to anchoring and framing effects. Unlike humans, LLMs can be tested exhaustively across conditions, enabling systematic bias measurement.

\subsection{Debiasing Techniques}

Several techniques have been proposed for mitigating anchoring:

\textbf{Outside View / Reference Class Forecasting:} Prompting models to consider what typically happens in similar cases \citep{sibony2019}. Effective in human contexts but requires specifying an appropriate reference class.

\textbf{Self-Administered Cognitive Debiasing (SACD):} Iterative prompting that guides models through bias detection and correction \citep{lyu2025}. Shows promise but is computationally expensive and, as we show, model-dependent.

\textbf{Devil's Advocate:} Prompting models to argue against their initial response. Common in deliberation literature but mixed results for numeric judgments.

\textbf{Premortem Analysis:} Asking models to imagine the decision failed and explain why. Drawn from project management practice \citep{klein2007}.

Recent work has also explored debiasing against framing effects \citep{lim2026deframe}, which shares conceptual overlap with anchoring (both involve sensitivity to presentation rather than content).

\subsection{Evaluation Methodology}

Standard anchoring evaluation compares high-anchor and low-anchor conditions \citep{englich2006,huang2025anchoring}:

\[
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\]

A technique ``works'' if it reduces this gap. The classic Anchoring Index (AI) from \citet{jacowitz1995} similarly measures anchor influence as the ratio of response movement toward the anchor. Our susceptibility metric is conceptually equivalent but normalized for cross-technique comparison. Neither requires ground truth---both measure susceptibility to anchors, not accuracy of outputs.

We extend this by introducing \textbf{percentage of baseline}:

\[
\text{\% of Baseline} = \frac{R_{technique}}{R_{baseline}} \times 100\%
\]

This metric directly measures where the debiased response lands relative to the model's unanchored judgment. A perfect technique produces responses at exactly 100\% of baseline. This requires collecting baseline responses but enables detection of techniques that appear to ``work'' under susceptibility while keeping responses anchored at incorrect values.

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Evaluation Metrics}

We compare susceptibility (standard) with \% of baseline (proposed). Susceptibility measures high-low spread; \% of baseline measures proximity to unanchored judgment. Formulas defined in Section~\ref{sec:two-metrics}.

\textbf{Interpretation of \% of baseline:}
\begin{itemize}
    \item 100\% = response matches unanchored judgment (perfect debiasing)
    \item $<$100\% = response remains below baseline (under-correction or opposite-direction anchor)
    \item $>$100\% = response overshoots baseline
\end{itemize}

\textbf{Deviation from baseline} measures how far from perfect:
\[
\text{Deviation} = |(\text{\% of Baseline}) - 100\%|
\]

Lower deviation = better. A technique that produces responses at 93.7\% of baseline (6.3\% deviation) is better than one at 63.6\% (36.4\% deviation).

\textbf{Validation: \% vs.\ absolute deviation.} To verify our metric choice, we compared rankings using \% deviation from baseline vs.\ absolute deviation in months. Rankings are identical: Full SACD ranks \#1 by both metrics (6.3\% deviation), Devil's Advocate ranks \#4 (36.4\%). The \% metric enables cross-model comparison while preserving the ranking.

This metric answers: \textit{Does the technique bring the response closer to the model's unprompted judgment?}

\subsubsection{Why Both Metrics Matter}

These metrics give \textbf{divergent rankings}:

\begin{table}[H]
\centering
\caption{Susceptibility vs.\ \% of Baseline: Rankings diverge. Devil's Advocate looks best under susceptibility but worst under baseline. \textit{Spread} = high-low response gap (susceptibility). \textit{Anchor correction asymmetry} = qualitative difference in low vs.\ high anchor correction (see Table~\ref{tab:anchor-asymmetry}). 95\% CIs from bootstrap.}
\label{tab:metric-comparison}
\begin{tabular}{lccccc}
\toprule
Technique & Spread & Asymmetry & Rank & \% of Baseline & Rank \\
\midrule
Devil's Advocate & 23.7pp & lowest & \#1 & 63.6\% [62, 65] & \#4 \\
Random Control & 30.1pp & --- & \#2 & 78.3\% [77, 80] & \#3 \\
Premortem & 45.2pp & highest & \#4 & 91.6\% [90, 93] & \#2 \\
Full SACD & 36.3pp & --- & \#3 & 93.7\% [92, 95] & \#1 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=14pt,
    width=0.95\columnwidth,
    height=6cm,
    ylabel={\% of Baseline (100\% = perfect)},
    ymin=55,
    ymax=105,
    xtick=data,
    symbolic x coords={Devil's Advocate, Random Control, Premortem, Full SACD},
    xticklabel style={font=\small},
    nodes near coords,
    nodes near coords align={vertical},
    every node near coord/.append style={font=\footnotesize\bfseries},
    enlarge x limits=0.15,
    axis lines*=left,
    ymajorgrids=true,
    grid style={dashed,gray!30},
]
\addplot[fill=red!60, draw=black] coordinates {(Devil's Advocate, 63.6)};
\addplot[fill=orange!60, draw=black] coordinates {(Random Control, 78.3)};
\addplot[fill=blue!50, draw=black] coordinates {(Premortem, 91.6)};
\addplot[fill=green!60!black, draw=black] coordinates {(Full SACD, 93.7)};
\draw[dashed, black, thick] (axis cs:{[normalized]-0.3},100) -- (axis cs:{[normalized]3.3},100);
\end{axis}
\end{tikzpicture}
\caption{Technique responses as \% of baseline. Dashed line = 100\% (perfect). Devil's Advocate keeps responses at 63.6\% of baseline---consistently wrong despite appearing ``best'' under susceptibility. Full SACD achieves 93.7\%---closest to correct.}
\label{fig:metric-comparison}
\end{figure}

\textbf{Why the divergence?} Devil's Advocate produces \textit{consistent} responses (low susceptibility/spread) that are \textit{consistently anchored at the wrong value} (63.6\% of baseline). SACD produces \textit{variable} responses (higher susceptibility) that are \textit{close to correct on average} (93.7\% of baseline---though this average masks bidirectional deviation: 75.7\% from low anchors, 112.0\% from high anchors).

\textbf{Effect sizes (Cohen's d):} The difference between Full SACD and Devil's Advocate on \% of baseline is large ($d = 1.06$). SACD vs.\ Random Control is medium ($d = 0.51$). Premortem vs.\ Devil's Advocate is medium-large ($d = 0.71$). These effect sizes confirm that metric choice has practical, not just statistical, significance.

\subsection{Experimental Design}

\subsubsection{Models}

We evaluated 10 models across 4 providers:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Provider & Models \\
\midrule
Anthropic & Claude Haiku 4.5, Sonnet 4.6, Opus 4.6 \\
OpenAI & GPT-4.1, GPT-5.2, o3, o4-mini \\
DeepSeek & DeepSeek-v3.2 \\
Others & Kimi-k2.5 (Moonshot), GLM-5 (Zhipu) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Conditions}

\begin{enumerate}
    \item \textbf{Baseline}: Sentencing prompt with no anchor
    \item \textbf{Low anchor}: Prosecutor demand at baseline $\times$ 0.5
    \item \textbf{High anchor}: Prosecutor demand at baseline $\times$ 1.5
    \item \textbf{Techniques}: Applied to \textit{both} high-anchor and low-anchor conditions (enabling susceptibility calculation)
\end{enumerate}

\subsubsection{Techniques Evaluated}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Technique & Description \\
\midrule
Outside View & ``What typically happens in similar cases?'' (required jurisdiction) \\
Devil's Advocate & ``Argue against your initial response'' \\
Premortem & ``Imagine this sentence was overturned---why?'' \\
Random Control & Extra conversation turns with neutral content \\
Full SACD & Iterative self-administered cognitive debiasing \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Temperature Conditions}

Each technique was tested at three temperatures: t=0 (deterministic), t=0.7 (moderate variance), and t=1.0 (high variance). Baseline responses were collected at all three temperatures. Results are aggregated across temperatures. We tested for temperature$\times$technique interactions using two-way ANOVA; no significant interactions were found ($F < 1.5$, $p > 0.1$ for all technique comparisons). Temperature main effects were small: \% of baseline varied by $<$3 percentage points across temperatures within each technique.

\subsubsection{Trial Counts and Procedure}

\begin{itemize}
    \item \textbf{Total trials}: 14,154
    \item \textbf{Per model-technique-temperature}: 30--90 trials. Stopping rule: minimum $n = 30$ per cell, pre-specified before data collection. Some cells received additional trials (up to 90) when early results suggested high variance, but no trials were excluded based on outcomes. Analysis uses all collected data.
    \item \textbf{Baseline trials}: 909 total (approximately 90 per model across all temperatures)
    \item \textbf{Response extraction}: Final numeric response extracted via regex pattern matching for integer month values
    \item \textbf{Trial assignment}: Trials run in batches by model and technique; order randomized within batches
    \item \textbf{Anchor values}: To ensure equivalent relative anchor strength across models, we use constant proportional anchors: high anchor = baseline $\times$ 1.5 (50\% above baseline); low anchor = baseline $\times$ 0.5 (50\% below baseline). This design ensures each model experiences the same relative anchor pressure, enabling valid within-model comparisons of technique effectiveness. Fixed absolute anchors would create unequal anchor strength across models with different baselines.
\end{itemize}

\begin{table}[H]
\centering
\caption{Trial distribution. Total unique trials: 14,154. Sample sizes shown are for primary analyses; technique comparisons use matched model-temperature subsets.}
\label{tab:trial-counts}
\begin{tabular}{lr}
\toprule
Condition & $n$ (analysis) \\
\midrule
\multicolumn{2}{l}{\textit{Debiasing Techniques}} \\
\quad Full SACD & 2,391 \\
\quad Outside View & 2,423 \\
\quad Random Control & 2,215 \\
\quad Premortem & 2,186 \\
\quad Devil's Advocate & 2,166 \\
\midrule
\multicolumn{2}{l}{\textit{Control Conditions}} \\
\quad Anchored (no technique) & 1,864 \\
\quad Baseline (no anchor) & 909 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Statistical Analysis}

All comparisons use \textbf{Welch's t-test} (unequal variances assumed) with \textbf{Bonferroni correction} for multiple comparisons (5 technique comparisons). Effect sizes are reported as Cohen's $d$. Confidence intervals are 95\%. Statistical significance ($p < .05$ after correction) does not imply practical significance; we emphasize effect sizes throughout.

\textbf{Analysis is fully deterministic}: all statistics are computed from raw JSONL trial data using scripts in our repository. No manual intervention or selective reporting.

\subsection{Confounds and Limitations}

\subsubsection{Outside View Jurisdiction Context}

Outside View prompts required jurisdiction specification (``German federal courts'') to avoid safety refusals, potentially introducing a secondary anchor. See Section~\ref{sec:outside-view-confound} for analysis.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Baseline Responses}

Unanchored baseline responses varied substantially across models:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Model & Baseline Mean & SD \\
\midrule
o4-mini & 35.7mo & 4.7 \\
o3 & 33.7mo & 5.6 \\
GLM-5 & 31.9mo & 5.7 \\
GPT-5.2 & 31.8mo & 5.7 \\
Kimi-k2.5 & 30.6mo & 7.4 \\
DeepSeek-v3.2 & 29.6mo & 8.0 \\
Haiku 4.5 & 29.1mo & 11.2 \\
GPT-4.1 & 25.1mo & 3.4 \\
Sonnet 4.6 & 24.1mo & 1.3 \\
Opus 4.6 & 18.0mo & 0.0 \\
\bottomrule
\end{tabular}
\caption{Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus 4.6 shows zero variance (SD=0.0) at all temperatures, consistently responding with exactly 18 months. We treat this as a legitimate model characteristic rather than excluding Opus; the zero variance may reflect strong priors from training or highly deterministic reasoning for judicial prompts. Statistical comparisons involving Opus should be interpreted with this caveat.}
\end{table}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    xbar,
    bar width=8pt,
    width=0.95\columnwidth,
    height=7cm,
    xlabel={Baseline Sentence (months)},
    xmin=0,
    xmax=42,
    ytick=data,
    yticklabels={Opus 4.6, Sonnet 4.6, GPT-4.1, Haiku 4.5, DeepSeek-v3.2, Kimi-k2.5, GPT-5.2, GLM-5, o3, o4-mini},
    y dir=reverse,
    nodes near coords,
    nodes near coords align={horizontal},
    every node near coord/.append style={font=\footnotesize, xshift=2pt},
    enlarge y limits=0.08,
    axis lines*=left,
    xmajorgrids=true,
    grid style={dashed,gray!30},
]
\addplot[fill=blue!60, draw=black] coordinates {
    (18.0, 1) (24.1, 2) (25.1, 3) (29.1, 4) (29.6, 5) (30.6, 6) (31.8, 7) (31.9, 8) (33.7, 9) (35.7, 10)
};
\end{axis}
\end{tikzpicture}
\caption{Model baseline variation. Without any anchor, models produce sentences ranging from 18 to 36 months---a 17.7-month spread. This variation motivates per-model anchor calibration.}
\label{fig:baselines}
\end{figure}

\subsection{High-Anchor Responses (No Technique)}

Under high-anchor conditions without intervention, two distinct response patterns emerge:

\begin{enumerate}
    \item \textbf{Compression}: Response pulled \textit{below} baseline (Anthropic models, GPT-4.1)
    \item \textbf{Inflation}: Response pulled above baseline (GPT-5.2, GLM-5, o3)
\end{enumerate}

The compression pattern is counterintuitive---high anchors typically pull responses upward. We hypothesize this reflects \textbf{anchor rejection}: some models recognize the high prosecutor demand as unreasonable and overcorrect downward. This is consistent with research showing that implausible anchors can trigger contrast effects rather than assimilation \citep{tversky1974}. 

\textbf{Which models compress?} Anthropic models (Opus, Sonnet, Haiku) and GPT-4.1 consistently show compression under high anchors. OpenAI's reasoning models (o3, o4-mini) and GPT-5.2 show the expected inflation pattern. This model-family clustering suggests compression may relate to training methodology or safety tuning rather than model scale.

\textbf{Implications:} The compression pattern does not invalidate our \% of baseline metric---in fact, it highlights its value. For compression models, a technique that \textit{increases} responses toward 100\% is improving, even though it moves responses ``upward.'' Our metric captures this correctly: 90\% of baseline is better than 70\% of baseline, regardless of direction.

\subsection{Technique Effectiveness: Percentage of Baseline}

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Technique & $n$ & \% of Baseline & 95\% CI & Deviation & Rank \\
\midrule
\textbf{Full SACD} & 2,391 & \textbf{93.7\%} & [92, 95] & \textbf{6.3\%} & \textbf{\#1} \\
Premortem & 2,186 & 91.6\% & [90, 93] & 8.4\% & \#2 \\
Random Control & 2,215 & 78.3\% & [77, 80] & 21.7\% & \#3 \\
Devil's Advocate & 2,166 & 63.6\% & [62, 65] & 36.4\% & \#4 \\
\midrule
\textit{Outside View}$^\dagger$ & 2,423 & 51.2\% & [49, 53] & 48.8\% & --- \\
\bottomrule
\end{tabular}
\caption{Technique effectiveness measured as percentage of baseline. 100\% = response matches unanchored judgment. Full SACD is closest to baseline (93.7\%, 95\% CI [92, 95]). Devil's Advocate keeps responses at 63.6\% of baseline (95\% CI [62, 65])---the CIs do not overlap with Full SACD, confirming the ranking difference is statistically reliable. $^\dagger$Outside View confounded.}
\label{tab:baseline-pct}
\end{table}

% Percentage of baseline visualization
% Figure removed - redundant with Figure 1 (fig:metric-comparison)

\subsection{Model-Specific Results: Full SACD}

Full SACD shows high variance across models:

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Model & \% of Baseline & 95\% CI & Deviation & Assessment \\
\midrule
\textbf{DeepSeek-v3.2} & \textbf{100.8\%} & [98, 103] & \textbf{0.8\%} & Near-perfect \\
Kimi-k2.5 & 100.9\% & [97, 105] & 0.9\% & Near-perfect \\
o3 & 92.0\% & [91, 93] & 8.0\% & Good \\
Sonnet 4.6 & 91.9\% & [90, 93] & 8.1\% & Good \\
GPT-4.1 & 90.8\% & [89, 93] & 9.2\% & Good \\
o4-mini & 79.5\% & [78, 81] & 20.5\% & Undershoot \\
GPT-5.2 & 122.4\% & [118, 126] & 22.4\% & Overshoot \\
GLM-5 & 123.1\% & [120, 126] & 23.1\% & Overshoot \\
Opus 4.6 & 127.8\% & [123, 132] & 27.8\% & Significant overshoot \\
\textbf{Haiku 4.5} & \textbf{47.8\%} & [46, 50] & \textbf{52.2\%} & Severe undershoot \\
\bottomrule
\end{tabular}
\caption{Full SACD model-specific results (percentage of baseline). 95\% CIs from bootstrap. DeepSeek and Kimi achieve near-perfect debiasing ($\sim$100\%). Several models overshoot significantly, while Haiku severely undershoots (47.8\%---SACD makes it worse).}
\label{tab:sacd-by-model}
\end{table}

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    xbar,
    bar width=8pt,
    width=0.95\columnwidth,
    height=7cm,
    xlabel={\% of Baseline},
    xmin=40,
    xmax=140,
    ytick=data,
    yticklabels={Haiku 4.5, o4-mini, o3, Sonnet 4.6, GPT-4.1, DeepSeek-v3.2, Kimi-k2.5, GPT-5.2, GLM-5, Opus 4.6},
    y dir=reverse,
    nodes near coords,
    nodes near coords align={horizontal},
    every node near coord/.append style={font=\footnotesize},
    enlarge y limits=0.08,
    axis lines*=left,
    xmajorgrids=true,
    grid style={dashed,gray!30},
]
% Red = severe undershoot (Haiku 47.8%)
\addplot[fill=red!70, draw=black] coordinates {(47.8, 1)};
% Orange = undershoot >10% (o4-mini 79.5%)
\addplot[fill=orange!60, draw=black] coordinates {(79.5, 2)};
% Blue = 5-10% deviation (o3, Sonnet, GPT-4.1)
\addplot[fill=blue!50, draw=black] coordinates {(92.0, 3) (91.9, 4) (90.8, 5)};
% Green = within 5% (DeepSeek, Kimi)
\addplot[fill=green!60!black, draw=black] coordinates {(100.8, 6) (100.9, 7)};
% Orange = overshoot >10% (GPT-5.2, GLM-5, Opus)
\addplot[fill=orange!60, draw=black] coordinates {(122.4, 8) (123.1, 9) (127.8, 10)};
\draw[dashed, red, thick] (axis cs:100,0.5) -- (axis cs:100,10.5);
\end{axis}
\end{tikzpicture}
\caption{Full SACD by model (percentage of baseline). Dashed line = 100\% (perfect). Green = within 5\% of baseline. Blue = 5--10\% deviation. Orange = $>$10\% over/undershoot. Red = severe undershoot (Haiku at 47.8\%).}
\label{fig:sacd-by-model}
\end{figure}

Key findings:
\begin{enumerate}
    \item \textbf{DeepSeek and Kimi achieve near-perfect debiasing} ($\sim$100\% of baseline)
    \item \textbf{Several models overshoot} --- responses go past baseline (122--128\%)
    \item \textbf{Haiku 4.5 severely undershoots} --- SACD makes it worse (47.8\%)
    \item \textbf{High variance}: best = 0.8\% deviation, worst = 52.2\%
\end{enumerate}

\subsection{Asymmetry: High vs.\ Low Anchor}

Aggregate results hide an important asymmetry. Breaking down by anchor direction reveals that \textbf{all techniques correct high anchors better than low anchors}:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Technique & Low Anchor & 95\% CI & High Anchor & 95\% CI & Asymmetry \\
\midrule
Full SACD & 75.7\% & [73, 78] & 112.0\% & [109, 115] & 36.3 pp \\
Premortem & 69.0\% & [68, 70] & 114.2\% & [112, 117] & 45.2 pp \\
Random Control & 63.4\% & [62, 65] & 93.5\% & [90, 96] & 30.1 pp \\
Devil's Advocate & 51.8\% & [50, 53] & 75.5\% & [73, 78] & 23.7 pp \\
\bottomrule
\end{tabular}
\caption{Technique effectiveness by anchor direction. 95\% CIs from bootstrap. All techniques show asymmetry---high anchors are corrected more than low anchors. Full SACD shows bidirectional deviation: it undershoots from low anchors (75.7\%) and overshoots from high anchors (112.0\%).}
\label{tab:anchor-asymmetry}
\end{table}

\textbf{Key insight:} SACD's aggregate 93.7\% results from averaging over bidirectional deviation. From low anchors, it undershoots (75.7\%); from high anchors, it overshoots (112.0\%). The average is close to 100\%, but individual trials deviate in predictable directions.

\textbf{Devil's Advocate fails in both directions} but stays consistently below baseline (52--76\%), explaining its low susceptibility (small spread) despite poor baseline alignment.

\subsection{Mixed Effects Analysis}

To account for non-independence of observations within models, we fit a linear mixed effects model:

\begin{equation}
y_{ij} = \beta_0 + \beta_{\text{technique}} + u_j + \epsilon_{ij}
\end{equation}

where $y_{ij}$ is the \% of baseline for trial $i$ in model $j$, $\beta_{\text{technique}}$ is the fixed effect for technique (reference: grand mean), $u_j \sim N(0, \sigma^2_u)$ is the random intercept for model $j$, and $\epsilon_{ij} \sim N(0, \sigma^2_\epsilon)$ is the residual error. Analysis includes 8,958 trials across 10 models and 4 techniques (excluding Outside View due to confound).

The intraclass correlation coefficient (ICC) is 0.17:
\begin{equation}
\text{ICC} = \frac{\sigma^2_u}{\sigma^2_u + \sigma^2_\epsilon} = \frac{294.9}{294.9 + 1411.1} = 0.17
\end{equation}

This indicates that \textbf{17\% of variance} in \% of baseline is attributable to model differences.

\textbf{Fixed effects} (technique, relative to grand mean of 81.8\%):
\begin{itemize}
    \item Full SACD: $+11.9$ pp (93.7\% of baseline)
    \item Premortem: $+9.8$ pp (91.6\%)
    \item Random Control: $-3.5$ pp (78.3\%)
    \item Devil's Advocate: $-18.2$ pp (63.6\%)
\end{itemize}

The ranking is robust after accounting for model-level variance. A model with random slopes for technique would capture model $\times$ technique interactions (e.g., SACD works differently on Haiku vs.\ Opus); our per-model breakdowns in Table~\ref{tab:sacd-by-model} and Figure~\ref{fig:sacd-by-model} provide this detail descriptively. The ICC justifies our recommendation to test per-model before deployment.

\subsection{The Metric Inversion}

Table~\ref{tab:metric-comparison} confirms the divergence: Devil's Advocate ranks best on susceptibility but worst on baseline; SACD shows the opposite pattern. The bidirectional deviation in Table~\ref{tab:anchor-asymmetry} reveals that SACD's 93.7\% average results from direction-dependent over/under-correction.

\subsection{The SACD vs.\ Premortem Tradeoff}

Even within baseline-aware evaluation, two reasonable metrics give \textbf{opposite recommendations}:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Metric & SACD & Premortem & Winner \\
\midrule
Average response deviation from 100\% & 6.3\% & 8.4\% & SACD \\
Mean absolute per-trial error & 18.1\% & 22.6\% & SACD \\
\bottomrule
\end{tabular}
\caption{Two metrics for ``closeness to baseline.'' SACD's bidirectional deviation (75.7\% low, 112.0\% high) averages to 93.7\%---close to 100\%. Per-trial error is also lower for SACD (18.1\% vs.\ 22.6\%). Both metrics favor SACD, unlike our earlier analysis which showed metric-dependent recommendations.}
\label{tab:sacd-premortem}
\end{table}

\textbf{Practitioner guidance:} Both metrics favor SACD over Premortem. However, model-specific variation is substantial (Haiku at 47.8\% vs.\ DeepSeek at 100.8\%)---per-model testing is essential before deployment.

This analysis is only possible by collecting baselines and examining per-anchor results.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Why Full SACD Works (and Fails)}

Full SACD achieves 93.7\% of baseline (closest to 100\%) but shows the highest model variance (48--128\%). We propose:

\textbf{Hypothesis 1: Iterative reflection enables genuine reconsideration.} Multiple rounds of ``examine your reasoning'' prompts may help models escape local optima in their reasoning chains.

\textbf{Hypothesis 2: Some models perform ``debiasing theater.''} Opus 4.6 overshoots to 127.8\% of baseline (27.8\% deviation), suggesting the technique can activate surface compliance without genuine reconsideration---the model may be optimizing for \textit{appearing} to reconsider rather than actually doing so.

\textbf{Hypothesis 3: Baseline proximity matters.} Opus 4.6 has the lowest baseline (18mo), meaning SACD may be pulling it \textit{away} from its natural judgment toward a perceived ``expected answer.''

\textbf{Hypothesis 4: Haiku's severe undershoot (47.8\%).} Unlike most models that overshoot, Haiku undershoots dramatically, suggesting SACD can backfire entirely for some model architectures.

\subsection{Theoretical Grounding}

Recent theoretical work helps explain our empirical findings:

\textbf{Positional encoding breaks exchangeability.} \citet{llm-bayesian-2025} show that LLMs are ``Bayesian in expectation, not in realization''---the same evidence presented in different orders yields different posteriors due to positional encoding effects. This may explain SACD's model-dependent effectiveness: iterative self-reflection changes the \textit{order} of reasoning steps, and models with stronger positional biases (potentially Haiku) may amplify rather than correct errors through repeated passes.

\textbf{Self-judgment induces overconfidence.} \citet{llm-judge-overconfidence-2025} demonstrate that LLMs systematically overstate confidence when judging their own outputs. Their proposed fix---an ensemble ``Fuser'' approach where models synthesize external perspectives rather than self-evaluate---aligns with our finding that external-challenge techniques (Devil's Advocate, Premortem) show more consistent debiasing than internal-iteration techniques (SACD). The ``ironic process'' we observe in SACD may be a manifestation of this overconfidence: extended reasoning produces outputs that \textit{sound} more considered while actually drifting further from calibrated judgment.

These theoretical accounts suggest a unified mechanism: more sequential reasoning passes create more opportunities for positional biases and self-reinforcing confidence, explaining why SACD's effectiveness varies dramatically across model architectures while simpler external-challenge techniques show more robust (if modest) improvements.

\subsection{Per-Trial Distribution Analysis}

Aggregate means can mask important distributional properties. Examining individual trial distributions reveals:

\begin{itemize}
    \item \textbf{Devil's Advocate compresses variance} toward the wrong target: SD = 34.6, median = 69\%, only 11\% of trials within $\pm$10\% of baseline.
    \item \textbf{Premortem shows highest baseline proximity}: 13.9\% of trials within $\pm$10\% of baseline, though with higher variance (SD = 41.9).
    \item \textbf{All techniques show positive skew}: trials cluster below baseline with a long tail above. This suggests anchoring effects are asymmetric at the individual trial level, not just in aggregate.
\end{itemize}

The compression phenomenon explains Devil's Advocate's favorable susceptibility score---but compression toward 67\% of baseline is not useful.

\subsection{Why Random Control Works}

Random Control (78.3\% of baseline) outperforms Devil's Advocate (63.6\%) despite having no debiasing content. \textbf{This condition serves as a critical ablation:} Full SACD and Premortem are multi-turn techniques, so any improvement could stem from either (a) the debiasing content or (b) the multi-turn structure itself. Random Control isolates (b)---it uses additional turns with neutral, non-debiasing content.

Both mechanisms contribute: structure provides partial correction (Random Control at 78.3\%), and debiasing content adds further benefit (SACD at 93.7\%). The 15.4 percentage point difference represents the contribution of debiasing content beyond structural effects.

\textbf{Direct comparison:} Random Control (78.3\%) outperforms Devil's Advocate (63.6\%) by 14.7 percentage points. Cohen's $d = 0.39$ (small-to-medium), suggesting the practical difference is meaningful---structure alone helps more than Devil's Advocate content.

\subsection{The Outside View Confound}
\label{sec:outside-view-confound}

Outside View performed worst despite recommendations in human debiasing literature. Our prompts required jurisdiction specification (``German federal courts'') to avoid safety refusals, likely introducing a secondary anchor toward German norms ($\sim$12--18 months). Baselines without this context ranged 18--36 months; Outside View pulled toward $\sim$15 months.

\textbf{Practitioner implication:} Reference classes may import unintended anchors.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Single vignette.} All experiments use one judicial sentencing case (Lena M., 12th shoplifting offense). While we achieve statistical power through repetition, findings may not generalize to other case types or anchoring domains. Replication across multiple vignettes is needed.
    
    \item \textbf{Proportional anchor design.} Our anchors scale with each model's baseline (high = baseline $\times$ 1.5, low = baseline $\times$ 0.5). This design choice introduces a potential circularity: we use baseline to set anchors, then measure response as \% of baseline. However, the anchoring phenomenon itself is not circular---models are genuinely influenced by the anchor values they receive. The circularity concern applies only to cross-model comparison of anchor ``strength,'' which we address by reporting within-model effects alongside aggregates. Future work should validate findings with fixed absolute anchors.
    
    \item \textbf{Metric divergence holds without Outside View.} While Outside View shows the most dramatic divergence, the core finding---that metrics give opposite rankings---holds even excluding it. Without Outside View: Devil's Advocate ranks \textit{best} on susceptibility (lowest spread) but \textit{worst} on \% of baseline (63.6\%); Full SACD shows higher susceptibility but \textit{best} on \% of baseline (93.7\%). The divergence is robust.
    
    \item \textbf{Outside View confound.} See Section~\ref{sec:outside-view-confound}. Future work should test jurisdiction-neutral prompts.
    
    \item \textbf{Baseline interpretation.} Our baseline still includes numeric context (``12th offense''); it is ``without explicit anchor,'' not truly ``unanchored.'' We measure proximity to the model's considered judgment, not an objective ground truth---which does not exist for sentencing decisions.
    
    \item \textbf{Model coverage.} 10 models from 4 providers is substantial but not exhaustive. Results may not apply to other model families. \textbf{Sensitivity analysis:} Excluding Opus 4.6 (which shows zero baseline variance) shifts all technique means by 2--3 percentage points but preserves rankings: SACD \#1 (93.4\%), Premortem \#2 (89.7\%), Random Control \#3 (77.0\%), Devil's Advocate \#4 (61.2\%).
    
    \item \textbf{Stopping rule.} We targeted $n \geq 30$ per condition based on central limit theorem requirements for normal approximation. We did not use adaptive stopping based on effect size stabilization. However, our bootstrap CIs provide valid inference regardless of stopping rule, and effect sizes (Cohen's $d > 0.5$ for key comparisons) suggest adequate power.
\end{enumerate}

\subsection{Practical Recommendations}

Based on our findings in the judicial sentencing domain (generalization to other domains requires validation):

\begin{enumerate}
    \item \textbf{Consider structural interventions.} Adding conversation turns (Random Control, +15pp over Devil's Advocate) provides meaningful improvement with minimal prompt engineering.
    \item \textbf{Test per-model.} Technique effectiveness varies substantially across models; SACD ranges from 48\% (Haiku) to 128\% (Opus) of baseline.
    \item \textbf{Collect baselines.} We propose \% of baseline as a complementary metric to susceptibility. Measuring how close responses are to the model's unprompted judgment catches techniques that appear effective but keep responses anchored at wrong values.
    \item \textbf{Be cautious with reference class prompts.} See Section~\ref{sec:outside-view-confound}.
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

We demonstrated that \textbf{metric choice determines technique recommendation}. Susceptibility (spread reduction) and \% of baseline give divergent rankings:

\begin{itemize}
    \item \textbf{Devil's Advocate}: Best on susceptibility (lowest spread), worst on baseline (63.6\%)
    \item \textbf{Full SACD}: Higher susceptibility but best on baseline (93.7\%)
\end{itemize}

Devil's Advocate produces consistent responses that are \textit{consistently wrong}---anchored at 63.6\% of where they should be. Full SACD achieves 93.7\% of baseline (closest to correct), with bidirectional deviation: from low anchors it reaches 75.7\%, from high anchors 112.0\%.

\textbf{Both metrics now favor SACD over Premortem:}
\begin{itemize}
    \item \textbf{Average response deviation}: SACD's average (93.7\%) is 6.3\% from baseline; Premortem's (91.6\%) is 8.4\% from baseline. \textbf{SACD wins.}
    \item \textbf{Average per-trial error}: SACD's mean absolute deviation is 18.1\%; Premortem's is 22.6\%. \textbf{SACD wins.}
\end{itemize}

However, model-specific variation is substantial (Haiku at 47.8\% vs.\ DeepSeek at 100.8\%), making per-model testing essential.

Our key findings from 14,154 trials across 10 models:

\begin{enumerate}
    \item \textbf{Metric divergence}: Devil's Advocate ranks \#1 on susceptibility but \#4 on \% of baseline. SACD shows the opposite pattern.
    \item \textbf{Bidirectional deviation}: SACD's 93.7\% average results from 75.7\% (low anchor) and 112.0\% (high anchor)---direction-dependent over/under-correction.
    \item \textbf{High model variance}: SACD ranges from 48\% (Haiku) to 128\% (Opus). Practitioners must test per-model.
    \item \textbf{Baseline collection is essential}: Without it, both the ``consistently wrong'' failure (Devil's Advocate) and model-specific failures (Haiku's 47.8\%) are invisible.
\end{enumerate}

For practitioners: test techniques per-model before deployment. Measure responses as \% of baseline, not just susceptibility.

For researchers: collect baselines without explicit anchors. The standard high-vs-low methodology has a blind spot that can recommend techniques that produce consistent but incorrect responses.

%==============================================================================
\appendix
\section{Prompt Templates}
\label{app:prompts}

\subsection{Case Vignette (All Conditions)}

\begin{quote}
\small
You are a trial judge in a criminal court. Read the case summary and determine an appropriate sentence.

Case summary:
\begin{itemize}
\item The defendant, Lena M., has been convicted of shoplifting.
\item This is her 12th shoplifting offense.
\item She stole several items from a supermarket.
\item A psycho-legal expert has provided an advisory opinion on the case.
\item Statements from the defendant and a witness have been reviewed.
\item Based on the applicable penal code provisions, sentences for this type of offense typically involve probation.
\end{itemize}
\end{quote}

\subsection{Anchor Introduction (High/Low Anchor Conditions)}

\begin{quote}
\small
For experimental purposes, the following prosecutor's sentencing demand was randomly determined, therefore, it does not reflect any judicial expertise:

The prosecutor demands as a sentence for the accused Lena M. \texttt{[ANCHOR]} months on probation.
\end{quote}

\subsection{Outside View (As Implemented)}

\begin{quote}
\small
In German federal courts, what is the TYPICAL probation sentence for a defendant with 12 prior shoplifting offenses?
\end{quote}

\textit{Note: Jurisdiction specification was required to avoid model safety refusals but may have introduced a secondary anchor.}

\subsection{Full SACD (Iterative Self-Administered Cognitive Debiasing)}

Following \citet{lyu2025}, Full SACD implements three iterative steps:

\begin{enumerate}
\item \textbf{Bias Determination}: ``Analyze the following prompt... For each sentence, determine if it contains a cognitive bias''
\item \textbf{Bias Analysis}: If bias detected, classify the type (anchoring, framing, etc.)
\item \textbf{Cognitive Debiasing}: ``Rewrite the flagged sentences to remove the bias''
\end{enumerate}

Steps repeat until no bias is detected or maximum iterations (5) reached. Average iterations to convergence: 2.3.

\subsection{Random Control}

Random Control prompts consisted of unrelated elaboration requests (e.g., ``Describe the courtroom setting in detail'') designed to add conversation turns without debiasing content.

%==============================================================================
\section*{Data and Code Availability}

All trial data, analysis scripts, and prompts are available at \url{https://github.com/voder-ai/bAIs}. The repository includes raw JSONL trial data for all 14,154 trials, statistical analysis scripts reproducible from raw data, complete prompts for all debiasing techniques, and response distributions by model and condition.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
