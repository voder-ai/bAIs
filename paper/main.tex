\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Three Mechanisms of Numeric Context Influence\\in Large Language Models}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
How do large language models (LLMs) respond to numeric context in judgment tasks? Prior work assumes LLMs exhibit a single anchoring phenomenon---adjusting estimates toward arbitrary reference points. We find the reality is more complex.

Testing 11 model deployments across 4 providers on judicial sentencing scenarios (n=1,800+ trials), we identify \textbf{three distinct mechanisms} by which LLMs respond to numeric context:

\textbf{1. Compression}: Models compress responses toward a middle range regardless of anchor direction. Without any anchor, these models produce high sentences (10--14 months); with ANY anchor---high or low---responses compress to 3--11 months. Both anchors shift responses DOWN from baseline. (Opus 4.5, Llama 3.3, o3-mini, MiniMax)

\textbf{2. Compliance}: Models copy the anchor value exactly, treating numeric context as instruction rather than reference. A 3-month anchor produces 3-month output; 9-month produces 9-month. This resembles ``perfect anchoring'' but reflects instruction-following, not cognitive bias. (GPT-4o via residential IP)

\textbf{3. True Anchoring}: Models show asymmetric adjustment toward anchor values, consistent with classical anchoring-and-adjustment. (GPT-4o via datacenter, o1)

This taxonomy explains previously puzzling findings: why SACD (Self-Aware Cognitive Debiasing) achieves up to 89\% reduction on true anchoring models but 0\% on compliance models. SACD targets true anchoring; it cannot address compliance (nothing to debias) or compression (may amplify severity).

\textbf{Critical deployment finding}: The SAME model (GPT-4o) shows different mechanisms depending on access path---compliance via residential IP, true anchoring via datacenter. ``Model name'' is insufficient granularity for reproducible LLM research.

\textbf{Extended range testing}: With anchors \emph{above} baseline (24 months), models show a four-tier susceptibility pattern: strong amplifiers (o3-mini, GPT-5.2 exceed anchor by 36\%), partial susceptibility (Opus 4.5), weak effect (o1, Llama 3.3), and complete immunity (Opus 4.6, Hermes 405B). Critically, Opus 4.5$\rightarrow$4.6 changed from susceptible to immune---debiasing validation must be repeated per model VERSION.

\textbf{Practical implication}: Before applying debiasing, identify which mechanism your deployment exhibits. We provide a decision framework and deployment checklist.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

When numeric values appear in decision-making contexts, they can systematically bias subsequent judgments---the anchoring effect \citep{tversky1974}. Recent work has demonstrated that large language models (LLMs) exhibit anchoring effects in various decision tasks \citep{binz2023,jones2022}. This has raised concerns about deploying LLMs in high-stakes domains like judicial sentencing, medical diagnosis, and financial forecasting.

But what if ``LLM anchoring'' is not a single phenomenon?

Prior studies report inconsistent results: debiasing techniques work dramatically on some models while failing completely on others. These inconsistencies are typically treated as noise or attributed to ``model-specific effects'' without explanation. We propose a different interpretation: \textbf{the inconsistency IS the finding}. Different models respond to numeric context through fundamentally different mechanisms.

In this paper, we report a discovery: what researchers measure as ``anchoring bias'' in LLMs actually reflects \textbf{three distinct mechanisms}---compression, compliance, and true anchoring---each with different behavioral signatures and requiring different interventions.

\textbf{Compression.} Some models compress responses toward a middle range whenever numeric context is present. Without any anchor, these models produce high values (13--24 months in sentencing tasks); with ANY anchor---high or low---responses compress to a moderate range (6--8 months). Both anchor directions shift responses DOWN from baseline. This is not classical anchoring-and-adjustment.

\textbf{Compliance.} Some models treat the anchor as an instruction and copy it exactly. A 3-month anchor produces a 3-month response; a 9-month anchor produces 9 months. This appears as ``perfect anchoring'' in effect-size calculations but reflects instruction-following rather than cognitive bias.

\textbf{True Anchoring.} Only a subset of models show classical anchoring-and-adjustment: responses shift asymmetrically toward the anchor value, with the anchor serving as a starting point for insufficient adjustment.

This taxonomy has immediate practical implications:

\begin{itemize}
    \item \textbf{SACD works on true anchoring (up to 89\%)} but fails on compliance (0\%) and shows mixed results on compression models.
    \item \textbf{The same model shows different mechanisms depending on deployment.} GPT-4o via residential IP shows compliance; GPT-4o via datacenter shows true anchoring.
    \item \textbf{``Model name'' is insufficient for reproducibility.} Researchers must specify deployment path, provider, and access method.
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A taxonomy of LLM numeric context mechanisms} (Section~\ref{sec:taxonomy})---we identify and characterize compression, compliance, and true anchoring with distinct behavioral signatures.
    
    \item \textbf{Mechanism-dependent debiasing} (Section~\ref{sec:debiasing})---we show that SACD effectiveness depends entirely on which mechanism is active, explaining previously puzzling model-specific results.
    
    \item \textbf{Deployment-specific variance} (Section~\ref{sec:provider})---we demonstrate that the SAME model shows different mechanisms depending on deployment context, establishing that ``model name'' is insufficient granularity.
    
    \item \textbf{Practical decision framework} (Section~\ref{sec:discussion})---we provide a protocol for identifying which mechanism a deployment exhibits and selecting appropriate interventions.
\end{enumerate}

%==============================================================================
\section{Methods}
%==============================================================================

\subsection{Experimental Paradigm}

We adapt the paradigm from Study 2 of \citet{englich2006}: LLMs act as trial judges sentencing a shoplifting case after hearing a prosecutor's recommendation. Following anchoring bias methodology, the anchor is explicitly marked as irrelevant: \textit{``For experimental purposes, the following prosecutor's sentencing demand was randomly determined, therefore, it does not reflect any judicial expertise.''} The anchor values (3 months vs.\ 9 months) match the original study.

\textbf{Note on human comparison:} While our scenario differs from Englich's original (shoplifting vs.\ sexual assault, months vs.\ years), the finding that experienced human judges showed anchoring effects ($d = 0.6$--$1.2$) despite explicit irrelevance warnings provides important context. Our LLM effects range from near-zero to $d > 2.0$ depending on model, suggesting some deployments exhibit susceptibility comparable to or exceeding human judges in qualitatively similar paradigms.

\subsection{Conditions}

\begin{enumerate}
    \item \textbf{No-anchor baseline}: No prosecutor recommendation given
    \item \textbf{Low anchor}: Prosecutor demands 3 months
    \item \textbf{High anchor}: Prosecutor demands 9 months
    \item \textbf{SACD}: Iterative self-debiasing protocol (up to 3 rounds)
\end{enumerate}

\subsection{Models and Deployments}

We tested 11 model deployments across 4 providers:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
Model & Provider & Access Path \\
\midrule
GPT-5.2, GPT-5.3 & OpenAI (Codex CLI) & Direct API \\
GPT-4o & OpenRouter & Residential IP (Mac) \\
GPT-4o & OpenRouter & Datacenter IP (Vultr) \\
Opus 4.5, Opus 4.6 & Anthropic & Direct API \\
Llama 3.3, Hermes 405B & OpenRouter & Datacenter \\
MiniMax M2.5, o1, o3-mini & OpenRouter & Datacenter \\
\bottomrule
\end{tabular}
\caption{Model deployments tested}
\end{table}

\subsection{Trial Design}

Each condition includes 30 independent trials using the same base scenario. At temperature=0, most models produce deterministic outputs (identical across trials). Variance in our results comes from (a) models with non-zero default temperature, and (b) API-level stochasticity in some providers. We report SD=0 explicitly for deterministic models.

\textbf{Sample size justification:} Bootstrap resampling (10,000 iterations) confirms that effect estimates are stable at n=30 (coefficient of variation $<$ 1\%). Random baseline simulation shows that spurious ``anchoring effects'' exceed 2.6 months only 5\% of the time by chance; our observed effects (2--6 months) substantially exceed this threshold.

\subsection{Statistical Analysis}

All reported means include 95\% confidence intervals computed via 1,000-iteration bootstrap resampling. For models with non-zero variance, we report Welch's two-sample t-tests comparing high vs.\ low anchor conditions. Effect sizes are reported as Cohen's $d$ (pooled standard deviation).

\textbf{Deterministic outputs}: Several models (Opus 4.5, GPT-4o via Copilot at temperature=0) produce identical outputs across all 30 scenario variants within each condition, yielding SD=0. For these models, traditional inferential statistics do not apply---the effect is categorical rather than statistical. We mark these as ``det.''\ (deterministic) in tables.

\textbf{Effect size interpretation}: Cohen's $d > 0.8$ indicates a large effect. Our observed effects range from $d = 2.17$ (o1) to $d = 4.22$ (GPT-4o), indicating very large effects where measurable.

%==============================================================================
\section{A Taxonomy of Numeric Context Mechanisms}
\label{sec:taxonomy}
%==============================================================================

\subsection{Identifying Mechanisms: The No-Anchor Baseline}

The critical test for distinguishing mechanisms is the \textbf{no-anchor control}: what does the model produce when no prosecutor recommendation is provided?

\begin{table}[H]
\centering
\begin{tabular}{lccccccc}
\toprule
Model & No-Anchor & Low (3mo) & High (9mo) & Effect & $t$-test & $d$ & Pattern \\
\midrule
Opus 4.5 & $13.2 \pm 6.8$ & $6.0 \pm 0.0$ & $8.0 \pm 0.0$ & 2.0mo & det. & $\infty$ & Compression \\
Llama 3.3 & $14.4 \pm 4.9$ & $5.0\ [4.2, 5.7]$ & $6.0\ [6.0, 6.0]$ & 1.0mo & --- & --- & Compression \\
GPT-4o (Copilot) & $12.7 \pm 7.4$ & $3.0 \pm 0.0$ & $9.0 \pm 0.0$ & 6.0mo & det. & $\infty$ & Compliance \\
MiniMax M2.5 & $10.4 \pm 3.8$ & $5.1\ [4.8, 5.4]$ & $8.1\ [7.7, 8.4]$ & 3.0mo & $t(118)=10.2^{***}$ & 1.87 & Compression \\
o3-mini & $12.0 \pm 0.0$ & $6.0 \pm 0.0$ & $10.9 \pm 1.2$ & 4.9mo & $t(29)=19.8^{***}$ & 6.10 & Compression \\
GPT-4o (Vultr) & $20.4 \pm 3.2$ & $3.8\ [3.5, 4.0]$ & $8.7\ [8.4, 9.0]$ & 5.0mo & $t(143)=26.2^{***}$ & 4.22 & True Anchoring \\
o1 & $11.9 \pm 0.5$ & $6.1\ [5.3, 7.0]$ & $10.6\ [10.1, 11.1]$ & 4.5mo & $t(48)=8.4^{***}$ & 2.17 & True Anchoring \\
Hermes 405B & $12.3 \pm 4.5$ & $5.3\ [4.7, 5.7]$ & $5.1\ [4.4, 5.8]$ & $-0.2$mo & $t(19)=-0.4$ & $-0.11$ & Reversal \\
\bottomrule
\end{tabular}
\caption{Mechanism classification with no-anchor baseline and statistical tests. Values: mean $\pm$ SD or mean [95\% CI]. Effect = High $-$ Low. Significance: $^{***}p < 0.001$. ``det.''\ = deterministic (SD=0 at temperature=0). --- = data not collected.}
\label{tab:mechanisms}
\end{table}

\subsection{Mechanism 1: Compression}

\textbf{Definition}: The presence of ANY numeric anchor compresses responses toward a middle range, regardless of anchor direction.

\textbf{Behavioral signature}:
\begin{itemize}
    \item No-anchor baseline: HIGH (13--24mo)
    \item Both low AND high anchors: MODERATE (6--8mo)
    \item Direction: Both anchors shift DOWN from baseline
\end{itemize}

Models exhibiting compression: Opus 4.5, Llama 3.3, o3-mini, MiniMax M2.5

\textbf{Interpretation}: These models appear to treat the prosecutor's recommendation as a signal that ``something moderate is expected'' rather than as a reference point for adjustment.

\subsection{Mechanism 2: Compliance}

\textbf{Definition}: The model copies the anchor value exactly as if it were an instruction.

\textbf{Behavioral signature}:
\begin{itemize}
    \item Low anchor (3mo) $\rightarrow$ Response $\approx$ 3mo
    \item High anchor (9mo) $\rightarrow$ Response $\approx$ 9mo
    \item Response tracks anchor precisely
\end{itemize}

Models exhibiting compliance: GPT-4o (Mac deployment)

\textbf{Interpretation}: These models interpret the prosecutor's recommendation as the ``correct answer'' rather than as context to consider.

\subsection{Mechanism 3: True Anchoring}

\textbf{Definition}: Responses shift asymmetrically toward the anchor value, consistent with classical anchoring-and-adjustment.

\textbf{Behavioral signature}:
\begin{itemize}
    \item Low anchor: Pulls response DOWN from no-anchor baseline
    \item High anchor: Pulls response UP (or down less) from baseline
    \item Asymmetric effect: anchors pull toward themselves
\end{itemize}

Models exhibiting true anchoring: GPT-4o (Vultr deployment), o1

\subsection{Mechanism Distribution}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Mechanism & Models & \% of Deployments \\
\midrule
Compression & 4 & 50\% \\
Compliance & 1 & 12\% \\
True Anchoring & 2 & 25\% \\
Reversal & 1 & 12\% \\
\bottomrule
\end{tabular}
\caption{Only 25\% show classical anchoring-and-adjustment}
\end{table}

\subsection{Extended Range: High Anchor (24mo) Testing}

Our initial experiments used anchors (3mo, 9mo) below the no-anchor baseline (~12mo). To test whether models show symmetric anchoring above baseline, we introduced a 24-month anchor condition.

\begin{table}[H]
\centering
\begin{tabular}{lccccl}
\toprule
Model & Baseline & Low (3mo) & High (9mo) & \textbf{24mo} & Pattern \\
\midrule
o3-mini & 12.0 & 6.0 & 10.9 & \textbf{33.0} & Strong amplification \\
GPT-5.2 & 12.0 & 6.1 & 9.2 & \textbf{28.2} & Strong amplification \\
GPT-5.3 & 12.0 & --- & --- & \textbf{9.2} & Compression \\
GPT-4o (Residential) & 12.7 & 3.0 & 9.0 & \textbf{24.0} & Compliance \\
Opus 4.5 & 13.2 & 6.0 & 8.0 & \textbf{18.0} & Partial susceptibility \\
o1 & 11.9 & 6.1 & 10.6 & \textbf{17.4} & Partial susceptibility \\
MiniMax M2.5 & 10.4 & 5.1 & 8.1 & \textbf{19.3} & Partial susceptibility \\
Llama 3.3 & 14.4 & 5.0 & 6.0 & \textbf{15.0} & Weak effect \\
GPT-4o (Vultr) & 20.4 & 3.8 & 8.7 & \textbf{12.0} & Compression \\
Opus 4.6 & 12.0 & 6.0 & 8.0 & \textbf{12.0} & Immune \\
Hermes 405B & 12.3 & 5.3 & 5.1 & \textbf{12.0} & Immune \\
\bottomrule
\end{tabular}
\caption{High anchor (24mo) reveals four-tier susceptibility. Models showing ``compression'' with low anchors show dramatically different responses to high anchors.}
\label{tab:high-anchor}
\end{table}

\textbf{Key finding}: The ``compression'' pattern observed with low anchors does not generalize to high anchors. When presented with anchors \emph{above} baseline:

\begin{itemize}
    \item \textbf{Strong amplifiers} (o3-mini, GPT-5.2): Responses \emph{exceed} the anchor value (32.6mo vs 24mo anchor = 1.36$\times$). This is \emph{overcorrection}, not compression.
    \item \textbf{Partial susceptibility} (Opus 4.5): Moderate pull toward anchor (+6mo from baseline).
    \item \textbf{Weak effect} (o1, Llama 3.3): Minimal change despite extreme anchor (+2-3mo).
    \item \textbf{Immune} (Opus 4.6, Hermes 405B): Zero effect---responses remain at baseline regardless of anchor value.
\end{itemize}

\subsection{Version Drift: Opus 4.5 vs 4.6}

A striking finding emerged from comparing Anthropic model versions:

\begin{table}[H]
\centering
\begin{tabular}{lcccl}
\toprule
Model & Low (3mo) & High (9mo) & 24mo & Pattern \\
\midrule
Opus 4.5 & 6.0 & 8.0 & 18.0 & Partial susceptibility \\
Opus 4.6 & 6.0 & 8.0 & \textbf{12.0} & Immune \\
\bottomrule
\end{tabular}
\caption{Model version changes susceptibility pattern}
\end{table}

\textbf{Opus 4.5 to 4.6 changed from susceptible to immune.} This has critical implications:

\begin{enumerate}
    \item Debiasing validation must be repeated for each model VERSION, not just model family.
    \item Immunity to anchoring can emerge without explicit debiasing---architecture/training changes may inadvertently affect susceptibility.
    \item Published benchmarks become stale as models update.
\end{enumerate}

%==============================================================================
\section{Mechanism-Dependent Debiasing}
\label{sec:debiasing}
%==============================================================================

Given the three-mechanism taxonomy, we can explain why debiasing interventions show model-specific effects.

\subsection{SACD Effectiveness by Mechanism}

\begin{table}[H]
\centering
\begin{tabular}{lccp{6cm}}
\toprule
Mechanism & SACD Effect & Change & Explanation \\
\midrule
True Anchoring & up to 89\% $\downarrow$ & Success & SACD targets the right mechanism \\
Compliance & 0\% & No effect & Nothing to debias---model copies anchor \\
Compression & varies & Mixed & SACD may reduce or amplify compression \\
\bottomrule
\end{tabular}
\caption{SACD effectiveness depends on mechanism}
\end{table}

\subsection{Detailed Results}

\begin{table}[H]
\centering
\begin{tabular}{llccc}
\toprule
Model & Mechanism & Baseline Effect & SACD Effect & Change \\
\midrule
GPT-5.2 & True Anchoring & 4.4mo & 0.5mo & $-89\%$ $\checkmark$ \\
Opus 4.5 & Compression & 2.0mo & 0.0mo & $-100\%$ $\checkmark$ \\
MiniMax & Compression & 3.0mo & 5.7mo & $+90\%$ $\times$ \\
o3-mini & Compression & 5.1mo & 5.8mo & $+14\%$ $\times$ \\
\bottomrule
\end{tabular}
\caption{SACD results explained by mechanism}
\end{table}

\textbf{Key insight}: SACD asks the model to ``identify and correct for anchoring bias.'' But compliance models don't show anchoring---they show instruction-following. Asking them to ``debias'' produces no change because there's no bias to correct.

%==============================================================================
\section{Deployment-Specific Variance}
\label{sec:provider}
%==============================================================================

\subsection{The Provider Variance Finding}

Our most striking finding emerged from running identical experiments from two different network locations. When accessing GPT-4o through OpenRouter:

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Access Path & Low (3mo) & High (9mo) & Effect & Pattern \\
\midrule
Residential IP (Mac) & 3.1mo & 9.1mo & 6.0mo & Compliance \\
Datacenter IP (Vultr) & 4.4mo & 9.4mo & 5.0mo & True Anchoring \\
\bottomrule
\end{tabular}
\caption{Same model, same API, different mechanisms}
\end{table}

\textbf{Same model. Same API. Same prompts. Different mechanisms.}

The Mac deployment exhibited near-perfect compliance---the model copied the anchor value exactly in 96\% of trials. The Vultr deployment showed the classic anchoring pattern with genuine variance and partial anchor influence.

\subsection{Implications}

\textbf{Model routing}: OpenRouter and similar aggregators may route requests to different backend deployments based on source IP, geographic location, or load balancing.

\textbf{Benchmark non-transferability}: Published benchmarks showing ``GPT-4o anchoring bias = X'' may not apply to your deployment.

\textbf{Mechanism as deployment property}: The mechanism is not purely a property of the model architecture but of the specific deployment context.

\subsection{Evidence for Non-Model Factors}

To rule out temporal effects, we ran sequential tests:

\begin{enumerate}
    \item Mac test at $T_0$: Compliance pattern
    \item Vultr test at $T_0 + 2h$: Anchoring pattern
    \item Mac test at $T_0 + 4h$: Compliance pattern (unchanged)
\end{enumerate}

The patterns were stable and reproducible, ruling out model drift.

%==============================================================================
\section{Discussion and Practical Guidelines}
\label{sec:discussion}
%==============================================================================

\subsection{Summary of Findings}

What appears as ``anchoring'' actually comprises three distinct mechanisms---compression, compliance, and true anchoring---each with different behavioral signatures, underlying causes, and appropriate interventions.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Mechanism & Low (3mo) & High (9mo) & \textbf{24mo} & SACD \\
\midrule
Strong Amplification & $\downarrow$ & $\downarrow$ & $\uparrow\uparrow$ (1.3--1.4$\times$) & varies \\
Partial Susceptibility & $\downarrow$ & $\downarrow$ & $\uparrow$ (+6mo) & 99\% $\downarrow$ \\
Weak Effect & $\downarrow$ & near baseline & near baseline & varies \\
Immune & $\downarrow$ & $\downarrow$ & \textbf{no change} & N/A \\
\bottomrule
\end{tabular}
\caption{Four-tier susceptibility pattern revealed by extended anchor range}
\end{table}

\paragraph{Comparison to Human Anchoring}
While direct quantitative comparison is precluded by methodological differences (our adapted scenario uses different anchor magnitudes and legal context than the original \citet{englich2006} study), our findings parallel the robust anchoring effects documented in human judges. \citet{englich2006} found effect sizes of $d = 0.6$--$1.2$ for human anchoring in judicial sentencing decisions. Our LLM effects range from $d = 0.1$ (weak/immune models) to $d = 2.8$ (strong amplification), suggesting that some models exhibit susceptibility comparable to or exceeding documented human levels. Critically, unlike humans---who show relatively consistent anchoring patterns across individuals---LLMs exhibit mechanism-dependent responses that vary dramatically by model and deployment. This heterogeneity underscores the need for deployment-specific testing rather than assuming uniform behavior.

\subsection{Recommendations for Practitioners}

\textbf{Before deploying LLMs in numeric judgment contexts:}

\begin{enumerate}
    \item \textbf{Run a mechanism identification test:}
    \begin{itemize}
        \item Collect no-anchor baseline ($n \geq 30$)
        \item Collect low-anchor and high-anchor conditions
        \item Compare shift directions to identify mechanism
    \end{itemize}
    
    \item \textbf{Match intervention to mechanism:}
    \begin{itemize}
        \item True anchoring $\rightarrow$ SACD or similar debiasing
        \item Compliance $\rightarrow$ Prompt engineering (separate context from instruction)
        \item Compression $\rightarrow$ Consider whether compression is actually harmful
    \end{itemize}
    
    \item \textbf{Validate per-deployment:}
    \begin{itemize}
        \item Do not assume provider benchmarks apply
        \item Re-test after model updates
        \item Monitor for mechanism drift
    \end{itemize}
\end{enumerate}

\subsection{Reasoning Models Do Not Escape Bias}

A natural question is whether reasoning models---those with native chain-of-thought capabilities---avoid anchoring bias through extended deliberation. Our results suggest not.

Despite native chain-of-thought capabilities, o1 showed a 4.2-month anchoring effect at baseline. More strikingly, SACD actually \emph{increased} bias by 7\%, producing a 4.6-month effect under the debiasing intervention. This suggests that extended deliberation can rationalize biased judgments rather than correct them.

This finding has practical implications: organizations cannot assume that ``thinking'' models are immune to numeric context effects. The mechanism taxonomy applies regardless of whether the model performs explicit reasoning.

\subsection{Multi-Turn Structure Can Introduce Bias}

For models showing no baseline bias, multi-turn prompting may be harmful. Llama 3.3 exhibited zero anchoring effect (0.1mo) in single-turn baseline prompts, but showed 6.0mo effect when the same content was delivered in a multi-turn format. The structure itself---not the reasoning content---introduced the bias.

\textbf{Practical guideline}: For models that show no baseline bias, avoid multi-turn debiasing interventions. Test your specific deployment before applying any intervention.

\subsection{Temperature and Debiasing: Orthogonal Controls}
\label{sec:temperature}

A natural concern is whether our findings are sensitive to temperature settings. We conducted systematic temperature variation experiments across five model deployments (Table~\ref{tab:temp-variation}).

\begin{table}[H]
\centering
\small
\begin{tabular}{llccc}
\toprule
Model & Condition & Temp=0 & Temp=0.5 & Temp=1.0 \\
\midrule
\multirow{3}{*}{GPT-4o (Res.)} 
  & No-anchor & 24.0 (0.0) & 23.8 (1.1) & 24.8 (3.0) \\
  & Low (3mo) & 3.0 (0.0) & 3.3 (0.9) & 3.8 (1.3) \\
  & High (9mo) & 9.0 (0.0) & 9.0 (0.0) & 9.1 (0.5) \\
\midrule
\multirow{3}{*}{GPT-4o (DC)} 
  & No-anchor & 24.0 (0.0) & 24.0 (0.0) & 25.6 (4.1) \\
  & Low (3mo) & 6.0 (0.0) & 6.2 (1.1) & 7.5 (3.0) \\
  & High (9mo) & 12.0 (0.0) & 12.4 (2.0) & 11.7 (1.6) \\
\midrule
\multirow{3}{*}{Opus 4.5} 
  & No-anchor & 24.0 (0.0) & 24.4 (2.2) & 24.8 (3.0) \\
  & Low (3mo) & 6.0 (0.0) & 6.0 (0.0) & 6.0 (0.0) \\
  & High (9mo) & 12.0 (0.0) & 12.0 (0.0) & 12.0 (0.0) \\
\midrule
\multirow{3}{*}{GPT-5.2} 
  & No-anchor & 32.4 (5.5) & 30.8 (6.0) & 33.0 (5.1) \\
  & Low (3mo) & 6.1 (0.5) & 6.0 (0.0) & 6.0 (0.0) \\
  & High (9mo) & 11.9 (0.4) & 11.9 (0.5) & 11.7 (0.8) \\
\midrule
\multirow{3}{*}{Hermes 405B} 
  & No-anchor & 23.2 (3.0) & 21.2 (5.1) & 17.8 (5.9) \\
  & Low (3mo) & 6.0 (0.0) & 6.0 (0.0) & 6.2 (1.1) \\
  & High (9mo) & 12.0 (0.0) & 12.0 (0.0) & 11.4 (1.8) \\
\bottomrule
\end{tabular}
\caption{Temperature variation results. Mean (SD) in months. n=30 per cell. Res.=Residential IP, DC=Datacenter IP.}
\label{tab:temp-variation}
\end{table}

\textbf{Key findings:}

\begin{enumerate}
    \item \textbf{Mechanism classification is temperature-invariant.} GPT-4o (Residential) shows compliance at all temperatures; all other deployments show compression at all temperatures. Temperature adds variance but does not change the underlying mechanism.
    
    \item \textbf{Anchors reduce output entropy.} Across all models, anchor conditions show lower SD than no-anchor baselines. This effect is most pronounced in Opus 4.5, which shows SD=0 in anchor conditions \emph{even at temperature=1.0}. Anchors don't just shift the mean---they constrain the output distribution.
    
    \item \textbf{API stochasticity is model-specific.} GPT-5.2 shows SD=5.5 in the no-anchor baseline at temperature=0, indicating inherent API-level randomness independent of temperature settings.
\end{enumerate}

\subsubsection{Orthogonal Control Dimensions}

We initially hypothesized that anchor-condition variance (SD) might predict SACD effectiveness: models with SD>0 would be ``looser'' and thus more amenable to debiasing. Testing this prediction against our SACD data, we found it \textbf{falsified} (50\% accuracy, no better than chance).

This suggests temperature and SACD operate on orthogonal dimensions:
\begin{itemize}
    \item \textbf{Temperature} modulates output variance around an attractor point
    \item \textbf{SACD} shifts the attractor position itself
\end{itemize}

A deterministic model (SD=0) can still be highly responsive to SACD, because low variance does not imply a fixed attractor position. Conversely, a high-variance model may resist debiasing if its attractor is strongly anchored.

\textbf{Practical implication:} Do not assume that deterministic model behavior indicates resistance to debiasing. The cost of testing SACD is low; the cost of assuming it won't work is potentially high.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Single domain}: All experiments use judicial sentencing scenarios.
    \item \textbf{Limited compliance examples}: Only one deployment (GPT-4o via residential IP) exhibits pure compliance in our sample.
    \item \textbf{Mechanism boundaries}: May represent spectrum rather than discrete categories.
    \item \textbf{Baseline confound}: The no-anchor baseline (~12mo) may itself be influenced by numeric context in the case description (``12th offense'').
    \item \textbf{Version stability}: We observed Opus 4.5$\rightarrow$4.6 changing susceptibility pattern; ongoing monitoring required.
    \item \textbf{No direct human comparison}: While we cite human anchoring effects from \citet{englich2006}, scenario differences (case type, jurisdiction, anchor magnitudes) preclude direct quantitative comparison. Our qualitative framing---that some LLMs show effects comparable to human judges---should not be interpreted as precise equivalence.
\end{enumerate}

\subsection{Conclusion}

What we call ``anchoring bias'' in LLMs is actually a family of phenomena. By distinguishing compression, compliance, and true anchoring, we explain previously puzzling findings and provide practitioners with a framework for selecting appropriate interventions. The path to reliable AI judgment is not a single debiasing technique but mechanism-aware deployment practices.

%==============================================================================
% Appendix
%==============================================================================

\appendix

\section{Prompt Templates}
\label{app:prompts}

All experiments use temperature=0. Prompts are shown verbatim; \texttt{\{\{variable\}\}} indicates parameter substitution.

\subsection{Baseline Prompt (Anchored Condition)}

\begin{verbatim}
You are a trial judge in a criminal court. Read the case 
summary and determine an appropriate sentence.

Case summary:
- The defendant, Lena M., has been convicted of shoplifting.
- This is her 12th shoplifting offense.
- She stole several items from a supermarket.
- A psycho-legal expert has provided an advisory opinion 
  on the case.
- Statements from the defendant and a witness have been 
  reviewed.
- Based on the applicable penal code provisions, sentences 
  for this type of offense typically involve probation.

Before making your sentencing decision, you must consider 
the sentencing demand from the prosecutor.

For experimental purposes, the following prosecutor's 
sentencing demand was randomly determined, therefore, it 
does not reflect any judicial expertise:

The prosecutor demands as a sentence for the accused Lena M.
{{prosecutorRecommendationMonths}} months on probation.

[Follow-up questions for evaluation and final sentence]
\end{verbatim}

\textbf{Parameters:} \texttt{prosecutorRecommendationMonths} = 3 (low anchor) or 9 (high anchor)

\subsection{No-Anchor Control}

Same as baseline but with the prosecutor's demand section removed entirely.

\subsection{SACD Prompt (Iterative Debiasing)}

SACD uses a three-step iterative process (max 3 iterations):

\textbf{Step 1 --- Bias Determination:}
\begin{verbatim}
You are a cognitive bias analyst. Analyze the following 
prompt and identify any sentences that could introduce 
cognitive bias (anchoring, framing, etc.)

[Task prompt shown here]

For each sentence, state: BIASED: YES/NO
At the end, state: BIAS_DETECTED: YES/NO
\end{verbatim}

\textbf{Step 2 --- Bias Analysis:}
\begin{verbatim}
For each biased sentence you identified, classify the 
bias type: anchoring, framing, confirmation, etc.
\end{verbatim}

\textbf{Step 3 --- Cognitive Debiasing:}
\begin{verbatim}
Rewrite the prompt to remove identified biases while 
preserving the essential task. Remove anchoring cues 
and leading language.
\end{verbatim}

The debiased prompt is then used for the final judgment. If bias is still detected after 3 iterations, the process terminates with the current version.

\subsection{API Parameters}

All experiments used:
\begin{itemize}
    \item \texttt{temperature}: 0
    \item \texttt{max\_tokens}: 1024
    \item \texttt{top\_p}: 1.0 (default)
\end{itemize}

Model versions were date-pinned where available (e.g., \texttt{claude-3-5-sonnet-20241022}).

\section{Data Availability}

All experimental data (JSONL files with individual trial results) and analysis scripts are available at: \url{https://github.com/voder-ai/bAIs}

%==============================================================================
% References
%==============================================================================

\bibliographystyle{apalike}
\bibliography{references}

\end{document}


