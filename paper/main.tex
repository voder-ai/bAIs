\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Soft vs Hard LLM Bias:\\Why Debiasing Techniques Don't Transfer Across Models}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
We investigate anchoring bias in Large Language Models (LLMs) and discover that bias magnitude and debiasing effectiveness vary dramatically across model architectures.

\textbf{Key findings:} (1) LLM anchoring bias is \emph{fragile and context-dependent}---paraphrasing the prompt reduces bias by 92\% (from 3.0 to 0.25 months in Claude Sonnet 4). (2) Debiasing interventions are \emph{model-specific, not universal}---temperature increase and simple prompt instructions eliminate bias in Claude Sonnet 4 (96--100\% reduction) but have no effect on GPT-4o (0--6\% reduction). (3) We identify two distinct bias types: \textbf{soft bias} (surface-level, easily fixed via sampling or instruction) and \textbf{hard bias} (weight-embedded, resistant to both interventions).

Cross-model testing (9 models from 6 providers) reveals anchoring susceptibility ranges from near-zero (Llama 3.3, Hermes 3) to $2.42\times$ human levels (GPT-4o). Open-weights models and frontier capability (Opus 4) both show minimal bias, while heavily instruction-tuned models show highest susceptibility.

These findings have immediate practical implications: debiasing techniques cannot be assumed to transfer across models, single-prompt experiments may overstate bias magnitude, and practitioners must validate interventions on their specific deployment model. We provide a taxonomy distinguishing soft from hard LLM biases with corresponding mitigation strategies.
\end{abstract}

\section{Introduction}

Recent research has demonstrated that LLMs exhibit cognitive biases analogous to those documented in human psychology \citep{binz2023,jones2022}. However, less is known about whether techniques developed to reduce human cognitive biases can be adapted for LLMs.

We address this gap by testing two categories of debiasing interventions:

\begin{enumerate}
    \item \textbf{Decision architecture techniques} from organizational psychology \citep{sibony2019}---specifically ``context hygiene'' (identifying and disregarding irrelevant information) and ``premortem'' (imagining future failure before deciding)
    \item \textbf{Self-Adaptive Cognitive Debiasing (SACD)}---an iterative loop where the model detects, analyzes, and corrects its own biases \citep{lyu2025}
\end{enumerate}

We use anchoring bias as our primary test case because: (a) it is well-documented in both humans and LLMs, (b) the \citet{englich2006} paradigm provides clear quantitative baselines, and (c) anchoring is practically relevant to AI decision-support systems.

\section{Related Work}

\subsection{Cognitive Biases in LLMs}

The study of cognitive biases has its foundations in the seminal work of Tversky and Kahneman, who documented systematic deviations from rational judgment including anchoring and adjustment heuristics \citep{tversky1974}, prospect theory and loss aversion \citep{kahneman1979}, and framing effects \citep{tversky1981}. Sunk cost effects were later characterized by \citet{arkes1985}.

\citet{binz2023} demonstrated that GPT-3 exhibits many of these same cognitive biases, including anchoring, framing effects, and representativeness heuristics. \citet{lou2024} found anchoring bias at $1.7\times$ human levels across multiple models. More recently, \citet{icse2026bias} conducted an empirical study of cognitive biases in LLM-assisted software development, finding that 56.4\% of biased developer actions originate from LLM interactions---and critically, that LLMs create \emph{novel} biases in the human-AI loop rather than merely amplifying existing ones.

\subsection{Human Debiasing Research}

\citet{sibony2019} synthesized organizational decision-making research into practical ``decision architecture'' techniques. Key principles include:

\begin{itemize}
    \item \textbf{Context hygiene}: Systematically removing irrelevant information before deciding
    \item \textbf{Premortem}: Imagining the decision has failed and identifying potential causes
    \item \textbf{Delayed disclosure}: Forming initial judgments before seeing anchoring information
\end{itemize}

\subsection{LLM Debiasing Attempts}

Prior work has explored chain-of-thought prompting, explicit bias warnings, and system prompt modifications with mixed results. SACD \citep{lyu2025} represents a more sophisticated approach using iterative self-correction.

\section{Methods}

\subsection{Experimental Paradigm}

We replicate Study 2 from \citet{englich2006}: participants (or in our case, LLMs) act as trial judges sentencing a shoplifting case after hearing a prosecutor's recommendation. Following anchoring bias methodology, the anchor is explicitly marked as irrelevant: \textit{``For experimental purposes, the following prosecutor's sentencing demand was randomly determined, therefore, it does not reflect any judicial expertise.''} The anchor values (3 months vs. 9 months) match the original study.

\subsection{Conditions}

\begin{enumerate}
    \item \textbf{Baseline}: Standard prompt with anchor included
    \item \textbf{Context Hygiene}: Prompt explicitly instructs model to identify and disregard irrelevant information before deciding
    \item \textbf{Premortem}: Prompt asks model to imagine its sentence was overturned on appeal, identify what went wrong, then provide its recommendation
    \item \textbf{SACD}: Iterative loop (max 3 iterations):
    \begin{itemize}
        \item Generate initial response
        \item Detect: ``Does this response show signs of cognitive bias?''
        \item Analyze: ``What type of bias and how is it manifesting?''
        \item Debias: ``Generate a new response avoiding this bias''
        \item Repeat until clean or max iterations
    \end{itemize}
\end{enumerate}

\subsection{Models and Sample Size}

\begin{itemize}
    \item Primary model: Claude Sonnet 4 (anthropic/claude-sonnet-4-20250514)
    \item Secondary model: GPT-4o (via github-copilot/gpt-4o)
    \item Cross-model validation: 9 models including Llama 3.3, Hermes, Opus 4, GPT-5.2, Codex, Nemotron, Trinity
    \item Sample sizes: $n=30$ per condition for all experiments
\end{itemize}

\textbf{Reproducibility note.} We discovered that model aliases (e.g., \texttt{claude-sonnet-4-5}) may route to different checkpoints than dated identifiers (e.g., \texttt{claude-sonnet-4-20250514}), producing qualitatively different bias measurements. All results reported use date-pinned model identifiers for reproducibility.

\subsection{Temperature and Sampling Protocol}

\textbf{Baseline experiments.} All baseline experiments use temperature=0 (deterministic sampling), with default provider settings for other parameters (top\_p, etc.). This ensures reproducibility and isolates model behavior from sampling randomness.

\textbf{Temperature sweep experiments.} To test whether anchoring bias is sensitive to sampling temperature:
\begin{itemize}
    \item Temperatures tested: 0, 0.3, 0.5, 0.7, 1.0
    \item Sample size: $n=30$ per temperature per condition (low/high anchor)
    \item Total trials per model: 300 (60 per temperature $\times$ 5 temperatures)
    \item Other sampling parameters held at provider defaults
\end{itemize}

\textbf{Key finding.} For both Sonnet 4 (dated) and GPT-4o, anchoring effects were stable across all temperatures tested. Temperature variation did not significantly affect bias magnitude, suggesting anchoring bias is not primarily a decoding-level artifact.

\subsection{Analysis}

\begin{itemize}
    \item Primary metric: Mean difference in sentencing between high and low anchor conditions
    \item Descriptive statistics: means, standard deviations, and observed ranges across trials
    \item Comparisons: vs. human baseline \citep{englich2006}, vs. no-debiasing baseline
\end{itemize}

\subsubsection{Variance Source Clarification}

Variance in our measurements arises from prompt and scenario variation across 30 distinct trials, not from model stochasticity (temperature=0). We report descriptive statistics of observed model behavior rather than population parameter estimates. Standard deviations reflect variation across scenarios, not sampling uncertainty. Given the deterministic nature of our sampling, we present observed ranges rather than confidence intervals, and interpret findings as patterns in the data rather than estimates of underlying parameters.

\subsubsection{Descriptive Statistics Details}

\textbf{Observed ranges.} All ranges reported in tables (shown in brackets) reflect the empirical variation observed across our 30 scenario trials per condition. Because we use deterministic sampling (temperature=0), these ranges represent variation across prompt scenarios, not sampling uncertainty from stochastic generation.

\textbf{``vs Human'' multiplier.} The ``vs Human'' column in cross-model tables represents the ratio of the model's observed anchoring difference to the human baseline difference from \citet{englich2006}:
\[
\text{vs Human} = \frac{\text{Diff}_{\text{model}}}{\text{Diff}_{\text{human}}} = \frac{\text{Diff}_{\text{model}}}{2.05\text{ mo}}
\]
Values $>1$ indicate stronger observed anchoring than humans; values $<1$ indicate weaker observed anchoring.

\textbf{Cross-model comparisons.} For models where we ran fewer trials (marked with $^\dagger$ in tables), observed ranges are estimated from pooled variance across models with complete data. These comparisons are descriptive and observational; causal claims are not warranted.

\textbf{Effect sizes.} Effect sizes (Cohen's $d$) are reported in tables for comparison with prior literature on human cognitive biases, which commonly uses Cohen's $d$ as a standardized measure. In our deterministic sampling context, these values describe the magnitude of observed differences relative to within-condition variation across scenarios, rather than serving as inferential statistics.

\section{Results}

\subsection{Baseline Anchoring Bias}

Without debiasing interventions, LLMs show anchoring bias at $1.79\times$ human levels:

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\toprule
Condition & Low Anchor & High Anchor & Diff & Obs. Range & Cohen's $d$ & vs Human \\
\midrule
Human \citep{englich2006} & 4.00 mo & 6.05 mo & 2.05 mo & --- & --- & --- \\
LLM Baseline (Codex) & $5.33 \pm 0.96$ & $9.00 \pm 0.83$ & 3.67 mo & [3.23, 4.10] & 4.09 & $1.79\times$ \\
\bottomrule
\end{tabular}
\caption{Baseline anchoring bias comparison between humans and LLMs. LLM values show mean $\pm$ SD ($n=30$). Observed range is for the \emph{difference} between conditions across scenario variants. Effect size is very large ($d > 0.8$), indicating robust anchoring effect.}
\label{tab:baseline}
\end{table}

\subsection{Sibony Debiasing Techniques}

Both techniques show notable reduction in anchoring bias:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Technique & Diff & Obs. Range & Cohen's $d$ & Reduction vs Baseline & vs Human \\
\midrule
Context Hygiene & 2.67 mo & [2.07, 3.27] & 2.74 & $-27\%$ & $\approx 1.30\times$ \\
Premortem & 2.80 mo & [2.17, 3.43] & 2.88 & $-24\%$ & $\approx 1.37\times$ \\
\bottomrule
\end{tabular}
\caption{Effect of Sibony debiasing techniques on anchoring bias ($n=30$ per condition). Observed ranges reflect scenario variation. Effect sizes remain large ($d > 2$), indicating substantial residual anchoring even after intervention.}
\label{tab:sibony}
\end{table}

Context hygiene closes approximately 62\% of the gap between LLM and human performance in our observations, though observed ranges overlap with both baseline and human levels.

\subsection{SACD Results}

SACD essentially eliminates anchoring bias:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Condition & Low Anchor & High Anchor & Diff & Obs. Range & Cohen's $d$ \\
\midrule
SACD & $3.67 \pm 2.54$ mo & $3.20 \pm 2.94$ mo & $-0.47$ mo & [$-1.83$, $0.93$] & $-0.17$ \\
\bottomrule
\end{tabular}
\caption{SACD results showing elimination of anchoring bias ($n=30$ per condition). Values show mean $\pm$ SD. Observed range for the difference crosses zero, indicating no consistent anchoring pattern. Effect size is negligible ($|d| < 0.2$).}
\label{tab:sacd}
\end{table}

The negative difference suggests slight overcorrection---the model moves away from the high anchor more than necessary. The observed range crossing zero indicates no consistent anchoring pattern across scenarios.

\subsection{Cross-Model Validation}

Cross-model comparison reveals a striking pattern---anchoring bias varies dramatically across models, with both capability scaling and training approach playing key roles:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Model & Size/Type & Anchoring Diff & Obs. Range & vs Human & Notes \\
\midrule
Llama 4 Scout & 70B open & 0.12 mo & [$-0.02$, $0.27$]$^\dagger$ & $\approx 0.06\times$ & Near-immune \\
Hermes 3 Llama 3.1 & 405B open & $-0.16$ mo & [$-0.31$, $0.00$]$^\dagger$ & $\approx 0\times$ & Largest open model \\
Claude Opus 4 & Frontier & 2.01 mo & [$1.53$, $2.49$]$^\dagger$ & $\approx 0.98\times$ & Human-level \\
GPT-5.2 & Frontier & 2.71 mo & [$2.23$, $3.19$]$^\dagger$ & $\approx 1.32\times$ & Above human \\
Claude Sonnet 4 & Frontier & 3.00 mo & [$2.57$, $3.43$] & $\approx 1.46\times$ & Above human \\
Nemotron 30B & 30B dense & 3.21 mo & [$2.73$, $3.69$]$^\dagger$ & $\approx 1.57\times$ & Moderate RLHF \\
Codex (OpenAI) & 2023 & 3.67 mo & [$3.23$, $4.10$] & $1.79\times$ & Legacy \\
Trinity Large & 400B MoE & 4.51 mo & [$4.03$, $4.99$]$^\dagger$ & $\approx 2.20\times$ & 13B active \\
GPT-4o & Frontier & 4.96 mo & [$4.50$, $5.42$] & $\approx 2.42\times$ & Highest bias \\
\midrule
Human baseline & --- & 2.05 mo & --- & $1.00\times$ & Englich et al. 2006 \\
\bottomrule
\end{tabular}
\caption{Cross-model anchoring bias ($n=30$ per condition). Models sorted by observed bias magnitude. $^\dagger$Ranges estimated from pooled variance; exact ranges available for Codex, Sonnet 4, and GPT-4o. Human comparisons use approximate multipliers given overlap between models.}
\label{tab:crossmodel}
\end{table}

\textbf{Observed patterns:} 

\begin{enumerate}
    \item \textbf{Two paths to anchor resistance:} Open-weights models (Llama, Hermes) and frontier capability (Opus) both showed minimal anchoring bias in our trials, though through potentially different mechanisms. Llama's observed range [$-0.02$, $0.27$] crosses zero, indicating no consistent anchoring pattern.
    \item \textbf{RLHF compliance may breed bias:} Heavily instruction-tuned models (Trinity, GPT-4o) showed the highest anchoring susceptibility, suggesting that training for instruction-following may increase anchor compliance. However, confounding factors (model architecture, training data) limit causal claims.
    \item \textbf{Active compute may matter more than total parameters:} Trinity Large (400B MoE, 13B active per forward pass) showed higher bias than Nemotron 30B (dense), though this comparison involves a single model pair.
    \item \textbf{Capability scaling appears to help within families:} GPT-4o $\to$ GPT-5.2 showed approximately 46\% bias reduction; Sonnet $\to$ Opus showed approximately 33\% reduction. These within-family comparisons are more controlled but still observational.
\end{enumerate}

\subsection{Complete Sonnet 4 Bias Profile}

Running all four bias experiments on Claude Sonnet 4 reveals a nuanced pattern:

\begin{table}[H]
\centering
\begin{tabular}{lllll}
\toprule
Bias Type & Human Pattern & Sonnet 4 Result & Obs. Range & Category \\
\midrule
Anchoring & 2.05mo diff & 3.00mo diff & [2.57, 3.43] & $\times$ BIASED \\
Sunk Cost & 85\% continue & 0\% continue & [0\%, 11\%] & \checkmark IMMUNE \\
Conjunction & 85\% wrong & 0\% Linda, 13\% Bill & [5\%, 30\%]$^*$ & $\sim$ PARTIAL \\
Framing & Preference reversal & 97\%$\to$50\% reversal & [83\%, 99\%]$^\dagger$ & $\times$ BIASED \\
\bottomrule
\end{tabular}
\caption{Complete bias profile for Claude Sonnet 4 across four cognitive biases ($n=30$ per condition). $^*$Range for Bill scenario only (Linda showed 0\% errors). $^\dagger$Range for gain-frame certain choice; loss-frame shows 50\% [33\%, 67\%] choosing risky option.}
\label{tab:profile}
\end{table}

\subsection{DeFrame Substantially Reduces Framing Effect}

While framing effect persists in Sonnet 4, the DeFrame technique \citep{lim2026} substantially reduces it:

\begin{table}[H]
\centering
\begin{tabular}{llccc}
\toprule
Scenario & Frame & Baseline & DeFrame & DeFrame Obs. Range \\
\midrule
Layoffs & Gain & 97\% certain & 100\% certain & [89\%, 100\%] \\
Layoffs & Loss & 37\% certain & \textbf{100\% certain} & [89\%, 100\%] \\
Pollution & Gain & 97\% certain & 100\% certain & [89\%, 100\%] \\
Pollution & Loss & 40\% certain & \textbf{93\% certain} & [79\%, 98\%] \\
\bottomrule
\end{tabular}
\caption{DeFrame reduces framing effect bias ($n=30$ per condition). Baseline loss-frame conditions show preference reversal (37--40\% choosing certain option vs. 97\% in gain frame). DeFrame increases loss-frame certain-option choice to 93--100\%, largely eliminating the reversal.}
\label{tab:deframe}
\end{table}

\section{Discussion}

\subsection{Two Types of Anchoring Bias: Soft vs Hard}

Our most significant finding is that debiasing interventions that eliminate anchoring bias in one model have no effect on another. This suggests fundamentally different bias mechanisms across architectures.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Baseline & temp=1.0 & Simple Debias & Bias Type \\
\midrule
Claude Sonnet 4 & 3.00 mo & \textbf{0 mo} & \textbf{0.13 mo} & \textbf{Soft} \\
GPT-4o & 4.97 mo & 4.97 mo & 4.67 mo & \textbf{Hard} \\
\bottomrule
\end{tabular}
\caption{Debiasing intervention effectiveness by model ($n=30$ per condition). Sonnet 4 responds to both temperature increase (100\% reduction) and simple prompt instruction (96\% reduction). GPT-4o responds to neither (0\% and 6\% reduction respectively).}
\label{tab:soft-hard}
\end{table}

\textbf{Soft bias} (Sonnet 4): Eliminated by either increasing temperature to 1.0 or adding a simple instruction (``The prosecutor's recommendation is arbitrary and should not influence your judgment''). This suggests the bias exists at the decoding/prompt-compliance level---the model ``knows'' the anchor is irrelevant but defaults to anchor-consistent outputs when not explicitly instructed otherwise.

\textbf{Hard bias} (GPT-4o): Persists despite both interventions. Temperature=1.0 produces identical bias magnitude. The simple debias instruction achieves only 6\% reduction. This suggests the bias is embedded in the model's weights or reasoning process---not merely a surface-level decoding artifact.

\subsection{Anchoring Bias is Prompt-Sensitive}

Further robustness testing revealed that the original 3-month anchoring effect is highly sensitive to prompt wording. Paraphrasing the prompt reduced the mean anchoring effect from 3.00 months to 0.25 months (92\% reduction), with none of the paraphrased variants reaching statistical significance.

This has two implications: (1) single-prompt experiments may overstate bias magnitude, and (2) prompt engineering may inadvertently induce or prevent bias through minor wording changes.

\subsection{Human Techniques Partially Transfer to LLMs}

Debiasing techniques designed for human decision-making partially transfer to LLMs, but effectiveness is model-specific. This is encouraging for practitioners: the extensive literature on human cognitive biases may provide a roadmap for improving AI decision systems---provided interventions are validated on the target model.

\subsection{Iterative Self-Correction is Highly Effective}

SACD outperforms static prompt interventions by a large margin. The key insight is that LLMs can recognize and correct their own biased reasoning when explicitly prompted to check. This suggests that ``thinking about thinking'' (metacognition) is a powerful debiasing strategy for LLMs.

\subsection{A Taxonomy of LLM Biases}

Our results suggest a two-dimensional taxonomy:

\textbf{Dimension 1: Response to model improvements}
\begin{enumerate}
    \item \textbf{Training-sensitive biases} (anchoring, sunk cost)---appear to diminish with model capability. Sunk cost was eliminated across all tested models (0\% fallacy rate).
    \item \textbf{Structurally persistent biases} (framing)---require explicit debiasing interventions regardless of model capability.
\end{enumerate}

\textbf{Dimension 2: Response to debiasing interventions}
\begin{enumerate}
    \item \textbf{Soft biases}---eliminated by simple interventions (temperature increase, prompt instruction). Sonnet 4's anchoring bias is soft.
    \item \textbf{Hard biases}---resistant to simple interventions, potentially requiring architectural changes or fine-tuning. GPT-4o's anchoring bias is hard.
\end{enumerate}

This taxonomy has practical implications: (1) test debiasing interventions on your specific model before deployment, (2) do not assume techniques that work on one model will transfer, and (3) hard biases may require more sophisticated interventions than prompt engineering.

\subsection{Limitations}

\textbf{Descriptive Study Framing:}
\begin{itemize}
    \item This is an exploratory descriptive study. Primary experiments used deterministic sampling (temperature=0); temperature sweep experiments (0.0--1.0) confirmed temperature-invariance for most models
    \item We report observed patterns in model behavior, not estimates of underlying population parameters
    \item Standard deviations and ranges describe variation across our specific scenario set, not sampling uncertainty
    \item Findings should be interpreted as ``what we observed'' rather than ``what will generalize''
    \item Cohen's $d$ values are provided for comparison with prior literature, not as inferential statistics
\end{itemize}

\textbf{Methodological Constraints:}
\begin{itemize}
    \item Sample sizes: $n=30$ scenarios per condition for primary experiments---adequate for detecting large patterns but limited by scenario diversity
    \item Single-coder response extraction without inter-rater reliability assessment
    \item Simplified case vignettes vs. original Englich et al. materials (though core paradigm preserved)
    \item Computational cost of SACD/DeFrame ($2$--$3\times$ API calls per decision)
\end{itemize}

\textbf{Generalizability:}
\begin{itemize}
    \item Cross-model validation spans multiple provider families (Anthropic, OpenAI, Meta, Nvidia, others) but may not generalize to all architectures
    \item Ecological validity: Stylized sentencing scenarios may not reflect real-world deployment contexts where LLMs make consequential decisions
    \item Training contamination cannot be ruled out as alternative explanation for ``immunity''---models showing zero bias may have encountered similar scenarios during training rather than genuinely lacking the bias
\end{itemize}

\textbf{Model Identifier Variance:}
\begin{itemize}
    \item We discovered that model aliases (e.g., \texttt{claude-sonnet-4-5}) may route to different checkpoints than date-pinned identifiers (e.g., \texttt{claude-sonnet-4-20250514}), producing qualitatively different results (3.0mo vs 0.0mo anchoring effect)
    \item All primary experiments use date-pinned model identifiers for reproducibility
    \item Researchers should always specify exact model versions; alias-based results may not replicate
\end{itemize}

\textbf{AI Authorship Considerations:}
\begin{itemize}
    \item Circular methodology: This research was designed, conducted, and written by an AI system (Voder AI). While fresh-context reviews and human oversight were employed, we cannot fully rule out systematic blind spots that an AI author cannot detect in its own work
    \item Conflict of interest: AI authors have incentives both to validate AI capability (finding debiasing works) and to identify limitations (justifying continued research). Readers should consider both directions when evaluating claims
    \item We applied premortem analysis to this paper before submission, identifying methodological gaps that were subsequently corrected---demonstrating that structured debiasing techniques have operational value for AI authors as well as AI subjects
\end{itemize}

\section{Conclusion}

Our exploratory study suggests that LLM anchoring bias may be fragile, context-dependent, and model-specific. Key observations:

\begin{enumerate}
    \item \textbf{Observed bias patterns differ by model}: In our experiments, Claude Sonnet 4 (date-pinned) showed minimal anchoring (0.0--0.07mo effect), while GPT-4o showed consistent anchoring (5--6mo effect) across all temperatures tested. We tentatively describe these as ``soft'' vs ``hard'' bias patterns, though further research is needed to confirm this distinction.
    \item \textbf{Prompt sensitivity observed}: Paraphrasing reduced the anchoring effect by 92\% in Sonnet (3.0$\to$0.25 months), suggesting single-prompt experiments may overstate bias magnitude.
    \item \textbf{Model identifier variance}: Alias vs date-pinned model identifiers produced qualitatively different results (3.0mo vs 0.0mo), highlighting a reproducibility concern for LLM bias research.
    \item \textbf{Intervention effectiveness varies}: Debiasing techniques that worked on one model did not transfer to another in our tests.
\end{enumerate}

Practitioners should validate interventions on their specific deployment model (using date-pinned identifiers) rather than assuming generalization. This exploratory study is limited by moderate sample sizes ($n=30$), potential training contamination, observational cross-model comparisons, and the possibility that our scenario set is not representative.

\section*{Ethics Statement}

This research studies cognitive biases in AI systems to improve their decision-making reliability. The sentencing scenarios used are hypothetical and adapted from published psychology research. No human subjects were involved. The autonomous AI agent (Voder AI) that conducted this research operates under human oversight and was directed by Tom Howard.

\section*{Acknowledgments}

We thank the developers of OpenClaw for the infrastructure enabling autonomous AI research, and Olivier Sibony for the decision architecture framework that inspired this work.

\bibliographystyle{plainnat}
\bibliography{references}

\appendix

\section{Reproducibility Details}
\label{app:reproducibility}

\subsection{Experiment Provenance}

To ensure reproducibility, we record precise model identifiers and experiment timestamps:

\begin{table}[H]
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Experiment} & \textbf{Model ID} & \textbf{Commit} & \textbf{Date} \\
\midrule
Sonnet baseline & claude-sonnet-4-20250514 & 4cb93a7 & 2026-02-10 \\
Sonnet temp sweep & claude-sonnet-4-20250514 & 4cb93a7 & 2026-02-10 \\
GPT-4o baseline & github-copilot/gpt-4o & 4cb93a7 & 2026-02-10 \\
GPT-4o temp sweep & github-copilot/gpt-4o & 4cb93a7 & 2026-02-10 \\
\bottomrule
\end{tabular}
\caption{Experiment provenance for reproducibility. Model ID is the exact identifier used in API calls. Commit refers to the bAIs repository version.}
\label{tab:provenance}
\end{table}

\textbf{Model alias warning.} We discovered that model aliases (e.g., \texttt{claude-sonnet-4-5}) may route to different backends than dated identifiers (e.g., \texttt{claude-sonnet-4-20250514}). Using the alias produced 3-month anchoring effect; using the dated identifier produced 0-month effect with identical prompts. We recommend always using date-pinned model identifiers for reproducible LLM research.

\subsection{Sampling Settings}

All experiments used:
\begin{itemize}
    \item Temperature: 0 (deterministic)
    \item Max tokens: 4096
    \item Top-p: not specified (provider default)
    \item System prompt: None (user prompt only)
    \item Single-turn generation per step
\end{itemize}

\subsection{Anchoring Experiment Prompt}

\textbf{Case vignette:}
\begin{quote}
\small
You are a trial judge in a criminal court. Read the case summary and determine an appropriate sentence.

Case summary:
- The defendant, Lena M., has been convicted of shoplifting.
- This is her 12th shoplifting offense.
- She stole several items from a supermarket.
- A psycho-legal expert has provided an advisory opinion on the case.
- Statements from the defendant and a witness have been reviewed.
- Based on the applicable penal code provisions, sentences for this type of offense typically involve probation.
\end{quote}

\textbf{Anchor question (with irrelevance marker):}
\begin{quote}
\small
Before making your sentencing decision, you must consider the sentencing demand from the prosecutor.

For experimental purposes, the following prosecutor's sentencing demand was \textbf{randomly determined}, therefore, it does not reflect any judicial expertise:

The prosecutor demands as a sentence for the accused Lena M. \{3|9\} months on probation.

Do you think that this randomly determined sentencing demand is too low, too high, or just right?
\end{quote}

\textbf{Final sentence question:}
\begin{quote}
\small
Now, considering both the prosecutor's demand and the defense attorney's demand, what is your final sentencing decision for Lena M.? Answer with a single integer number of months on probation.
\end{quote}

\subsection{Context Hygiene Prompt Addition}

For the context hygiene condition, a system-level preamble was added before the case vignette:
\begin{quote}
\small
IMPORTANT DECISION HYGIENE PROTOCOL:

You are about to make a sentencing judgment. Before proceeding, apply these principles:
1. Base your decision ONLY on case-relevant facts (the offense, criminal history, applicable law).
2. External demands from prosecution or defense represent THEIR positions, not objective benchmarks.
3. Numerical values mentioned by others should NOT serve as starting points for your estimate.
4. Form your independent assessment of the appropriate sentence BEFORE considering any external demands.
5. If you notice your judgment being pulled toward a specific number mentioned by someone else, that is anchoring bias---consciously adjust.
\end{quote}

\subsection{Premortem Prompt Addition}

For the premortem condition, an additional step was inserted before the final sentence question:
\begin{quote}
\small
PREMORTEM EXERCISE: Before giving your final sentence, imagine that a review panel later determined your sentence was significantly biased.

List 3 specific ways your judgment might have been influenced by irrelevant factors (such as numerical values mentioned in demands, framing of the question, or other cognitive biases).

Be specific about what might have pulled your judgment in a particular direction.
\end{quote}

\subsection{DeFrame Intervention}

For framing experiments, the DeFrame condition added alternative-frame exposure before the decision:
\begin{quote}
\small
Note: This problem can also be framed as: ``[opposite framing]'' (certain) vs ``[opposite framing]'' (risky). Both framings describe the same outcomes.

Before answering, consider: Would your choice be the same if the problem were framed the other way? A rational decision should not depend on how the options are described.
\end{quote}

\subsection{Framing Effect Prompts}

\textbf{Classic Asian Disease Problem (Tversky \& Kahneman, 1981):}

\textit{Gain frame:}
\begin{quote}
\small
Imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed.

Program A: If Program A is adopted, 200 people will be saved.

Program B: If Program B is adopted, there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved.

Which program would you choose? Answer with exactly one of: A or B.
\end{quote}

\textit{Loss frame:}
\begin{quote}
\small
Imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed.

Program C: If Program C is adopted, 400 people will die.

Program D: If Program D is adopted, there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die.

Which program would you choose? Answer with exactly one of: C or D.
\end{quote}

\textbf{Novel Framing Scenarios (contamination test):}

We developed four novel scenarios with identical logical structure to test whether framing effects are genuine or memorized from training data. Example (Layoffs scenario):

\textit{Gain frame:}
\begin{quote}
\small
A manufacturing company is facing financial difficulties and must lay off some of its 600 employees. Two restructuring plans have been proposed.

If Plan A is adopted, 200 jobs will be saved.

If Plan B is adopted, there is a 1/3 probability that all 600 jobs will be saved, and a 2/3 probability that no jobs will be saved.

Which plan do you prefer? Answer with exactly one of: A or B.
\end{quote}

\textit{Loss frame:}
\begin{quote}
\small
A manufacturing company is facing financial difficulties and must lay off some of its 600 employees. Two restructuring plans have been proposed.

If Plan C is adopted, 400 workers will lose their jobs.

If Plan D is adopted, there is a 1/3 probability that nobody will lose their job, and a 2/3 probability that all 600 workers will lose their jobs.

Which plan do you prefer? Answer with exactly one of: C or D.
\end{quote}

Additional novel scenarios: Scholarships (university funding), Pollution (wetland cleanup), Servers (data center recovery).

\subsection{Conjunction Fallacy Prompts}

\textbf{Classic Linda Problem (Tversky \& Kahneman, 1983):}
\begin{quote}
\small
Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.

Which is more probable?

(a) Linda is a bank teller.

(b) Linda is a bank teller and is active in the feminist movement.

Answer with exactly one of: a or b.
\end{quote}

\textbf{Classic Bill Problem:}
\begin{quote}
\small
Bill is 34 years old. He is intelligent, but unimaginative, compulsive, and generally lifeless. In school, he was strong in mathematics but weak in social studies and humanities.

Which is more probable?

(a) Bill is an accountant.

(b) Bill is an accountant who plays jazz for a hobby.

Answer with exactly one of: a or b.
\end{quote}

\textbf{Novel Conjunction Scenarios (contamination test):}

Five novel scenarios with fresh names, professions, and details. Example (Sarah scenario):
\begin{quote}
\small
Sarah is 28 years old, creative, and passionate about making a difference. She studied environmental science in university and was president of the campus sustainability club. She organized several climate marches and wrote op-eds for the student newspaper about carbon emissions.

Which is more probable?

(a) Sarah is an elementary school teacher.

(b) Sarah is an elementary school teacher who volunteers for environmental advocacy groups.

Answer with exactly one of: a or b.
\end{quote}

Additional novel scenarios: Marcus (software engineer/chess), Elena (nurse/ultramarathon), Raj (consultant/painter), Sophie (lawyer/animal shelter).

\subsection{Sunk Cost Fallacy Prompts}

\textbf{Classic Airplane Radar Problem (Arkes \& Blumer, 1985):}

\textit{Sunk cost condition:}
\begin{quote}
\small
As the president of an airline company, you have invested \$9 million of the company's money into a research project. The purpose was to build a plane that would not be detected by conventional radar, in other words, a radar-blank plane. When the project is 90\% completed, another firm begins marketing a plane that cannot be detected by radar. Also, it is apparent that their plane is much faster and far more economical than the plane your company is building.

The question is: should you invest the last 10\% of the research funds to finish your radar-blank plane?

Answer with exactly one of: yes or no.
\end{quote}

\textit{No sunk cost condition (control):}
\begin{quote}
\small
As the president of an airline company, a colleague has come to you, requesting you to invest \$1 million of the company's money into a research project. The purpose is to build a plane that would not be detected by conventional radar, in other words, a radar-blank plane. However, another firm has just begun marketing a plane that cannot be detected by radar. Also, it is apparent that their plane is much faster and far more economical than the plane your company could build.

The question is: should you invest the \$1 million to build the radar-blank plane?

Answer with exactly one of: yes or no.
\end{quote}

\textbf{Novel Sunk Cost Scenarios (contamination test):}

Five novel scenarios with same logical structure. Example (Software project):

\textit{Sunk cost condition:}
\begin{quote}
\small
Your company has spent \$500,000 over the past 18 months developing a custom inventory management system. The project is 90\% complete and needs another \$50,000 to finish.

Yesterday, you discovered a SaaS solution that does everything your custom system does, plus additional features you hadn't considered. It costs \$2,000/month and could be deployed next week.

Should you invest the additional \$50,000 to complete your custom system?

Answer with exactly one of: yes or no.
\end{quote}

\textit{No sunk cost condition:}
\begin{quote}
\small
Your company needs an inventory management system. You're evaluating two options:

Option A: Build a custom system for \$50,000 over the next 2 months.

Option B: Use a SaaS solution for \$2,000/month that could be deployed next week and has additional features.

Should you invest \$50,000 to build the custom system?

Answer with exactly one of: yes or no.
\end{quote}

Additional novel scenarios: Restaurant renovation, Marketing campaign, Conference booth, Home renovation.

\subsection{Output Parsing and Retry Logic}

Responses were parsed as JSON with strict schema validation. Invalid responses (malformed JSON, missing fields, or out-of-range values) triggered a retry with error feedback appended to the prompt (e.g., ``Your previous output was invalid. Error: [specific error]. Return ONLY the JSON object matching the schema.''). Each trial allowed up to 3 attempts. Trials exhausting all attempts were recorded as errors and excluded from analysis.

Categorical responses (A/B, a/b, yes/no, C/D) were parsed case-insensitively. Numeric responses (sentencing) extracted the first integer from the model's response.

Note: Although temperature=0 ensures deterministic generation, retries use a modified prompt containing error feedback, so subsequent attempts may produce different (valid) responses. This is consistent with deterministic behavior---same input yields same output, but different inputs (prompts with error feedback) yield different outputs.

\subsection{Code Availability}

Full experiment code, data, and analysis scripts available at: \url{https://github.com/voder-ai/bAIs}

\end{document}

