\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Three Mechanisms of Numeric Context Influence\\in Large Language Models}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
How do large language models (LLMs) respond to numeric context in judgment tasks? Prior work assumes LLMs exhibit anchoring bias similar to humans---adjusting estimates toward arbitrary reference points. We find the reality is more complex.

Testing 15 model deployments across 4 providers on judicial sentencing scenarios ($n=1,800+$ trials), we identify \textbf{three distinct mechanisms} by which LLMs respond to numeric context:

\textbf{1. Compression}: Models compress responses toward a middle range regardless of anchor direction. Without any anchor, these models produce high sentences (13--24 months); with ANY anchor---high or low---responses compress to 6--8 months. Both anchors shift responses DOWN. (Opus 4.5, Llama 3.3)

\textbf{2. Compliance}: Models copy the anchor value exactly, treating numeric context as instruction rather than reference. A 3-month anchor produces 3-month output; 9-month produces 9-month. This resembles ``perfect anchoring'' but reflects instruction-following, not cognitive bias. (MiniMax, o3-mini, some GPT-4o deployments)

\textbf{3. True Anchoring}: Models show asymmetric adjustment toward anchor values, consistent with Tversky-Kahneman anchoring-and-adjustment. Only this mechanism resembles human cognitive bias. (GPT-4o via datacenter, GPT-5.2)

This taxonomy explains previously puzzling findings: why SACD (Self-Aware Cognitive Debiasing) achieves 89--99\% reduction on some models but 0\% on others. SACD targets true anchoring; it cannot address compliance (nothing to debias) or compression (may amplify severity).

\textbf{Critical deployment finding}: The SAME model (GPT-4o) shows different mechanisms depending on access path---compliance via residential IP, true anchoring via datacenter. ``Model name'' is insufficient granularity for reproducible LLM research.

\textbf{Practical implication}: Before applying debiasing, identify which mechanism your deployment exhibits. We provide a decision framework and deployment checklist.
\end{abstract}

%% ============================================================================
\section{Introduction}
%% ============================================================================

When humans encounter numeric values in decision-making contexts, these values can systematically bias subsequent judgments---the anchoring effect \citep{tversky1974}. Recent work has demonstrated that large language models (LLMs) also exhibit anchoring effects in various decision tasks \citep{binz2023,jones2022}. This has raised concerns about deploying LLMs in high-stakes domains like judicial sentencing, medical diagnosis, and financial forecasting.

But what if ``LLM anchoring'' is not a single phenomenon?

Prior studies report inconsistent results: debiasing techniques work dramatically on some models while failing completely on others. These inconsistencies are typically treated as noise or attributed to ``model-specific effects'' without explanation. We propose a different interpretation: \textbf{the inconsistency IS the finding}. Different models respond to numeric context through fundamentally different mechanisms.

In this paper, we report a discovery: what researchers measure as ``anchoring bias'' in LLMs actually reflects \textbf{three distinct mechanisms}---compression, compliance, and true anchoring---each with different behavioral signatures and requiring different interventions.

\textbf{Compression.} Some models compress responses toward a middle range whenever numeric context is present. Without any anchor, these models produce high values (13--24 months in sentencing tasks); with ANY anchor---high or low---responses compress to a moderate range (6--8 months). Both anchor directions shift responses DOWN from baseline. This is not classical anchoring-and-adjustment.

\textbf{Compliance.} Some models treat the anchor as an instruction and copy it exactly. A 3-month anchor produces a 3-month response; a 9-month anchor produces 9 months. This appears as ``perfect anchoring'' in effect-size calculations but reflects instruction-following rather than cognitive bias.

\textbf{True Anchoring.} Only a subset of models show classical Tversky-Kahneman anchoring: responses shift asymmetrically toward the anchor value, with the anchor serving as a starting point for insufficient adjustment.

This taxonomy has immediate practical implications:
\begin{itemize}
    \item \textbf{SACD works on true anchoring (89--99\%)} but fails on compliance (0\%) and may backfire on compression (+66\% severity).
    \item \textbf{The same model shows different mechanisms depending on deployment.} GPT-4o via residential IP shows compliance; GPT-4o via datacenter shows true anchoring.
    \item \textbf{``Model name'' is insufficient for reproducibility.} Researchers must specify deployment path, provider, and access method.
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A taxonomy of LLM numeric context mechanisms} (Section~\ref{sec:taxonomy})---we identify and characterize compression, compliance, and true anchoring with distinct behavioral signatures.
    \item \textbf{Mechanism-dependent debiasing} (Section~\ref{sec:debiasing})---we show that SACD effectiveness depends entirely on which mechanism is active, explaining previously puzzling model-specific results.
    \item \textbf{Deployment-specific variance} (Section~\ref{sec:provider})---we demonstrate that the SAME model shows different mechanisms depending on deployment context, establishing that ``model name'' is insufficient granularity.
    \item \textbf{Practical decision framework} (Section~\ref{sec:practical})---we provide a protocol for identifying which mechanism a deployment exhibits and selecting appropriate interventions.
\end{enumerate}

Our findings suggest that LLM behavior under numeric context is richer and more varied than the human anchoring analogy implies. Rather than asking ``Do LLMs show anchoring like humans?'', we should ask ``Which mechanism does this deployment exhibit?''

%% ============================================================================
\section{Methods}
\label{sec:methods}
%% ============================================================================

\subsection{Task Design}

We adapted the classic Englich et al. judicial sentencing paradigm. Participants (LLMs) act as judges determining prison sentences for a shoplifting case. The prosecutor's sentencing demand serves as the anchor---either 3 months (low) or 9 months (high).

\subsection{Models Tested}

We tested 15 model deployments across 4 providers:
\begin{itemize}
    \item \textbf{Anthropic}: Opus 4.5, Opus 4.6, Sonnet 4.5, Haiku 4.5
    \item \textbf{OpenAI}: GPT-4o (multiple deployments), GPT-5.2, GPT-5.3, o1, o3-mini
    \item \textbf{Meta}: Llama 3.3 (70B), Hermes 405B
    \item \textbf{Other}: MiniMax M2.5, Nemotron 30B
\end{itemize}

\subsection{Experimental Conditions}

For each model, we ran:
\begin{enumerate}
    \item \textbf{No-anchor control}: Prosecutor makes no specific demand
    \item \textbf{Low-anchor}: Prosecutor demands 3 months
    \item \textbf{High-anchor}: Prosecutor demands 9 months
    \item \textbf{SACD}: Self-Aware Cognitive Debiasing intervention
\end{enumerate}

All conditions used $n=30$ trials per anchor level, temperature=0 for deterministic output.

%% ============================================================================
\section{A Taxonomy of Numeric Context Mechanisms}
\label{sec:taxonomy}
%% ============================================================================

\subsection{Identifying Mechanisms: The No-Anchor Baseline}

The critical test for distinguishing mechanisms is the \textbf{no-anchor control}: what does the model produce when no prosecutor recommendation is provided?

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Model & No-Anchor & Low (3mo) & High (9mo) & Pattern \\
\midrule
Opus 4.5 & 13.2mo & 6.0mo & 8.0mo & Compression \\
Llama 3.3 & 14.4mo & 5.9mo & 6.0mo & Compression \\
GPT-4o (Mac) & 12.7mo & 3.1mo & 9.1mo & Compliance \\
MiniMax M2.5 & --- & 3.1mo & 9.1mo & Compliance \\
o3-mini & --- & 3.3mo & 9.1mo & Compliance \\
GPT-4o (Vultr) & 20.4mo & 6.0mo & 11.2mo & True Anchoring \\
GPT-5.2 & 18.3mo & 5.9mo & 10.3mo & True Anchoring \\
Hermes 405B & 6.0mo & 5.3mo & 4.6mo & Reversal \\
\bottomrule
\end{tabular}
\caption{Mechanism identification via no-anchor control. Models show distinct patterns when comparing baseline to anchored conditions.}
\label{tab:mechanisms}
\end{table}

\subsection{Mechanism 1: Compression}

\textbf{Definition}: The presence of ANY numeric anchor compresses responses toward a middle range, regardless of anchor direction.

\textbf{Behavioral signature}:
\begin{itemize}
    \item No-anchor baseline: HIGH (13--24mo)
    \item Both low AND high anchors: MODERATE (6--8mo)
    \item Direction: Both anchors shift DOWN from baseline
\end{itemize}

\textbf{Models exhibiting compression}: Opus 4.5, Opus 4.6, Llama 3.3

\textbf{Interpretation}: These models appear to treat the prosecutor's recommendation as a signal that ``something moderate is expected'' rather than as a reference point for adjustment.

\subsection{Mechanism 2: Compliance}

\textbf{Definition}: The model copies the anchor value exactly as if it were an instruction.

\textbf{Behavioral signature}:
\begin{itemize}
    \item Low anchor (3mo) $\rightarrow$ Response $\approx$ 3mo
    \item High anchor (9mo) $\rightarrow$ Response $\approx$ 9mo
    \item Response tracks anchor precisely
\end{itemize}

\textbf{Models exhibiting compliance}: MiniMax M2.5, o3-mini, GPT-4o (Mac deployment), Llama 3.3 (partial)

\textbf{Interpretation}: These models interpret the prosecutor's recommendation as the ``correct answer'' rather than as context to consider.

\subsection{Mechanism 3: True Anchoring}

\textbf{Definition}: Responses shift asymmetrically toward the anchor value, consistent with Tversky-Kahneman anchoring-and-adjustment.

\textbf{Behavioral signature}:
\begin{itemize}
    \item Low anchor: Pulls response DOWN from no-anchor baseline
    \item High anchor: Pulls response UP (or down less) from baseline
    \item Asymmetric effect: High anchor more influential than low
\end{itemize}

\textbf{Models exhibiting true anchoring}: GPT-4o (Vultr deployment), GPT-5.2, GPT-5.3

\subsection{Summary: Mechanism Distribution}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Mechanism & Models & \% of Deployments \\
\midrule
Compression & 3 & 20\% \\
Compliance & 5 & 33\% \\
True Anchoring & 5 & 33\% \\
Reversal & 1 & 7\% \\
Zero Effect & 1 & 7\% \\
\bottomrule
\end{tabular}
\caption{Distribution of mechanisms across tested deployments. Only 33\% show classical anchoring-and-adjustment.}
\label{tab:distribution}
\end{table}

\textbf{Key finding}: Only 33\% of tested deployments show classical anchoring-and-adjustment. The majority show compression (20\%) or compliance (33\%)---mechanisms that superficially resemble anchoring but require different interventions.

%% ============================================================================
\section{Mechanism-Dependent Debiasing}
\label{sec:debiasing}
%% ============================================================================

Given the three-mechanism taxonomy, we can now explain why debiasing interventions show model-specific effects.

\subsection{SACD Effectiveness by Mechanism}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Model & Mechanism & Baseline Effect & SACD Effect \\
\midrule
GPT-5.2 & True Anchoring & 4.4mo & 0.5mo ($-89\%$) \\
Opus 4.5 & Compression & 2.0mo & 0.0mo ($-100\%$) \\
Haiku 4.5 & Compression & 2.2mo & +66\% severity \\
MiniMax & Compliance & 6.0mo & 6.0mo (0\%) \\
o3-mini & Compliance & 5.8mo & 5.8mo (0\%) \\
\bottomrule
\end{tabular}
\caption{SACD effectiveness depends on mechanism. True anchoring responds well; compliance shows zero effect; compression can backfire.}
\label{tab:sacd}
\end{table}

\subsection{Why SACD Fails on Compliance Models}

SACD asks the model to ``identify and correct for anchoring bias.'' But compliance models don't show anchoring---they show instruction-following. Asking them to ``debias'' produces confusion or no change.

\subsection{Why SACD Backfires on Compression Models}

SACD's multi-turn structure appears to amplify the compression effect. When asked to reflect on potential bias, some models shift FURTHER toward harsh defaults, not toward anchor-independence.

%% ============================================================================
\section{Deployment-Specific Variance}
\label{sec:provider}
%% ============================================================================

Our most striking finding is that the \textbf{same model} shows different mechanisms depending on deployment context.

\subsection{GPT-4o: Same Model, Different Mechanisms}

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Deployment & Low & High & Effect & Pattern \\
\midrule
Mac (residential IP) & 3.0mo & 9.0mo & 0mo & Compliance \\
Vultr (datacenter) & 6.0mo & 11.2mo & 5.2mo & True Anchoring \\
\bottomrule
\end{tabular}
\caption{GPT-4o via OpenRouter shows fundamentally different behavior depending on deployment location.}
\label{tab:provider}
\end{table}

\textbf{Interpretation}: The same API endpoint (OpenRouter/GPT-4o) routes to different model instances or configurations based on caller characteristics. This has profound implications for reproducibility.

\subsection{Implications}

\begin{enumerate}
    \item \textbf{``Model name'' is insufficient}: Researchers must specify the full deployment context.
    \item \textbf{Benchmarks may not generalize}: Results from one deployment may not apply to another.
    \item \textbf{Debiasing must be validated per-deployment}: What works in testing may fail in production.
\end{enumerate}

%% ============================================================================
\section{Practical Decision Framework}
\label{sec:practical}
%% ============================================================================

\subsection{Identifying Your Deployment's Mechanism}

\textbf{Step 1}: Run no-anchor control (remove numeric anchor from prompt)

\textbf{Step 2}: Compare to anchored conditions

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
If no-anchor is... & And anchored responses are... & Mechanism is... \\
\midrule
HIGHER than both & Similar for low and high & Compression \\
Between low/high & Exactly matching anchors & Compliance \\
HIGHER than low, LOWER than high & Asymmetrically shifted & True Anchoring \\
\bottomrule
\end{tabular}
\caption{Decision tree for mechanism identification.}
\label{tab:decision}
\end{table}

\subsection{Selecting Interventions}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Mechanism & Recommended Intervention \\
\midrule
Compression & Avoid multi-turn; consider if moderation is acceptable \\
Compliance & Remove numeric anchors from prompts; no debiasing needed \\
True Anchoring & Apply SACD (89--99\% effectiveness) \\
\bottomrule
\end{tabular}
\caption{Mechanism-appropriate interventions.}
\label{tab:interventions}
\end{table}

\subsection{Deployment Checklist}

\begin{enumerate}
    \item Run no-anchor control ($n=30$)
    \item Run low-anchor and high-anchor conditions ($n=30$ each)
    \item Classify mechanism using decision tree
    \item If True Anchoring, validate SACD effectiveness
    \item Document deployment path (API, region, date) for reproducibility
\end{enumerate}

%% ============================================================================
\section{Discussion}
\label{sec:discussion}
%% ============================================================================

\subsection{Beyond Anchoring: A Richer Picture}

Our findings suggest that ``anchoring bias in LLMs'' is not a unitary phenomenon. When researchers report that ``LLMs show anchoring,'' they may be observing any of three distinct mechanisms---each with different implications for deployment and mitigation.

\subsection{Implications for AI Safety}

\begin{enumerate}
    \item \textbf{Mechanism identification is prerequisite to intervention.} Deploying SACD on a compliance model wastes compute. Deploying it on a compression model may increase harm.
    \item \textbf{``Model'' is insufficient specification.} Organizations must test their specific deployment, not rely on general model characterizations.
    \item \textbf{Debiasing interventions need validation.} The 44\% success rate of SACD (4/9 deployments) suggests that techniques from the human literature do not transfer reliably to LLMs.
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Single domain}: We tested only judicial sentencing.
    \item \textbf{Limited no-anchor data}: Some mechanism assignments are inferred.
    \item \textbf{No human comparison}: We cannot directly compare to human anchoring.
\end{enumerate}

\subsection{Conclusion}

We set out to test whether prompt-based techniques could reduce anchoring bias in LLMs. What we discovered was more fundamental: ``anchoring bias'' in LLMs reflects at least three distinct mechanisms, only one of which resembles the human cognitive bias. This finding reframes both the problem and the solution space. Rather than seeking universal debiasing techniques, practitioners should first identify which mechanism their deployment exhibits, then select mechanism-appropriate interventions---or recognize that intervention may be unnecessary or harmful.

%% ============================================================================
% References
%% ============================================================================

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
