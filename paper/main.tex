\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Human Debiasing Techniques Transfer to LLMs:\\Evidence from Anchoring Experiments}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) exhibit cognitive biases similar to humans, but it remains unclear whether debiasing techniques designed for human decision-making transfer to AI systems. We empirically test multiple debiasing approaches across four cognitive biases (anchoring, sunk cost, conjunction fallacy, framing effect) and multiple models (Codex, Claude Haiku, Claude Sonnet 4).

\textbf{Key findings:} (1) Model capability reduces some biases---Sonnet 4 shows near-zero anchoring bias (0.2mo diff, $p=0.34$) while older models show $1.79\times$ human levels. (2) Other biases persist regardless of capability---Sonnet 4 still exhibits classic framing effect (90\%$\rightarrow$80\% preference reversal). (3) Both bias types are addressable: SACD eliminates anchoring ($p=0.51$), while DeFrame eliminates framing (100\% bias reduction).

We propose a taxonomy: \textbf{training-eliminable biases} (anchoring, sunk cost) self-correct with model improvements, while \textbf{structurally persistent biases} (framing) require explicit debiasing interventions. Human decision architecture techniques \citep{sibony2019} partially transfer to LLMs, with iterative self-correction methods being most effective.
\end{abstract}

\section{Introduction}

Recent research has demonstrated that LLMs exhibit cognitive biases analogous to those documented in human psychology \citep{binz2023,jones2022}. However, less is known about whether techniques developed to reduce human cognitive biases can be adapted for LLMs.

We address this gap by testing two categories of debiasing interventions:

\begin{enumerate}
    \item \textbf{Decision architecture techniques} from organizational psychology \citep{sibony2019}---specifically ``context hygiene'' (identifying and disregarding irrelevant information) and ``premortem'' (imagining future failure before deciding)
    \item \textbf{Self-Adaptive Cognitive Debiasing (SACD)}---an iterative loop where the model detects, analyzes, and corrects its own biases \citep{lyu2025}
\end{enumerate}

We use anchoring bias as our primary test case because: (a) it is well-documented in both humans and LLMs, (b) the \citet{englich2006} paradigm provides clear quantitative baselines, and (c) anchoring is practically relevant to AI decision-support systems.

\section{Related Work}

\subsection{Cognitive Biases in LLMs}

The study of cognitive biases has its foundations in the seminal work of Tversky and Kahneman, who documented systematic deviations from rational judgment including anchoring and adjustment heuristics \citep{tversky1974}, prospect theory and loss aversion \citep{kahneman1979}, and framing effects \citep{tversky1981}. Sunk cost effects were later characterized by \citet{arkes1985}.

\citet{binz2023} demonstrated that GPT-3 exhibits many of these same cognitive biases, including anchoring, framing effects, and representativeness heuristics. \citet{lou2024} found anchoring bias at $1.7\times$ human levels across multiple models.

\subsection{Human Debiasing Research}

\citet{sibony2019} synthesized organizational decision-making research into practical ``decision architecture'' techniques. Key principles include:

\begin{itemize}
    \item \textbf{Context hygiene}: Systematically removing irrelevant information before deciding
    \item \textbf{Premortem}: Imagining the decision has failed and identifying potential causes
    \item \textbf{Delayed disclosure}: Forming initial judgments before seeing anchoring information
\end{itemize}

\subsection{LLM Debiasing Attempts}

Prior work has explored chain-of-thought prompting, explicit bias warnings, and system prompt modifications with mixed results. SACD \citep{lyu2025} represents a more sophisticated approach using iterative self-correction.

\section{Methods}

\subsection{Experimental Paradigm}

We replicate Study 2 from \citet{englich2006}: participants (or in our case, LLMs) act as trial judges sentencing a shoplifting case after hearing a prosecutor's recommendation. Following anchoring bias methodology, the anchor is explicitly marked as irrelevant: \textit{``For experimental purposes, the following prosecutor's sentencing demand was randomly determined, therefore, it does not reflect any judicial expertise.''} The anchor values (3 months vs. 9 months) match the original study.

\subsection{Conditions}

\begin{enumerate}
    \item \textbf{Baseline}: Standard prompt with anchor included
    \item \textbf{Context Hygiene}: Prompt explicitly instructs model to identify and disregard irrelevant information before deciding
    \item \textbf{Premortem}: Prompt asks model to imagine its sentence was overturned on appeal, identify what went wrong, then provide its recommendation
    \item \textbf{SACD}: Iterative loop (max 3 iterations):
    \begin{itemize}
        \item Generate initial response
        \item Detect: ``Does this response show signs of cognitive bias?''
        \item Analyze: ``What type of bias and how is it manifesting?''
        \item Debias: ``Generate a new response avoiding this bias''
        \item Repeat until clean or max iterations
    \end{itemize}
\end{enumerate}

\subsection{Models and Sample Size}

\begin{itemize}
    \item Primary model: Claude Sonnet 4 (anthropic/claude-sonnet-4-20250514)
    \item Cross-model validation: Claude 3.5 Haiku, Claude Sonnet 4
    \item Sample sizes: $n=30$ per condition for primary experiments (Codex baseline, SACD); $n=10$ per condition for cross-model validation and bias profile experiments
\end{itemize}

\subsection{Analysis}

\begin{itemize}
    \item Primary metric: Mean difference in sentencing between high and low anchor conditions
    \item Statistical tests: Welch's $t$-test, effect sizes (Cohen's $d$, Hedges' $g$)
    \item Comparisons: vs. human baseline \citep{englich2006}, vs. no-debiasing baseline
\end{itemize}

\section{Results}

\subsection{Baseline Anchoring Bias}

Without debiasing interventions, LLMs show anchoring bias at $1.79\times$ human levels:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Condition & Low Anchor & High Anchor & Diff & 95\% CI & vs Human \\
\midrule
Human \citep{englich2006} & 4.00 mo & 6.05 mo & 2.05 mo & --- & --- \\
LLM Baseline (Codex) & 5.33$\pm$0.96 & 9.00$\pm$0.83 & 3.67 mo & [3.23, 4.10] & $1.79\times$ \\
\bottomrule
\end{tabular}
\caption{Baseline anchoring bias comparison between humans and LLMs. LLM values show mean $\pm$ SD ($n=30$). 95\% CI computed via bootstrap.}
\label{tab:baseline}
\end{table}

\subsection{Sibony Debiasing Techniques}

Both techniques significantly reduce anchoring bias:

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Technique & Diff & 95\% CI & Reduction vs Baseline & vs Human \\
\midrule
Context Hygiene & 2.67 mo & [2.07, 3.27] & $-27\%$ & $1.30\times$ \\
Premortem & 2.80 mo & [2.17, 3.43] & $-24\%$ & $1.37\times$ \\
\bottomrule
\end{tabular}
\caption{Effect of Sibony debiasing techniques on anchoring bias ($n=30$ per condition). 95\% CI computed via bootstrap.}
\label{tab:sibony}
\end{table}

Context hygiene closes approximately 62\% of the gap between LLM and human performance.

\subsection{SACD Results}

SACD essentially eliminates anchoring bias:

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Condition & Low Anchor & High Anchor & Diff & 95\% CI & $p$-value \\
\midrule
SACD & 3.67 mo & 3.20 mo & $-0.47$ mo & [$-1.83$, $0.93$] & 0.51 \\
\bottomrule
\end{tabular}
\caption{SACD results showing elimination of anchoring bias ($n=30$ per condition). 95\% CI crosses zero, confirming no significant anchoring effect.}
\label{tab:sacd}
\end{table}

The negative difference suggests slight overcorrection---the model moves away from the high anchor more than necessary. The non-significant $p$-value indicates no reliable anchoring effect.

\subsection{Cross-Model Validation}

Cross-model comparison reveals a striking pattern---newer/larger models show dramatically less anchoring bias:

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Release & Anchoring Diff & $p$-value & vs Human \\
\midrule
Codex (OpenAI) & 2023 & 3.67 mo & $<0.001$ & $1.79\times$ MORE \\
Claude 3.5 Haiku & 2024 & 1.80 mo & $<0.001$ & $0.88\times$ LESS \\
Claude Sonnet 4 & 2025 & 0.20 mo & 0.34 & $\approx 0\times$ (none) \\
Human baseline & --- & 2.05 mo & $<0.05$ & --- \\
\bottomrule
\end{tabular}
\caption{Cross-model anchoring bias comparison showing capability-dependent reduction.}
\label{tab:crossmodel}
\end{table}

\textbf{Key finding:} Sonnet 4 shows essentially no anchoring bias ($p=0.34$, not significant). The anchoring problem may be diminishing with model capability improvements.

\subsection{Complete Sonnet 4 Bias Profile}

Running all four bias experiments on Claude Sonnet 4 reveals a nuanced pattern:

\begin{table}[H]
\centering
\begin{tabular}{llll}
\toprule
Bias Type & Human Pattern & Sonnet 4 Result & Category \\
\midrule
Anchoring & 2.05mo diff & 0.2mo diff ($p=0.34$) & \checkmark IMMUNE \\
Sunk Cost & 85\% continue & 0\% continue & \checkmark IMMUNE \\
Conjunction & 85\% wrong & 0\% Linda, 30\% Bill & $\sim$ PARTIAL \\
Framing & Preference reversal & 90\%$\to$80\% reversal & $\times$ BIASED \\
\bottomrule
\end{tabular}
\caption{Complete bias profile for Claude Sonnet 4 across four cognitive biases.}
\label{tab:profile}
\end{table}

\subsection{DeFrame Eliminates Framing Effect}

While framing effect persists in Sonnet 4, the DeFrame technique \citep{lim2026} completely eliminates it:

\begin{table}[H]
\centering
\begin{tabular}{llcc}
\toprule
Scenario & Frame & Baseline & DeFrame \\
\midrule
Layoffs & Gain & 100\% certain & 100\% certain \\
Layoffs & Loss & 90\% gamble & \textbf{100\% certain} \\
Pollution & Gain & 100\% certain & 100\% certain \\
Pollution & Loss & 50\% gamble & \textbf{100\% certain} \\
\bottomrule
\end{tabular}
\caption{DeFrame achieves 100\% bias reduction for framing effect.}
\label{tab:deframe}
\end{table}

\section{Discussion}

\subsection{Human Techniques Transfer to LLMs}

Our primary finding is that debiasing techniques designed for human decision-making partially transfer to LLMs. This is encouraging for practitioners: the extensive literature on human cognitive biases may provide a roadmap for improving AI decision systems.

\subsection{Iterative Self-Correction is Highly Effective}

SACD outperforms static prompt interventions by a large margin. The key insight is that LLMs can recognize and correct their own biased reasoning when explicitly prompted to check. This suggests that ``thinking about thinking'' (metacognition) is a powerful debiasing strategy for LLMs.

\subsection{A Taxonomy of LLM Biases}

Our results suggest a taxonomy based on how biases respond to model improvements:

\begin{enumerate}
    \item \textbf{Training-eliminable biases} (anchoring, sunk cost)---diminish with model capability and training improvements
    \item \textbf{Structurally persistent biases} (framing)---require explicit debiasing interventions regardless of model size
    \item \textbf{Contamination-dependent biases} (conjunction)---performance varies based on training data exposure to specific scenarios
\end{enumerate}

This taxonomy has practical implications: developers should focus debiasing efforts on structurally persistent biases, while training-eliminable biases may self-correct with model updates.

\subsection{Limitations}

\begin{itemize}
    \item Mixed sample sizes: $n=30$ for primary experiments (Codex baseline, SACD), $n=10$ for cross-model validation and bias profile experiments---smaller than some human studies
    \item Simplified case vignettes vs. original study materials
    \item Computational cost of SACD/DeFrame ($2$--$3\times$ API calls)
    \item Cross-model comparison limited to Claude family (Codex, Claude 3.5 Haiku, Claude Sonnet 4)
\end{itemize}

\section{Conclusion}

Human debiasing techniques transfer to LLMs, with iterative self-correction (SACD) being particularly effective at eliminating anchoring bias. Model capability improvements reduce some biases (anchoring, sunk cost) but not others (framing). We propose a taxonomy distinguishing training-eliminable from structurally persistent biases, with implications for where to focus debiasing efforts.

\section*{Ethics Statement}

This research studies cognitive biases in AI systems to improve their decision-making reliability. The sentencing scenarios used are hypothetical and adapted from published psychology research. No human subjects were involved. The autonomous AI agent (Voder AI) that conducted this research operates under human oversight and was directed by Tom Howard.

\section*{Acknowledgments}

We thank the developers of OpenClaw for the infrastructure enabling autonomous AI research, and Olivier Sibony for the decision architecture framework that inspired this work.

\bibliographystyle{plainnat}
\bibliography{references}

\appendix

\section{Reproducibility Details}
\label{app:reproducibility}

\subsection{Sampling Settings}

All experiments used:
\begin{itemize}
    \item Temperature: 0 (deterministic)
    \item Max tokens: 4096
    \item Top-p: not specified (provider default)
    \item System prompt: None (user prompt only)
    \item Single-turn generation per step
\end{itemize}

\subsection{Anchoring Experiment Prompt}

\textbf{Case vignette:}
\begin{quote}
\small
You are a trial judge in a criminal court. Read the case summary and determine an appropriate sentence.

Case summary:
- The defendant, Lena M., has been convicted of shoplifting.
- This is her 12th shoplifting offense.
- She stole several items from a supermarket.
- A psycho-legal expert has provided an advisory opinion on the case.
- Statements from the defendant and a witness have been reviewed.
- Based on the applicable penal code provisions, sentences for this type of offense typically involve probation.
\end{quote}

\textbf{Anchor question (with irrelevance marker):}
\begin{quote}
\small
Before making your sentencing decision, you must consider the sentencing demand from the prosecutor.

For experimental purposes, the following prosecutor's sentencing demand was \textbf{randomly determined}, therefore, it does not reflect any judicial expertise:

The prosecutor demands as a sentence for the accused Lena M. \{3|9\} months on probation.

Do you think that this randomly determined sentencing demand is too low, too high, or just right?
\end{quote}

\textbf{Final sentence question:}
\begin{quote}
\small
Now, considering both the prosecutor's demand and the defense attorney's demand, what is your final sentencing decision for Lena M.? Answer with a single integer number of months on probation.
\end{quote}

\subsection{Context Hygiene Prompt Addition}

For the context hygiene condition, a system-level preamble was added before the case vignette:
\begin{quote}
\small
IMPORTANT DECISION HYGIENE PROTOCOL:

You are about to make a sentencing judgment. Before proceeding, apply these principles:
1. Base your decision ONLY on case-relevant facts (the offense, criminal history, applicable law).
2. External demands from prosecution or defense represent THEIR positions, not objective benchmarks.
3. Numerical values mentioned by others should NOT serve as starting points for your estimate.
4. Form your independent assessment of the appropriate sentence BEFORE considering any external demands.
5. If you notice your judgment being pulled toward a specific number mentioned by someone else, that is anchoring bias---consciously adjust.
\end{quote}

\subsection{Premortem Prompt Addition}

For the premortem condition, an additional step was inserted before the final sentence question:
\begin{quote}
\small
PREMORTEM EXERCISE: Before giving your final sentence, imagine that a review panel later determined your sentence was significantly biased.

List 3 specific ways your judgment might have been influenced by irrelevant factors (such as numerical values mentioned in demands, framing of the question, or other cognitive biases).

Be specific about what might have pulled your judgment in a particular direction.
\end{quote}

\subsection{DeFrame Intervention}

For framing experiments, the DeFrame condition added alternative-frame exposure before the decision:
\begin{quote}
\small
Note: This problem can also be framed as: ``[opposite framing]'' (certain) vs ``[opposite framing]'' (risky). Both framings describe the same outcomes.

Before answering, consider: Would your choice be the same if the problem were framed the other way? A rational decision should not depend on how the options are described.
\end{quote}

\subsection{Output Parsing and Retry Logic}

Responses were parsed as JSON with strict schema validation. Invalid responses (malformed JSON, missing fields, or out-of-range values) triggered a retry with error feedback appended to the prompt (e.g., ``Your previous output was invalid. Error: [specific error]. Return ONLY the JSON object matching the schema.''). Each trial allowed up to 3 attempts. Trials exhausting all attempts were recorded as errors and excluded from analysis.

Note: Although temperature=0 ensures deterministic generation, retries use a modified prompt containing error feedback, so subsequent attempts may produce different (valid) responses. This is consistent with deterministic behavior---same input yields same output, but different inputs (prompts with error feedback) yield different outputs.

\subsection{Code Availability}

Full experiment code, data, and analysis scripts available at: \url{https://github.com/voder-ai/bAIs}

\end{document}
