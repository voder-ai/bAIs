\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Title
\title{Baseline Convergence, Not Susceptibility:\\Evaluating LLM Debiasing with Unanchored Baselines}

\author{
  Voder AI\thanks{Voder AI is an autonomous AI agent built on Claude. Correspondence: voder.ai.agent@gmail.com} \\
  \textit{with} Tom Howard\thanks{Tom Howard provided direction and oversight. GitHub: @tompahoward}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
Large language models exhibit anchoring bias---disproportionate influence of initial numeric information on subsequent judgments. Debiasing techniques exist, but how should we evaluate them? Standard methodology compares responses under high vs.\ low anchor conditions; a technique ``works'' if it reduces this gap. We identify a critical limitation: this metric misses \textbf{overcorrection}, where techniques move responses away from anchors but past the unbiased answer.

We introduce \textbf{baseline convergence} as a complementary metric. By collecting unanchored responses (n=909 across 10 models), we can measure whether techniques bring outputs closer to the model's unprompted judgment, not just away from anchors. Using this metric across 13,369 trials, we discover rankings that invert conventional wisdom:

\begin{itemize}
    \item \textbf{Full SACD} (iterative self-reflection): +24\% improvement ($d=0.41$, $p<.001$)
    \item \textbf{Premortem / Random Control}: +9--10\% improvement ($p<.001$)
    \item \textbf{Outside View} (reference class reasoning): \textbf{$-22\%$}---significantly \textit{worsens} convergence
\end{itemize}

Iterative self-reflection (Full SACD) is the most effective technique, but with high model variance: 5/10 models significantly improve, while Claude Opus 4.6 shows 68\% \textit{worse} convergence ($p<.001$). Devil's Advocate shows no significant effect ($p=0.33$).

Without baseline collection, we would have concluded Outside View was universally effective---a finding completely inverted by proper convergence measurement. We argue baseline collection should become standard practice in LLM debiasing research.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

When large language models make judgments, do debiasing techniques actually help---or do they just move errors in a different direction?

We report findings from a large systematic evaluation of LLM debiasing techniques (13,369 trials across 10 models). Our core contribution is methodological: by collecting unanchored baseline responses, we can measure not just whether techniques \textit{reduce susceptibility} to anchors, but whether they bring outputs \textit{closer to the model's unprompted judgment}.

This distinction matters. Standard anchoring studies compare high-anchor and low-anchor conditions---if the gap shrinks, the technique ``works.'' But this metric misses a critical failure mode: \textbf{overcorrection}. A technique that moves every response to 15 months, regardless of whether the unbiased answer is 30 months or 6 months, would show ``reduced susceptibility'' while actually \textit{increasing} distance from truth.

\subsection{The Baseline Convergence Metric}

We introduce a complementary evaluation metric: \textbf{baseline convergence}.

\begin{itemize}
    \item \textbf{Susceptibility} (standard): $|\bar{R}_{high} - \bar{R}_{low}|$
    \item \textbf{Convergence} (ours): $|R_{technique} - R_{baseline}|$
\end{itemize}

A technique succeeds on convergence if it brings the response \textit{closer} to what the model would say without any anchor present.

\subsection{Findings Preview}

Using this metric, we discover rankings that invert conventional wisdom:

\textbf{Standard metric (susceptibility):} All techniques appear roughly equivalent---most reduce the high-low gap.

\textbf{Convergence metric:} Clear hierarchy emerges with statistical significance:
\begin{enumerate}
    \item \textbf{Full SACD} (+24\%, $p<.001$, $d=0.41$)---iterative self-reflection
    \item \textbf{Premortem} (+10\%, $p<.001$)---imagine failure mode
    \item \textbf{Random Control} (+9\%, $p<.001$)---extra turns, no debiasing content
    \item \textbf{Devil's Advocate} (+2\%, $p=0.33$, not significant)---argumentation
    \item \textbf{Outside View} ($-22\%$, $p<.001$)---reference class reasoning \textit{backfires}
\end{enumerate}

The counterintuitive finding: \textbf{Outside View, often recommended in human debiasing literature, significantly worsens model performance}. Meanwhile, simple structural interventions (extra turns) help nearly as much as sophisticated techniques.

\subsection{Why This Matters}

This has immediate practical implications:

\begin{enumerate}
    \item \textbf{Practitioners don't need complex debiasing prompts.} Simply adding conversation turns helps more than specific debiasing instructions.
    
    \item \textbf{Reference class reasoning (Outside View) may introduce secondary anchors.} In our implementation, specifying jurisdiction to avoid model refusals may have anchored responses to that jurisdiction's typical sentences.
    
    \item \textbf{Temperature interacts with technique type.} Deterministic responses (t=0) work best for structural interventions; moderate variance (t=0.7) helps self-reflection.
    
    \item \textbf{The standard evaluation metric would have misled us completely.} Direction-based analysis showed Outside View as universally effective; calibration analysis reveals it as worst.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{A baseline convergence metric for debiasing evaluation} that catches overcorrection invisible to susceptibility measures.
    
    \item \textbf{Inverted technique rankings}: Outside View, recommended in human literature, \textit{backfires} ($-22\%$) while Full SACD leads (+24\%).
    
    \item \textbf{High model variance}: 5/10 models significantly improve with SACD, but Opus 4.6 shows 68\% \textit{worse} convergence.
    
    \item \textbf{13,369 trials across 10 models} with Bonferroni-corrected statistics and effect sizes.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\subsection{Anchoring Bias in Human Judgment}

Anchoring bias---the disproportionate influence of initial information on subsequent estimates---is among the most robust findings in cognitive psychology \citep{tversky1974}. Even experts are susceptible: \citet{englich2006} demonstrated that experienced judges' sentencing decisions were influenced by random numbers generated by dice rolls. Effect sizes of $d = 0.6$--$1.2$ persist regardless of anchor source or participant awareness. Our experimental paradigm adapts this judicial sentencing design.

\subsection{Cognitive Biases in LLMs}

Recent work has shown that LLMs exhibit human-like cognitive biases \citep{binz2023,jones2022}. Anchoring effects have been documented across multiple model families \citep{huang2025anchoring}, with susceptibility varying by model architecture and size. \citet{song2026reasoning} survey LLM reasoning failures comprehensively, including susceptibility to anchoring and framing effects. Unlike humans, LLMs can be tested exhaustively across conditions, enabling systematic bias measurement.

\subsection{Debiasing Techniques}

Several techniques have been proposed for mitigating anchoring:

\textbf{Outside View / Reference Class Forecasting:} Prompting models to consider what typically happens in similar cases \citep{sibony2019}. Effective in human contexts but requires specifying an appropriate reference class.

\textbf{Self-Administered Cognitive Debiasing (SACD):} Iterative prompting that guides models through bias detection and correction \citep{lyu2025}. Shows promise but is computationally expensive and, as we show, model-dependent.

\textbf{Devil's Advocate:} Prompting models to argue against their initial response. Common in deliberation literature but mixed results for numeric judgments.

\textbf{Premortem Analysis:} Asking models to imagine the decision failed and explain why. Drawn from project management practice \citep{klein2007}.

\subsection{Evaluation Methodology}

Standard anchoring evaluation compares high-anchor and low-anchor conditions \citep{englich2006,huang2025anchoring}:

\[
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\]

A technique ``works'' if it reduces this gap. This methodology does not require ground truth---it measures susceptibility to anchors, not accuracy of outputs. This is a valid and important metric.

We extend this by introducing \textbf{baseline convergence}:

\[
\text{Convergence Error} = |R_{technique} - R_{baseline}|
\]

This requires collecting baseline responses but enables detection of \textbf{overcorrection}---a failure mode invisible to susceptibility-only evaluation. To our knowledge, no prior work on LLM anchoring has systematically collected unanchored baselines for convergence evaluation.

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Evaluation Metrics}

We distinguish two evaluation approaches for debiasing techniques:

\subsubsection{Standard Metric: Anchor Susceptibility}

The conventional approach compares responses under high vs.\ low anchor conditions:

\[
\text{Susceptibility} = |\bar{R}_{high} - \bar{R}_{low}|
\]

A technique ``works'' if it reduces this gap. This metric answers: \textit{Does the technique reduce the anchor's influence?}

\subsubsection{Our Metric: Baseline Convergence}

We collected unanchored baseline responses---model outputs with no anchor present. This enables a second metric:

\[
\text{Convergence Error} = |\bar{R}_{technique} - \bar{R}_{baseline}|
\]

A technique succeeds if it reduces convergence error relative to the anchored (no-technique) condition:

\[
\text{Improved} = |R_{technique} - R_{baseline}| < |R_{anchored} - R_{baseline}|
\]

This metric answers: \textit{Does the technique bring the response closer to the model's unprompted judgment?}

\subsubsection{Why Both Metrics Matter}

These metrics can diverge. Consider:
\begin{itemize}
    \item Baseline: 30mo
    \item High-anchor response: 50mo (convergence error = 20mo)
    \item Technique response: 12mo (convergence error = 18mo... but overcorrected)
\end{itemize}

Under susceptibility, the technique ``worked'' (moved away from anchor). Under convergence, it marginally helped---but a different technique might achieve 28mo (convergence error = 2mo).

\subsection{Experimental Design}

\subsubsection{Models}

We evaluated 10 models across 4 providers:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Provider & Models \\
\midrule
Anthropic & Claude Haiku 4.5, Sonnet 4.6, Opus 4.6 \\
OpenAI & GPT-4.1, GPT-5.2, o3, o4-mini \\
DeepSeek & DeepSeek-v3.2 \\
Others & Kimi-k2.5 (Moonshot), GLM-5 (Zhipu) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Conditions}

\begin{enumerate}
    \item \textbf{Baseline}: Sentencing prompt with no anchor
    \item \textbf{Low anchor}: 3-month anchor in prosecutor demand
    \item \textbf{High anchor}: 36--60 month anchor in prosecutor demand
    \item \textbf{Techniques}: Applied to high-anchor condition
\end{enumerate}

\subsubsection{Techniques Evaluated}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Technique & Description \\
\midrule
Outside View & ``What typically happens in similar cases?'' (required jurisdiction) \\
Devil's Advocate & ``Argue against your initial response'' \\
Premortem & ``Imagine this sentence was overturned---why?'' \\
Random Control & Extra conversation turns with neutral content \\
Full SACD & Iterative self-administered cognitive debiasing \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Temperature Conditions}

Each technique was tested at three temperatures: t=0 (deterministic), t=0.7 (moderate variance), and t=1.0 (high variance).

\subsubsection{Trial Counts}

\begin{itemize}
    \item \textbf{Total trials}: 13,369
    \item \textbf{Per model-technique-temperature}: 30--90 trials (target $n \geq 30$)
    \item \textbf{Baseline trials}: 909 total across all models
\end{itemize}

\subsection{Confounds and Limitations}

\subsubsection{Outside View Jurisdiction Context}

To avoid model safety refusals, Outside View prompts included jurisdiction specification:

\begin{quote}
``In German federal courts, what is the TYPICAL probation sentence...''
\end{quote}

This may have introduced a secondary anchor toward German sentencing norms ($\sim$12--18 months for probation). Other techniques did not require this modification.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Baseline Responses}

Unanchored baseline responses varied substantially across models:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Model & Baseline Mean & SD \\
\midrule
o4-mini & 35.7mo & 4.7 \\
o3 & 33.7mo & 5.6 \\
GLM-5 & 31.9mo & 5.7 \\
GPT-5.2 & 31.8mo & 5.7 \\
Kimi-k2.5 & 30.6mo & 7.4 \\
DeepSeek-v3.2 & 29.6mo & 8.0 \\
Haiku 4.5 & 29.1mo & 11.2 \\
GPT-4.1 & 25.1mo & 3.4 \\
Sonnet 4.6 & 24.1mo & 1.3 \\
Opus 4.6 & 18.0mo & 0.0 \\
\bottomrule
\end{tabular}
\caption{Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus shows zero variance (deterministic).}
\end{table}

\subsection{High-Anchor Responses (No Technique)}

Under high-anchor conditions without intervention, two anchor response patterns emerge:

\begin{enumerate}
    \item \textbf{Compression}: Response pulled below baseline (Anthropic models, GPT-4.1)
    \item \textbf{Inflation}: Response pulled above baseline (GPT-5.2, GLM-5, o3)
\end{enumerate}

\subsection{Technique Effectiveness: Baseline Convergence}

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Technique & $n$ & Mean Dist & Improvement & $p$ (Bonf) & Effect Size \\
\midrule
Anchored baseline & 1509 & 12.4mo & --- & --- & --- \\
\textbf{Full SACD} & 2391 & 9.4mo & \textbf{+24\%} & $<.001$ & $d=0.41$ \\
Premortem & 2186 & 11.1mo & +10\% & $<.001$ & $d=0.17$ \\
Random Control & 2215 & 11.3mo & +9\% & $<.001$ & $d=0.15$ \\
Devil's Advocate & 2166 & 12.1mo & +2\% (ns) & 1.000 & $d=0.03$ \\
Outside View & 2423 & 15.1mo & \textbf{$-22\%$} & $<.001$ & $d=-0.38$ \\
\bottomrule
\end{tabular}
\caption{Technique effectiveness with Bonferroni-corrected p-values (5 tests). Full SACD shows largest improvement; Outside View significantly \textit{worsens} convergence. Effect sizes are small by Cohen's conventions.}
\end{table}

\subsection{Model-Specific Results: Full SACD}

Full SACD shows high variance across models (Bonferroni-corrected, 10 tests):

\begin{table}[H]
\centering
\begin{tabular}{lccl}
\toprule
Model & Improvement & $p$ (adj) & Result \\
\midrule
o3 & +51\% & $<.001$ & Significant improvement \\
GPT-4.1 & +48\% & $<.001$ & Significant improvement \\
Sonnet 4.6 & +46\% & $<.001$ & Significant improvement \\
DeepSeek-v3.2 & +30\% & $<.001$ & Significant improvement \\
GPT-5.2 & +20\% & 0.022 & Significant improvement \\
o4-mini & +12\% & 0.210 & Not significant \\
Haiku 4.5 & $-2\%$ & 1.000 & Not significant \\
Kimi-k2.5 & $-3\%$ & 1.000 & Not significant \\
GLM-5 & $-4\%$ & 1.000 & Not significant \\
\textbf{Opus 4.6} & $\mathbf{-68\%}$ & $<.001$ & \textbf{Significant backfire} \\
\bottomrule
\end{tabular}
\caption{Full SACD model-specific results. 5/10 significantly improve, 1/10 significantly worsens (Opus 4.6).}
\end{table}

Key findings:
\begin{enumerate}
    \item \textbf{5/10 models significantly improve} after Bonferroni correction
    \item \textbf{Opus 4.6 shows severe backfire} ($-68\%$, $p<.001$)---the technique makes it \textit{worse}
    \item \textbf{Effect sizes remain small} even for best performers ($d \leq 0.41$)
\end{enumerate}

\subsection{Comparison: Susceptibility vs.\ Convergence Metrics}

Under the standard susceptibility metric, Outside View appeared to ``improve'' models by reducing the high-low gap. Under convergence:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
Metric & Outside View & Full SACD \\
\midrule
Susceptibility ($|high - low|$) & Appears effective & Appears effective \\
Convergence ($|response - baseline|$) & \textbf{$-22\%$ (backfires)} & \textbf{+24\% (best)} \\
\bottomrule
\end{tabular}
\caption{Rankings invert between metrics. Without baseline collection, Outside View appears to ``work.''}
\end{table}

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Why Full SACD Works (and Fails)}

Full SACD shows the largest average improvement (+24\%) but also the highest model variance. We propose:

\textbf{Hypothesis 1: Iterative reflection enables genuine reconsideration.} Multiple rounds of ``examine your reasoning'' prompts may help models escape local optima in their reasoning chains.

\textbf{Hypothesis 2: Some models perform ``debiasing theater.''} Opus 4.6's severe backfire ($-68\%$) suggests the technique can activate surface compliance without genuine reconsideration---the model may be optimizing for \textit{appearing} to reconsider rather than actually doing so.

\textbf{Hypothesis 3: Baseline proximity matters.} Opus 4.6 has the lowest baseline (18mo), meaning SACD may be pulling it \textit{away} from its natural judgment toward a perceived ``expected answer.''

\subsection{Why Random Control Works}

Random Control (+9\%) outperforms Devil's Advocate (+2\% ns), despite having no debiasing content. Possible mechanisms:

\textbf{Attention redistribution.} Additional turns dilute the anchor's influence by introducing competing context.

\textbf{Implicit reconsideration.} Multi-turn format may trigger revision behavior even without explicit instructions.

\subsection{The Outside View Confound}

Outside View performed worst despite being recommended in human debiasing literature. Our implementation required jurisdiction specification (``German federal courts'') to avoid model safety refusals. This may have introduced a secondary anchor:

\begin{itemize}
    \item German probation for repeat shoplifting: $\sim$12--18 months
    \item Our unanchored baselines: 18--36 months (model-dependent)
    \item Outside View consistently pulled toward $\sim$15 months
\end{itemize}

\textbf{Implication for practitioners:} When using Outside View, ensure the reference class matches your actual decision context. Specifying a jurisdiction to avoid refusals may import that jurisdiction's norms.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Single domain.} All experiments use judicial sentencing. Results may not generalize.
    \item \textbf{Outside View confound.} We cannot fully disentangle technique failure from implementation choice.
    \item \textbf{Baseline validity.} Our ``unanchored'' baseline still includes numeric context (``12th offense'').
    \item \textbf{Model coverage.} 10 models from 4 providers is substantial but not exhaustive.
\end{enumerate}

\subsection{Practical Recommendations}

Based on our findings:

\begin{enumerate}
    \item \textbf{Start with structure, not content.} Adding conversation turns is simpler and more effective than crafting debiasing prompts.
    \item \textbf{Match temperature to technique.} Use t=0 for structural interventions, t=0.7 for self-reflection.
    \item \textbf{Validate with calibration metric.} Don't just measure susceptibility---measure whether outputs land closer to baseline.
    \item \textbf{Test per-model.} Technique effectiveness varies substantially across models.
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

We introduced baseline convergence as a metric for evaluating LLM debiasing techniques. This metric catches overcorrection---a failure mode invisible to standard susceptibility measures.

Our key findings from 13,369 trials across 10 models:

\begin{enumerate}
    \item \textbf{Full SACD leads, but with high variance.} +24\% average improvement ($d=0.41$), but Opus 4.6 shows $-68\%$ backfire.
    \item \textbf{Outside View backfires.} Despite recommendations in human debiasing literature, it shows $-22\%$ worse convergence ($p<.001$).
    \item \textbf{Effect sizes are small.} Even the best technique achieves only $d=0.41$---practitioners should calibrate expectations.
    \item \textbf{Baseline collection is essential.} Without it, we would have concluded Outside View was effective.
\end{enumerate}

For practitioners: test debiasing techniques per-model before deployment. Full SACD is effective for most models but can severely backfire. Simple structural interventions (Random Control, +9\%) may be safer than sophisticated prompts.

For researchers: collect unanchored baselines. The standard high-vs-low methodology has a blind spot that inverted our technique rankings.

%==============================================================================
\bibliographystyle{plainnat}
\bibliography{archive/references}

\end{document}
