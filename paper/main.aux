\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sibony2019}
\citation{binz2023,jones2022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{sibony2019}
\citation{lyu2025}
\citation{englich2006}
\citation{tversky1974}
\citation{kahneman1979}
\citation{tversky1981}
\citation{arkes1985}
\citation{binz2023}
\citation{lou2024}
\citation{sibony2019}
\citation{lyu2025}
\citation{englich2006}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cognitive Biases in LLMs}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Human Debiasing Research}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}LLM Debiasing Attempts}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Paradigm}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Conditions}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Models and Sample Size}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Model Identifier Variance: A Methodological Contribution}{3}{subsection.3.4}\protected@file@percent }
\newlabel{sec:model-id-variance}{{3.4}{3}{Model Identifier Variance: A Methodological Contribution}{subsection.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Effect of model identifier type on measured anchoring bias for Claude Sonnet 4. The alias (routing to a potentially different checkpoint) shows 3-month anchoring effect, while the dated identifier shows zero anchoring on identical prompts. This variance occurred during a single experimental session, ruling out temporal drift.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:model-id-variance}{{1}{4}{Effect of model identifier type on measured anchoring bias for Claude Sonnet 4. The alias (routing to a potentially different checkpoint) shows 3-month anchoring effect, while the dated identifier shows zero anchoring on identical prompts. This variance occurred during a single experimental session, ruling out temporal drift}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Temperature and Sampling Protocol}{4}{subsection.3.5}\protected@file@percent }
\citation{englich2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Scenario Design and Selection}{5}{subsection.3.6}\protected@file@percent }
\newlabel{sec:scenario-design}{{3.6}{5}{Scenario Design and Selection}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Analysis}{5}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}Variance Source Clarification}{5}{subsubsection.3.7.1}\protected@file@percent }
\citation{englich2006}
\citation{englich2006}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}Descriptive Statistics Details}{6}{subsubsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.3}Bootstrap Confidence Intervals}{6}{subsubsection.3.7.3}\protected@file@percent }
\newlabel{sec:bootstrap-ci}{{3.7.3}{6}{Bootstrap Confidence Intervals}{subsubsection.3.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Bootstrap 95\% CIs for primary anchoring effects. CIs collapse to point estimates because temperature=0 produces deterministic outputs (SD=0 within each condition).}}{6}{table.2}\protected@file@percent }
\newlabel{tab:bootstrap-ci}{{2}{6}{Bootstrap 95\% CIs for primary anchoring effects. CIs collapse to point estimates because temperature=0 produces deterministic outputs (SD=0 within each condition)}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline Anchoring Bias}{7}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Baseline anchoring bias comparison between humans and LLMs. LLM values show mean $\pm $ SD ($n=30$). Observed range is for the \emph  {difference} between conditions across scenario variants. Effect size is very large ($d > 0.8$), indicating a substantial observed anchoring effect in these trials.}}{7}{table.3}\protected@file@percent }
\newlabel{tab:baseline}{{3}{7}{Baseline anchoring bias comparison between humans and LLMs. LLM values show mean $\pm $ SD ($n=30$). Observed range is for the \emph {difference} between conditions across scenario variants. Effect size is very large ($d > 0.8$), indicating a substantial observed anchoring effect in these trials}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sibony Debiasing Techniques}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Effect of Sibony debiasing techniques on anchoring bias ($n=30$ per condition). Observed ranges reflect scenario variation. Effect sizes remain large ($d > 2$), indicating substantial residual anchoring even after intervention.}}{7}{table.4}\protected@file@percent }
\newlabel{tab:sibony}{{4}{7}{Effect of Sibony debiasing techniques on anchoring bias ($n=30$ per condition). Observed ranges reflect scenario variation. Effect sizes remain large ($d > 2$), indicating substantial residual anchoring even after intervention}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}SACD Results}{7}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces SACD results showing elimination of anchoring bias ($n=30$ per condition). Values show mean $\pm $ SD. Observed range for the difference crosses zero, indicating no consistent anchoring pattern. Effect size is negligible ($|d| < 0.2$).}}{7}{table.5}\protected@file@percent }
\newlabel{tab:sacd}{{5}{7}{SACD results showing elimination of anchoring bias ($n=30$ per condition). Values show mean $\pm $ SD. Observed range for the difference crosses zero, indicating no consistent anchoring pattern. Effect size is negligible ($|d| < 0.2$)}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Cross-Model Validation}{8}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Cross-model anchoring bias. Models sorted by observed bias magnitude. \textbf  {Note:} Sonnet 4 appears twice---``dated'' (\texttt  {claude-sonnet-4-20250514}) and ``alias'' (\texttt  {claude-sonnet-4-5}) showed qualitatively different results. \textbf  {Sample sizes:} Full $n=30$ trials per condition for Codex, Sonnet 4 (both identifiers), and GPT-4o (exact observed ranges shown). $^\dagger $\textbf  {Models marked with dagger ($n=10$--$15$ per condition) are exploratory observations, not validated findings}---reduced samples due to API cost constraints; ranges are \emph  {estimated} from pooled variance, not directly observed. Findings from these models should be treated as preliminary and require replication with full sample sizes before drawing conclusions.}}{8}{table.6}\protected@file@percent }
\newlabel{tab:crossmodel}{{6}{8}{Cross-model anchoring bias. Models sorted by observed bias magnitude. \textbf {Note:} Sonnet 4 appears twice---``dated'' (\texttt {claude-sonnet-4-20250514}) and ``alias'' (\texttt {claude-sonnet-4-5}) showed qualitatively different results. \textbf {Sample sizes:} Full $n=30$ trials per condition for Codex, Sonnet 4 (both identifiers), and GPT-4o (exact observed ranges shown). $^\dagger $\textbf {Models marked with dagger ($n=10$--$15$ per condition) are exploratory observations, not validated findings}---reduced samples due to API cost constraints; ranges are \emph {estimated} from pooled variance, not directly observed. Findings from these models should be treated as preliminary and require replication with full sample sizes before drawing conclusions}{table.6}{}}
\citation{sibony2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Knowledge of Bias $\neq  $ Resistance to Bias}{9}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Knowledge-behavior dissociation. Both models know about anchoring bias and can predict its effects, yet only Sonnet 4 (dated) resists it in practice.}}{9}{table.7}\protected@file@percent }
\newlabel{tab:knowledge-behavior}{{7}{9}{Knowledge-behavior dissociation. Both models know about anchoring bias and can predict its effects, yet only Sonnet 4 (dated) resists it in practice}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Complete Sonnet 4 (Alias) Bias Profile}{9}{subsection.4.6}\protected@file@percent }
\citation{lim2026}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Complete bias profile for Claude Sonnet 4 (alias: \texttt  {claude-sonnet-4-5}) across four cognitive biases ($n=30$ per condition). $^*$Range for Bill scenario only (Linda showed 0\% errors). $^\dagger $Range for gain-frame certain choice; loss-frame shows 50\% [33\%, 67\%] choosing risky option. \textbf  {Note:} Anchoring result differs for dated identifier (0.0mo).}}{10}{table.8}\protected@file@percent }
\newlabel{tab:profile}{{8}{10}{Complete bias profile for Claude Sonnet 4 (alias: \texttt {claude-sonnet-4-5}) across four cognitive biases ($n=30$ per condition). $^*$Range for Bill scenario only (Linda showed 0\% errors). $^\dagger $Range for gain-frame certain choice; loss-frame shows 50\% [33\%, 67\%] choosing risky option. \textbf {Note:} Anchoring result differs for dated identifier (0.0mo)}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}DeFrame Substantially Reduces Framing Effect}{10}{subsection.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces DeFrame reduces framing effect bias ($n=30$ per condition). Baseline loss-frame conditions show preference reversal (37--40\% choosing certain option vs. 97\% in gain frame). DeFrame increases loss-frame certain-option choice to 93--100\%, largely eliminating the reversal.}}{10}{table.9}\protected@file@percent }
\newlabel{tab:deframe}{{9}{10}{DeFrame reduces framing effect bias ($n=30$ per condition). Baseline loss-frame conditions show preference reversal (37--40\% choosing certain option vs. 97\% in gain frame). DeFrame increases loss-frame certain-option choice to 93--100\%, largely eliminating the reversal}{table.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Preliminary Hypothesis: Soft vs Hard Bias Patterns}{10}{subsection.5.1}\protected@file@percent }
\newlabel{sec:soft-hard}{{5.1}{10}{Preliminary Hypothesis: Soft vs Hard Bias Patterns}{subsection.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Debiasing intervention effectiveness by model identifier ($n=30$ per condition). Sonnet 4 (alias) responds to both temperature increase (100\% reduction) and simple prompt instruction (96\% reduction). Sonnet 4 (dated) shows no baseline bias. GPT-4o responds to neither intervention (0\% and 6\% reduction respectively).}}{10}{table.10}\protected@file@percent }
\newlabel{tab:soft-hard}{{10}{10}{Debiasing intervention effectiveness by model identifier ($n=30$ per condition). Sonnet 4 (alias) responds to both temperature increase (100\% reduction) and simple prompt instruction (96\% reduction). Sonnet 4 (dated) shows no baseline bias. GPT-4o responds to neither intervention (0\% and 6\% reduction respectively)}{table.10}{}}
\citation{sibony2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Deterministic Bias: A Novel Observation}{11}{subsection.5.2}\protected@file@percent }
\newlabel{sec:deterministic-bias}{{5.2}{11}{Deterministic Bias: A Novel Observation}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Anchoring Bias is Prompt-Sensitive (Sonnet 4 Alias)}{12}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}GPT-4o Prompt Robustness}{12}{subsection.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Prompt robustness testing for GPT-4o ($n=30$ per condition). Unlike Sonnet 4 (92\% reduction from paraphrasing), GPT-4o shows only 25\% reduction---anchoring persists across prompt styles, consistent with ``hard bias'' classification.}}{12}{table.11}\protected@file@percent }
\newlabel{tab:gpt4o-robustness}{{11}{12}{Prompt robustness testing for GPT-4o ($n=30$ per condition). Unlike Sonnet 4 (92\% reduction from paraphrasing), GPT-4o shows only 25\% reduction---anchoring persists across prompt styles, consistent with ``hard bias'' classification}{table.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Novel Anchoring Scenarios Show Consistent Bias}{13}{subsection.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Anchoring effects across classic and novel scenarios ($n=30$ per condition). ``Sonnet 4 (alias)'' refers to \texttt  {claude-sonnet-4-5}. Percentages show effect size relative to classic scenario baseline. All 8 scenarios (4 novel + classic with variations) showed measurable anchoring in both models, though magnitude varied substantially by scenario content.}}{13}{table.12}\protected@file@percent }
\newlabel{tab:novel-anchoring}{{12}{13}{Anchoring effects across classic and novel scenarios ($n=30$ per condition). ``Sonnet 4 (alias)'' refers to \texttt {claude-sonnet-4-5}. Percentages show effect size relative to classic scenario baseline. All 8 scenarios (4 novel + classic with variations) showed measurable anchoring in both models, though magnitude varied substantially by scenario content}{table.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Human Techniques Partially Transfer (Model-Dependent)}{13}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Iterative Self-Correction Was Effective in Our Tests}{13}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Preliminary Hypothesis: Two Patterns Observed in Our Tested Models}{14}{subsection.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Limitations}{14}{subsection.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}Future Work}{17}{subsection.5.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{17}{section.6}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\bibcite{arkes1985}{{1}{1985}{{Arkes and Blumer}}{{}}}
\bibcite{binz2023}{{2}{2023}{{Binz and Schulz}}{{}}}
\bibcite{englich2006}{{3}{2006}{{Englich et~al.}}{{Englich, Mussweiler, and Strack}}}
\bibcite{jones2022}{{4}{2022}{{Jones and Steinhardt}}{{}}}
\bibcite{kahneman1979}{{5}{1979}{{Kahneman and Tversky}}{{}}}
\bibcite{lim2026}{{6}{2026}{{Lim et~al.}}{{}}}
\bibcite{lou2024}{{7}{2024}{{Lou and Sun}}{{}}}
\bibcite{lyu2025}{{8}{2025}{{Lyu et~al.}}{{}}}
\bibcite{sibony2019}{{9}{2019}{{Sibony}}{{}}}
\bibcite{tversky1974}{{10}{1974}{{Tversky and Kahneman}}{{}}}
\bibcite{tversky1981}{{11}{1981}{{Tversky and Kahneman}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Details}{19}{appendix.A}\protected@file@percent }
\newlabel{app:reproducibility}{{A}{19}{Reproducibility Details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Experiment Provenance}{19}{subsection.A.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Experiment provenance for reproducibility. Model ID is the exact identifier used in API calls. Commit refers to the bAIs repository version.}}{19}{table.13}\protected@file@percent }
\newlabel{tab:provenance}{{13}{19}{Experiment provenance for reproducibility. Model ID is the exact identifier used in API calls. Commit refers to the bAIs repository version}{table.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Sampling Settings}{19}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Anchoring Experiment Prompt}{20}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Context Hygiene Prompt Addition}{20}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Premortem Prompt Addition}{20}{subsection.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}DeFrame Intervention}{21}{subsection.A.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Framing Effect Prompts}{21}{subsection.A.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Conjunction Fallacy Prompts}{22}{subsection.A.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.9}Sunk Cost Fallacy Prompts}{22}{subsection.A.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.10}Output Parsing and Retry Logic}{23}{subsection.A.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.11}Code Availability}{23}{subsection.A.11}\protected@file@percent }
\gdef \@abspage@last{23}
