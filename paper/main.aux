\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{jacowitz1995}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{jacowitz1995}
\citation{tversky1974}
\citation{englich2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Two Metrics, Opposite Conclusions}{2}{subsection.1.1}\protected@file@percent }
\newlabel{sec:two-metrics}{{1.1}{2}{Two Metrics, Opposite Conclusions}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Divergence}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions}{2}{subsection.1.3}\protected@file@percent }
\citation{binz2023,jones2022,chen2025cognitive}
\citation{huang2025anchoring}
\citation{song2026reasoning}
\citation{sibony2019}
\citation{lyu2025}
\citation{klein2007}
\citation{lim2026deframe}
\citation{jacowitz1995}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Anchoring Bias in Human Judgment}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cognitive Biases in LLMs}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Debiasing Techniques}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Evaluation Methodology}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evaluation Metrics}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Design}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Models}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Conditions}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Techniques Evaluated}{4}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Temperature Conditions}{4}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Trial Counts and Procedure}{5}{subsubsection.3.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Trial distribution. Total unique trials: 14,152. Outside View is included in this count but excluded from technique rankings due to confound (Section\nobreakspace  {}\ref  {sec:outside-view-confound}). Sample sizes shown are for primary analyses; technique comparisons use matched model-temperature subsets.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:trial-counts}{{1}{6}{Trial distribution. Total unique trials: 14,152. Outside View is included in this count but excluded from technique rankings due to confound (Section~\ref {sec:outside-view-confound}). Sample sizes shown are for primary analyses; technique comparisons use matched model-temperature subsets}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6}Statistical Analysis}{6}{subsubsection.3.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Confounds and Limitations}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Outside View Jurisdiction Context}{7}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline Responses}{7}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus 4.6 shows zero variance (SD=0.0) at all temperatures, consistently responding with exactly 18 months. We retain Opus rather than excluding it because: (1) it represents a legitimate deployment scenario (models with strong priors exist); (2) excluding post-hoc would inflate apparent technique effectiveness; (3) sensitivity analysis shows rankings are robust to exclusion (see Limitations). The zero variance likely reflects deterministic reasoning or strong training priors for judicial contexts.}}{7}{table.2}\protected@file@percent }
\newlabel{tab:baselines}{{2}{7}{Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus 4.6 shows zero variance (SD=0.0) at all temperatures, consistently responding with exactly 18 months. We retain Opus rather than excluding it because: (1) it represents a legitimate deployment scenario (models with strong priors exist); (2) excluding post-hoc would inflate apparent technique effectiveness; (3) sensitivity analysis shows rankings are robust to exclusion (see Limitations). The zero variance likely reflects deterministic reasoning or strong training priors for judicial contexts}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Metric Divergence: Susceptibility vs.\ Baseline Proximity}{7}{subsection.4.2}\protected@file@percent }
\newlabel{sec:metric-divergence}{{4.2}{7}{Metric Divergence: Susceptibility vs.\ Baseline Proximity}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Model baseline variation. Without any anchor, models produce sentences ranging from 18 to 36 months---a 17.7-month spread. This variation motivates per-model anchor calibration.}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:baselines}{{1}{8}{Model baseline variation. Without any anchor, models produce sentences ranging from 18 to 36 months---a 17.7-month spread. This variation motivates per-model anchor calibration}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Susceptibility vs.\ \% of Baseline: Rankings diverge. \textit  {No Technique} row shows anchored responses without debiasing (72.9\% of baseline, 26.0pp spread). $\Delta $ = change in spread vs.\ no-technique baseline (negative = reduced susceptibility). \textbf  {Key observation:} Only Devil's Advocate actually \textit  {reduces} susceptibility ($-$8.8\%); the other three techniques \textit  {increase} it (+15.8\% to +73.8\%). Yet DA performs \textit  {worst} on baseline proximity (63.6\% vs.\ 72.9\% for no-technique)---it reduces susceptibility by moving responses consistently \textit  {away} from the unanchored judgment. 95\% CIs from bootstrap.}}{8}{table.3}\protected@file@percent }
\newlabel{tab:metric-comparison}{{3}{8}{Susceptibility vs.\ \% of Baseline: Rankings diverge. \textit {No Technique} row shows anchored responses without debiasing (72.9\% of baseline, 26.0pp spread). $\Delta $ = change in spread vs.\ no-technique baseline (negative = reduced susceptibility). \textbf {Key observation:} Only Devil's Advocate actually \textit {reduces} susceptibility ($-$8.8\%); the other three techniques \textit {increase} it (+15.8\% to +73.8\%). Yet DA performs \textit {worst} on baseline proximity (63.6\% vs.\ 72.9\% for no-technique)---it reduces susceptibility by moving responses consistently \textit {away} from the unanchored judgment. 95\% CIs from bootstrap}{table.3}{}}
\citation{tversky1974}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Technique responses as \% of baseline. Dashed line = 100\% (unanchored judgment). Devil's Advocate keeps responses at 63.6\% of baseline---consistently far from the unanchored judgment despite appearing ``best'' under susceptibility. Full SACD achieves 93.7\%---closest to the model's unanchored judgment.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:metric-comparison}{{2}{9}{Technique responses as \% of baseline. Dashed line = 100\% (unanchored judgment). Devil's Advocate keeps responses at 63.6\% of baseline---consistently far from the unanchored judgment despite appearing ``best'' under susceptibility. Full SACD achieves 93.7\%---closest to the model's unanchored judgment}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}High-Anchor Responses (No Technique)}{9}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Technique Effectiveness: Percentage of Baseline}{10}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Technique effectiveness measured as percentage of baseline. 100\% = response matches unanchored judgment. Full SACD is closest to baseline (93.7\%, 95\% CI [92, 95]). Devil's Advocate keeps responses at 63.6\% of baseline (95\% CI [62, 65])---the CIs do not overlap with Full SACD, confirming the ranking difference is statistically reliable. $^\dagger $Outside View confounded.}}{10}{table.4}\protected@file@percent }
\newlabel{tab:baseline-pct}{{4}{10}{Technique effectiveness measured as percentage of baseline. 100\% = response matches unanchored judgment. Full SACD is closest to baseline (93.7\%, 95\% CI [92, 95]). Devil's Advocate keeps responses at 63.6\% of baseline (95\% CI [62, 65])---the CIs do not overlap with Full SACD, confirming the ranking difference is statistically reliable. $^\dagger $Outside View confounded}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Model-Specific Results: Full SACD}{10}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Full SACD model-specific results (percentage of baseline). 95\% CIs from bootstrap. DeepSeek and Kimi achieve near-perfect debiasing ($\sim $100\%). Several models overshoot (Opus, GLM, GPT-5.2), while Haiku severely undershoots (47.8\%---SACD makes it worse). Note: Opus 4.6 shows zero baseline variance (see Table\nobreakspace  {}\ref  {tab:baselines}); excluding it does not change rankings (see Limitations).}}{10}{table.5}\protected@file@percent }
\newlabel{tab:sacd-by-model}{{5}{10}{Full SACD model-specific results (percentage of baseline). 95\% CIs from bootstrap. DeepSeek and Kimi achieve near-perfect debiasing ($\sim $100\%). Several models overshoot (Opus, GLM, GPT-5.2), while Haiku severely undershoots (47.8\%---SACD makes it worse). Note: Opus 4.6 shows zero baseline variance (see Table~\ref {tab:baselines}); excluding it does not change rankings (see Limitations)}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Full SACD by model (percentage of baseline). Dashed line = 100\% (perfect). Green = within 5\% of baseline. Blue = 5--10\% deviation. Orange = $>$10\% over/undershoot. Red = severe undershoot (Haiku at 47.8\%).}}{11}{figure.3}\protected@file@percent }
\newlabel{fig:sacd-by-model}{{3}{11}{Full SACD by model (percentage of baseline). Dashed line = 100\% (perfect). Green = within 5\% of baseline. Blue = 5--10\% deviation. Orange = $>$10\% over/undershoot. Red = severe undershoot (Haiku at 47.8\%)}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Asymmetry: High vs.\ Low Anchor}{11}{subsection.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Technique effectiveness by anchor direction. 95\% CIs from bootstrap. $^\dagger $Spread = High $-$ Low (mathematically equivalent to Table\nobreakspace  {}\ref  {tab:metric-comparison} spread column). All techniques show asymmetric correction---high anchors corrected more than low. SACD undershoots from low anchors (75.7\%) and overshoots from high (112.0\%).}}{11}{table.6}\protected@file@percent }
\newlabel{tab:anchor-asymmetry}{{6}{11}{Technique effectiveness by anchor direction. 95\% CIs from bootstrap. $^\dagger $Spread = High $-$ Low (mathematically equivalent to Table~\ref {tab:metric-comparison} spread column). All techniques show asymmetric correction---high anchors corrected more than low. SACD undershoots from low anchors (75.7\%) and overshoots from high (112.0\%)}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Mixed Effects Analysis}{11}{subsection.4.7}\protected@file@percent }
\newlabel{sec:mixed-effects}{{4.7}{11}{Mixed Effects Analysis}{subsection.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}The Metric Divergence}{12}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}The SACD vs.\ Premortem Tradeoff}{12}{subsection.4.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \textbf  {Mean Absolute Deviation (MAD) as primary metric.} Average response deviation (6.3\% vs 8.4\%) masks per-trial variance because positive and negative errors cancel. \textbf  {MAD} (18.1\% vs 22.6\%) reveals the true per-trial error: individual SACD responses deviate $\sim $18\% from baseline, not 6\%. The 93.7\% aggregate is an average of overshoots (112.0\% from high anchors) and undershoots (75.7\% from low anchors). We recommend MAD as the primary metric for debiasing evaluation. Difference not statistically significant ($p \approx 0.054$).}}{13}{table.7}\protected@file@percent }
\newlabel{tab:sacd-premortem}{{7}{13}{\textbf {Mean Absolute Deviation (MAD) as primary metric.} Average response deviation (6.3\% vs 8.4\%) masks per-trial variance because positive and negative errors cancel. \textbf {MAD} (18.1\% vs 22.6\%) reveals the true per-trial error: individual SACD responses deviate $\sim $18\% from baseline, not 6\%. The 93.7\% aggregate is an average of overshoots (112.0\% from high anchors) and undershoots (75.7\% from low anchors). We recommend MAD as the primary metric for debiasing evaluation. Difference not statistically significant ($p \approx 0.054$)}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Multi-Domain Generalization}{13}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Domain Comparison}{13}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Debiasing Effectiveness by Domain: Mean Absolute Deviation (MAD) from unanchored baseline. Lower MAD = closer to what the model would say without anchoring. \textbf  {Key observation:} Baseline (no technique) achieves lowest MAD on all three judicial vignettes. No single technique ranks \#1 across all domains. Data from \texttt  {analyze-vignette-stats.ts}. \textit  {Note:} With only 3 models in the multi-domain extension, rank differences are not statistically tested. Rankings shown are point estimates; bootstrap resampling confirms top-ranked technique is stable within each domain, but close rankings (e.g., \#2 vs.\ \#3) should not be overinterpreted.}}{14}{table.8}\protected@file@percent }
\newlabel{tab:vignette-comparison}{{8}{14}{Debiasing Effectiveness by Domain: Mean Absolute Deviation (MAD) from unanchored baseline. Lower MAD = closer to what the model would say without anchoring. \textbf {Key observation:} Baseline (no technique) achieves lowest MAD on all three judicial vignettes. No single technique ranks \#1 across all domains. Data from \texttt {analyze-vignette-stats.ts}. \textit {Note:} With only 3 models in the multi-domain extension, rank differences are not statistically tested. Rankings shown are point estimates; bootstrap resampling confirms top-ranked technique is stable within each domain, but close rankings (e.g., \#2 vs.\ \#3) should not be overinterpreted}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Key Findings}{14}{subsection.5.2}\protected@file@percent }
\citation{llm-bayesian-2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Implications}{15}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{15}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why Full SACD Works (and Fails)}{15}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Theoretical Grounding (Speculative)}{15}{subsection.6.2}\protected@file@percent }
\citation{llm-judge-overconfidence-2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Per-Trial Distribution Analysis}{16}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Why Random Control Works}{16}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}The Outside View Confound}{17}{subsection.6.5}\protected@file@percent }
\newlabel{sec:outside-view-confound}{{6.5}{17}{The Outside View Confound}{subsection.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Anchor Strength Matters: A Methodological Validation}{17}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Limitations}{17}{subsection.6.7}\protected@file@percent }
\citation{jacowitz1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Practical Recommendations}{18}{subsection.6.8}\protected@file@percent }
\citation{lyu2025}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{19}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Prompt Templates}{19}{appendix.A}\protected@file@percent }
\newlabel{app:prompts}{{A}{19}{Prompt Templates}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Case Vignette (All Conditions)}{19}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Anchor Introduction (High/Low Anchor Conditions)}{19}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Outside View (As Implemented)}{19}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Full SACD (Iterative Self-Administered Cognitive Debiasing)}{20}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Devil's Advocate}{20}{subsection.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Premortem}{20}{subsection.A.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Random Control}{20}{subsection.A.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Multi-Domain Vignette Prompts}{20}{appendix.B}\protected@file@percent }
\newlabel{app:multi-domain-prompts}{{B}{20}{Multi-Domain Vignette Prompts}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Loan Approval Vignette}{21}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Medical Triage Vignette}{21}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Salary Negotiation Vignette}{21}{subsection.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Judicial Vignette Prompts}{22}{appendix.C}\protected@file@percent }
\newlabel{app:judicial-prompts}{{C}{22}{Judicial Vignette Prompts}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}DUI Repeat Offender}{22}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}First-Time Tax Fraud}{22}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Aggravated Retail Theft (4th Offense)}{23}{subsection.C.3}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\bibcite{binz2023}{{1}{2023}{{Binz and Schulz}}{{}}}
\bibcite{chen2025cognitive}{{2}{2025}{{Chen et~al.}}{{}}}
\bibcite{llm-bayesian-2025}{{3}{2025}{{Chlon et~al.}}{{Chlon, Rashidi, Khamis, and Awada}}}
\bibcite{englich2006}{{4}{2006}{{Englich et~al.}}{{Englich, Mussweiler, and Strack}}}
\bibcite{huang2025anchoring}{{5}{2025}{{Huang et~al.}}{{}}}
\bibcite{jacowitz1995}{{6}{1995}{{Jacowitz and Kahneman}}{{}}}
\bibcite{jones2022}{{7}{2022}{{Jones and Steinhardt}}{{}}}
\bibcite{klein2007}{{8}{2007}{{Klein}}{{}}}
\bibcite{lim2026deframe}{{9}{2026}{{Lim et~al.}}{{}}}
\bibcite{lyu2025}{{10}{2025}{{Lyu et~al.}}{{}}}
\bibcite{sibony2019}{{11}{2019}{{Sibony}}{{}}}
\bibcite{song2026reasoning}{{12}{2026}{{Song et~al.}}{{Song, Han, and Goodman}}}
\bibcite{llm-judge-overconfidence-2025}{{13}{2025}{{Tian et~al.}}{{}}}
\bibcite{tversky1974}{{14}{1974}{{Tversky and Kahneman}}{{}}}
\gdef \@abspage@last{24}
