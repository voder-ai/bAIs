\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Baseline Convergence Metric}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Findings Preview}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Why This Matters}{2}{subsection.1.3}\protected@file@percent }
\citation{tversky1974}
\citation{englich2006}
\citation{binz2023,jones2022,chen2025cognitive}
\citation{huang2025anchoring}
\citation{song2026reasoning}
\citation{sibony2019}
\citation{lyu2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Contributions}{3}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Anchoring Bias in Human Judgment}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cognitive Biases in LLMs}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Debiasing Techniques}{3}{subsection.2.3}\protected@file@percent }
\citation{klein2007}
\citation{lim2026deframe}
\citation{englich2006,huang2025anchoring}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Evaluation Methodology}{4}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evaluation Metrics}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Standard Metric: Anchor Susceptibility}{4}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Our Metric: Baseline Convergence}{4}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Why Both Metrics Matter}{5}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Susceptibility vs.\ Convergence: Rankings diverge. Even excluding confounded Outside View, Devil's Advocate (best susceptibility) and Full SACD (best convergence) show inverted rankings.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:metric-comparison}{{1}{5}{Susceptibility vs.\ Convergence: Rankings diverge. Even excluding confounded Outside View, Devil's Advocate (best susceptibility) and Full SACD (best convergence) show inverted rankings}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Susceptibility vs.\ Convergence metrics diverge. Devil's Advocate appears best under susceptibility ($-8\%$ spread) but worst under convergence ($+2\%$). Full SACD shows the opposite: worst susceptibility ($+36\%$ spread) but best convergence ($+24\%$). The metrics capture different properties---neither is ``correct.''}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:metric-comparison}{{1}{6}{Susceptibility vs.\ Convergence metrics diverge. Devil's Advocate appears best under susceptibility ($-8\%$ spread) but worst under convergence ($+2\%$). Full SACD shows the opposite: worst susceptibility ($+36\%$ spread) but best convergence ($+24\%$). The metrics capture different properties---neither is ``correct.''}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Design}{6}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Models}{6}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Conditions}{6}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Techniques Evaluated}{7}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Temperature Conditions}{7}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Trial Counts and Procedure}{7}{subsubsection.3.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Trial distribution. Total unique trials: 13,799. Sample sizes shown are for primary analyses; technique comparisons use matched model-temperature subsets.}}{8}{table.2}\protected@file@percent }
\newlabel{tab:trial-counts}{{2}{8}{Trial distribution. Total unique trials: 13,799. Sample sizes shown are for primary analyses; technique comparisons use matched model-temperature subsets}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6}Statistical Analysis}{8}{subsubsection.3.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Confounds and Limitations}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Outside View Jurisdiction Context}{8}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline Responses}{8}{subsection.4.1}\protected@file@percent }
\citation{tversky1974}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model baseline variation. Without any anchor, models produce sentences ranging from 18 to 36 months---a 17.7-month spread. This variation motivates per-model anchor calibration.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:baselines}{{2}{9}{Model baseline variation. Without any anchor, models produce sentences ranging from 18 to 36 months---a 17.7-month spread. This variation motivates per-model anchor calibration}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Model baselines range from 18.0mo (Opus) to 35.7mo (o4-mini)---a 17.7mo spread. Opus 4.6 shows zero variance (SD=0.0) at all temperatures, consistently responding with exactly 18 months. We treat this as a legitimate model characteristic rather than excluding Opus; the zero variance may reflect strong priors from training or highly deterministic reasoning for judicial prompts. Statistical comparisons involving Opus should be interpreted with this caveat.}}{9}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-Anchor Responses (No Technique)}{9}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Baseline convergence by debiasing technique. Positive values indicate the technique moves judgments toward the model's unprompted baseline. Full SACD shows strongest convergence (+24\%, $p<.001$), significantly outperforming the Random Control (+9\%), demonstrating that iterative self-critique provides genuine debiasing beyond conversation length effects. Devil's Advocate shows no significant effect ($p=1.0$ after Bonferroni correction).}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:convergence}{{3}{10}{Baseline convergence by debiasing technique. Positive values indicate the technique moves judgments toward the model's unprompted baseline. Full SACD shows strongest convergence (+24\%, $p<.001$), significantly outperforming the Random Control (+9\%), demonstrating that iterative self-critique provides genuine debiasing beyond conversation length effects. Devil's Advocate shows no significant effect ($p=1.0$ after Bonferroni correction)}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Technique Effectiveness: Baseline Convergence}{10}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Technique effectiveness with 95\% confidence intervals and Bonferroni-corrected p-values. Effect sizes are small by Cohen's conventions ($d < 0.5$); statistical significance does not imply practical significance. $^\dagger $Outside View result confounded by required jurisdiction specification; included for transparency but excluded from primary conclusions.}}{10}{table.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Full SACD shows high model variance. Green: significant improvement ($p<.05$). Gray: not significant. Red: significant backfire. Opus 4.6 worsens by 68\%, while o3 improves by 51\%.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:sacd-by-model}{{4}{11}{Full SACD shows high model variance. Green: significant improvement ($p<.05$). Gray: not significant. Red: significant backfire. Opus 4.6 worsens by 68\%, while o3 improves by 51\%}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Model-Specific Results: Full SACD}{11}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Full SACD model-specific results. 5/10 significantly improve, 1/10 significantly worsens (Opus 4.6).}}{11}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Why Baseline Collection Matters}{11}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Why Full SACD Works (and Fails)}{12}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Why Random Control Works}{12}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}The Outside View Confound}{12}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Limitations}{13}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Practical Recommendations}{13}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Prompt Templates}{14}{appendix.A}\protected@file@percent }
\newlabel{app:prompts}{{A}{14}{Prompt Templates}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Case Vignette (All Conditions)}{14}{subsection.A.1}\protected@file@percent }
\citation{lyu2025}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{binz2023}{{1}{2023}{{Binz and Schulz}}{{}}}
\bibcite{chen2025cognitive}{{2}{2025}{{Chen et~al.}}{{}}}
\bibcite{englich2006}{{3}{2006}{{Englich et~al.}}{{Englich, Mussweiler, and Strack}}}
\bibcite{huang2025anchoring}{{4}{2025}{{Huang et~al.}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Anchor Introduction (High/Low Anchor Conditions)}{15}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Outside View (As Implemented)}{15}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Full SACD (Iterative Self-Administered Cognitive Debiasing)}{15}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Random Control}{15}{subsection.A.5}\protected@file@percent }
\bibcite{jones2022}{{5}{2022}{{Jones and Steinhardt}}{{}}}
\bibcite{klein2007}{{6}{2007}{{Klein}}{{}}}
\bibcite{lim2026deframe}{{7}{2026}{{Lim et~al.}}{{}}}
\bibcite{lyu2025}{{8}{2025}{{Lyu et~al.}}{{}}}
\bibcite{sibony2019}{{9}{2019}{{Sibony}}{{}}}
\bibcite{song2026reasoning}{{10}{2026}{{Song et~al.}}{{Song, Han, and Goodman}}}
\bibcite{tversky1974}{{11}{1974}{{Tversky and Kahneman}}{{}}}
\gdef \@abspage@last{16}
