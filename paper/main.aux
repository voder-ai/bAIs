\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{englich2006}
\citation{binz2023,jones2022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{sibony2019}
\citation{lyu2025}
\citation{englich2006}
\citation{tversky1974}
\citation{kahneman1979}
\citation{tversky1981}
\citation{arkes1985}
\citation{binz2023}
\citation{lou2024}
\citation{maynard2025trojan}
\citation{alessa2025}
\citation{sibony2019}
\citation{lyu2025}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cognitive Biases in LLMs}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Human Debiasing Research}{2}{subsection.2.2}\protected@file@percent }
\citation{englich2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}LLM Debiasing Attempts}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Paradigm}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Conditions}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Models and Sample Size}{3}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Per-model sample sizes for cross-model anchoring experiments. ``Valid'' = trials with parseable numeric response. ``Excluded'' = parsing failures after 3 retries. Mistral had high exclusion rate (57\%) due to difficulty following JSON output format; exclusions are scenario-independent.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:sample-sizes}{{1}{4}{Per-model sample sizes for cross-model anchoring experiments. ``Valid'' = trials with parseable numeric response. ``Excluded'' = parsing failures after 3 retries. Mistral had high exclusion rate (57\%) due to difficulty following JSON output format; exclusions are scenario-independent}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Model Identifier Variance: A Methodological Contribution}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:model-id-variance}{{3.4}{4}{Model Identifier Variance: A Methodological Contribution}{subsection.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Cross-generational difference in anchoring bias. Sonnet 4.5 (\texttt  {claude-sonnet-4-5-20250929}) shows 3-month anchoring effect, while Sonnet 4 (\texttt  {claude-sonnet-4-20250514}) shows zero anchoring on identical prompts. These are different model generations, not the same model with different identifiers.}}{4}{table.2}\protected@file@percent }
\newlabel{tab:model-id-variance}{{2}{4}{Cross-generational difference in anchoring bias. Sonnet 4.5 (\texttt {claude-sonnet-4-5-20250929}) shows 3-month anchoring effect, while Sonnet 4 (\texttt {claude-sonnet-4-20250514}) shows zero anchoring on identical prompts. These are different model generations, not the same model with different identifiers}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Temperature and Sampling Protocol}{5}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Verification of deterministic output. Same prompt (high anchor, 9mo) queried 5 times on GPT-4o at temp=0. All outputs identical (SD=0). Variance reported in other tables arises from \emph  {scenario variation}, not model stochasticity.}}{5}{table.3}\protected@file@percent }
\newlabel{tab:determinism-demo}{{3}{5}{Verification of deterministic output. Same prompt (high anchor, 9mo) queried 5 times on GPT-4o at temp=0. All outputs identical (SD=0). Variance reported in other tables arises from \emph {scenario variation}, not model stochasticity}{table.3}{}}
\citation{englich2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Scenario Design and Selection}{6}{subsection.3.6}\protected@file@percent }
\newlabel{sec:scenario-design}{{3.6}{6}{Scenario Design and Selection}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Analysis}{6}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}Variance Source Clarification}{6}{subsubsection.3.7.1}\protected@file@percent }
\citation{englich2006}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}Descriptive Statistics Details}{7}{subsubsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.3}Why We Do Not Report Inferential Statistics}{7}{subsubsection.3.7.3}\protected@file@percent }
\newlabel{sec:no-inferential-stats}{{3.7.3}{7}{Why We Do Not Report Inferential Statistics}{subsubsection.3.7.3}{}}
\citation{englich2006}
\citation{englich2006}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline Anchoring Bias}{8}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Baseline anchoring bias in Codex. Values show mean $\pm $ SD ($n=30$). Observed range is for the \emph  {difference} between conditions across scenario variants. Effect size is very large ($d > 0.8$). For context, \citet  {englich2006} observed 2.05mo in humans on different prompts.}}{8}{table.4}\protected@file@percent }
\newlabel{tab:baseline}{{4}{8}{Baseline anchoring bias in Codex. Values show mean $\pm $ SD ($n=30$). Observed range is for the \emph {difference} between conditions across scenario variants. Effect size is very large ($d > 0.8$). For context, \citet {englich2006} observed 2.05mo in humans on different prompts}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sibony Debiasing Techniques}{8}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Effect of Sibony debiasing techniques on anchoring bias in Codex ($n=30$ per condition). Observed ranges reflect scenario variation. Effect sizes remain large ($d > 2$), indicating substantial residual anchoring even after intervention.}}{8}{table.5}\protected@file@percent }
\newlabel{tab:sibony}{{5}{8}{Effect of Sibony debiasing techniques on anchoring bias in Codex ($n=30$ per condition). Observed ranges reflect scenario variation. Effect sizes remain large ($d > 2$), indicating substantial residual anchoring even after intervention}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}SACD Results}{8}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces SACD results showing elimination of anchoring bias ($n=30$ per condition). Values show mean $\pm $ SD. Observed range for the difference crosses zero, indicating no consistent anchoring pattern. Effect size is negligible ($|d| < 0.2$).}}{8}{table.6}\protected@file@percent }
\newlabel{tab:sacd}{{6}{8}{SACD results showing elimination of anchoring bias ($n=30$ per condition). Values show mean $\pm $ SD. Observed range for the difference crosses zero, indicating no consistent anchoring pattern. Effect size is negligible ($|d| < 0.2$)}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}GPT-4o Debiasing: SACD as the Only Effective Technique}{9}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Debiasing effectiveness on GPT-4o ($n = 137$ valid trials after deduplication\let \reserved@d =[\def \par }}{9}{table.7}\protected@file@percent }
\newlabel{tab:gpt4o-debiasing}{{7}{9}{Debiasing effectiveness on GPT-4o ($n = 137$ valid trials after deduplication\footnote {Deduplication removed duplicate API responses from retry logic. Sample sizes vary (n=25--29 per condition) because some scenarios required retries that produced duplicates, which were removed to ensure each scenario appears once per condition.}). Only SACD achieved measurable reduction. All other techniques showed exactly 0\% reduction---GPT-4o perfectly followed anchors with or without Sibony interventions}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Generic Reflection Control: SACD's Effect is Not Bias-Specific}{9}{subsection.4.5}\protected@file@percent }
\newlabel{sec:generic-reflection}{{4.5}{9}{Generic Reflection Control: SACD's Effect is Not Bias-Specific}{subsection.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Generic reflection control ($n=30$ valid trials). Generic prompts (``Review your answer carefully,'' ``Think step by step'') with the same multi-turn structure as SACD produced \emph  {stronger} debiasing than SACD's psychology-specific content.}}{9}{table.8}\protected@file@percent }
\newlabel{tab:generic-reflection}{{8}{9}{Generic reflection control ($n=30$ valid trials). Generic prompts (``Review your answer carefully,'' ``Think step by step'') with the same multi-turn structure as SACD produced \emph {stronger} debiasing than SACD's psychology-specific content}{table.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Random Elaboration Control: Identifying the Mechanism}{10}{subsubsection.4.5.1}\protected@file@percent }
\newlabel{sec:random-elaboration}{{4.5.1}{10}{Random Elaboration Control: Identifying the Mechanism}{subsubsection.4.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Random elaboration control on two models ($n=30$ each). On \textbf  {unbiased} Llama 3.3, random elaboration produces identical effects to CoT (+6.0mo)---structure alone introduces bias. On \textbf  {biased} GPT-4o, random elaboration achieves only 20\% reduction vs CoT's 66\%---reasoning content provides additional benefit.}}{10}{table.9}\protected@file@percent }
\newlabel{tab:random-elaboration}{{9}{10}{Random elaboration control on two models ($n=30$ each). On \textbf {unbiased} Llama 3.3, random elaboration produces identical effects to CoT (+6.0mo)---structure alone introduces bias. On \textbf {biased} GPT-4o, random elaboration achieves only 20\% reduction vs CoT's 66\%---reasoning content provides additional benefit}{table.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Cross-Model Replication: Generic Reflection is Model-Specific}{10}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Generic reflection across 5 models ($n=30$ per condition). On biased models, generic reflection reduces bias (66--97\%). On \textbf  {both} unbiased models (Sonnet 4 dated, Llama 3.3), it \emph  {introduces} substantial bias (3--6 months).}}{11}{table.10}\protected@file@percent }
\newlabel{tab:generic-reflection-crossmodel}{{10}{11}{Generic reflection across 5 models ($n=30$ per condition). On biased models, generic reflection reduces bias (66--97\%). On \textbf {both} unbiased models (Sonnet 4 dated, Llama 3.3), it \emph {introduces} substantial bias (3--6 months)}{table.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Temperature Sensitivity of Debiasing}{11}{subsubsection.4.5.3}\protected@file@percent }
\newlabel{sec:debiasing-temp}{{4.5.3}{11}{Temperature Sensitivity of Debiasing}{subsubsection.4.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Debiasing at temp=1.0 on GPT-4o ($n=28$ valid trials). Generic reflection achieves \emph  {stronger} debiasing at temp=1.0 (72\%) than temp=0 (66\%). Debiasing effectiveness is not limited to deterministic sampling.}}{11}{table.11}\protected@file@percent }
\newlabel{tab:debiasing-temp}{{11}{11}{Debiasing at temp=1.0 on GPT-4o ($n=28$ valid trials). Generic reflection achieves \emph {stronger} debiasing at temp=1.0 (72\%) than temp=0 (66\%). Debiasing effectiveness is not limited to deterministic sampling}{table.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}SACD Pathological Outputs on Llama 3.3}{11}{subsubsection.4.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces SACD output distribution on Llama 3.3 ($n=15$ per condition). Baseline: all responses = 6mo. SACD produces extreme variance with 120mo outliers (20\% of trials). Notably, the pattern is \textbf  {reversed}: high anchor produces \emph  {more} 0mo responses (6 vs 3), suggesting complete disruption rather than bias introduction. Effect: $-1.26$mo.}}{12}{table.12}\protected@file@percent }
\newlabel{tab:sacd-pathological}{{12}{12}{SACD output distribution on Llama 3.3 ($n=15$ per condition). Baseline: all responses = 6mo. SACD produces extreme variance with 120mo outliers (20\% of trials). Notably, the pattern is \textbf {reversed}: high anchor produces \emph {more} 0mo responses (6 vs 3), suggesting complete disruption rather than bias introduction. Effect: $-1.26$mo}{table.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Cross-Model Validation}{12}{subsection.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Cross-model anchoring bias, sorted by effect magnitude. Eight models tested. \textbf  {Caution:} Single prompt template; results are model-specific observations, not provider-level generalizations.}}{12}{table.13}\protected@file@percent }
\newlabel{tab:crossmodel}{{13}{12}{Cross-model anchoring bias, sorted by effect magnitude. Eight models tested. \textbf {Caution:} Single prompt template; results are model-specific observations, not provider-level generalizations}{table.13}{}}
\citation{sibony2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Knowledge of Bias $\neq  $ Resistance to Bias}{13}{subsection.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Knowledge-behavior dissociation. Both models know about anchoring bias and can predict its effects, yet only Sonnet 4 resists it in practice.}}{13}{table.14}\protected@file@percent }
\newlabel{tab:knowledge-behavior}{{14}{13}{Knowledge-behavior dissociation. Both models know about anchoring bias and can predict its effects, yet only Sonnet 4 resists it in practice}{table.14}{}}
\citation{lim2026}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Complete Sonnet 4.5 Bias Profile}{14}{subsection.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Complete bias profile for Claude Sonnet 4.5 (\texttt  {claude-sonnet-4-5-20250929}) across four cognitive biases ($n=30$ per condition). $^*$Range for Bill scenario only (Linda showed 0\% errors). $^\dagger $Range for gain-frame certain choice; loss-frame shows 50\% [33\%, 67\%] choosing risky option. \textbf  {Note:} Anchoring result differs for dated identifier (0.0mo).}}{14}{table.15}\protected@file@percent }
\newlabel{tab:profile}{{15}{14}{Complete bias profile for Claude Sonnet 4.5 (\texttt {claude-sonnet-4-5-20250929}) across four cognitive biases ($n=30$ per condition). $^*$Range for Bill scenario only (Linda showed 0\% errors). $^\dagger $Range for gain-frame certain choice; loss-frame shows 50\% [33\%, 67\%] choosing risky option. \textbf {Note:} Anchoring result differs for dated identifier (0.0mo)}{table.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}DeFrame Substantially Reduces Framing Effect}{14}{subsection.4.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces DeFrame reduces framing effect bias ($n=30$ per condition). Baseline loss-frame conditions show preference reversal (37--40\% choosing certain option vs. 97\% in gain frame). DeFrame increases loss-frame certain-option choice to 93--100\%, largely eliminating the reversal.}}{14}{table.16}\protected@file@percent }
\newlabel{tab:deframe}{{16}{14}{DeFrame reduces framing effect bias ($n=30$ per condition). Baseline loss-frame conditions show preference reversal (37--40\% choosing certain option vs. 97\% in gain frame). DeFrame increases loss-frame certain-option choice to 93--100\%, largely eliminating the reversal}{table.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Preliminary Hypothesis: Soft vs Hard Bias Patterns}{14}{subsection.5.1}\protected@file@percent }
\newlabel{sec:soft-hard}{{5.1}{14}{Preliminary Hypothesis: Soft vs Hard Bias Patterns}{subsection.5.1}{}}
\citation{sibony2019}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Temperature sensitivity across 5 models ($n=30$--60 per temperature). HARD models show constant bias regardless of temperature. SOFT models show low bias across all temperatures. Llama 3.3 shows 0.0mo at all temperatures tested.}}{15}{table.17}\protected@file@percent }
\newlabel{tab:soft-hard}{{17}{15}{Temperature sensitivity across 5 models ($n=30$--60 per temperature). HARD models show constant bias regardless of temperature. SOFT models show low bias across all temperatures. Llama 3.3 shows 0.0mo at all temperatures tested}{table.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Deterministic Bias: A Novel Observation}{16}{subsection.5.2}\protected@file@percent }
\newlabel{sec:deterministic-bias}{{5.2}{16}{Deterministic Bias: A Novel Observation}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Anchoring Bias is Prompt-Sensitive (Sonnet 4 Alias)}{17}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}GPT-4o Prompt Robustness}{17}{subsection.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Prompt robustness testing for GPT-4o ($n=30$ per condition). Unlike Sonnet 4 (92\% reduction from paraphrasing), GPT-4o shows only 25\% reduction---anchoring persists across prompt styles.}}{17}{table.18}\protected@file@percent }
\newlabel{tab:gpt4o-robustness}{{18}{17}{Prompt robustness testing for GPT-4o ($n=30$ per condition). Unlike Sonnet 4 (92\% reduction from paraphrasing), GPT-4o shows only 25\% reduction---anchoring persists across prompt styles}{table.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Novel Anchoring Scenarios Show Consistent Bias}{17}{subsection.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Anchoring effects across classic and novel scenarios ($n=30$ per condition). ``Sonnet 4.5'' refers to \texttt  {claude-sonnet-4-5}. Percentages show effect size relative to classic scenario baseline. All 8 scenarios (4 novel + classic with variations) showed measurable anchoring in both models, though magnitude varied substantially by scenario content.}}{18}{table.19}\protected@file@percent }
\newlabel{tab:novel-anchoring}{{19}{18}{Anchoring effects across classic and novel scenarios ($n=30$ per condition). ``Sonnet 4.5'' refers to \texttt {claude-sonnet-4-5}. Percentages show effect size relative to classic scenario baseline. All 8 scenarios (4 novel + classic with variations) showed measurable anchoring in both models, though magnitude varied substantially by scenario content}{table.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Human Techniques Partially Transfer (Model-Dependent)}{18}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Iterative Self-Correction Was Effective in Our Tests}{18}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Preliminary Hypothesis: Two Patterns Observed in Our Tested Models}{19}{subsection.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Limitations}{19}{subsection.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}Future Work}{22}{subsection.5.10}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{23}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Details}{24}{appendix.A}\protected@file@percent }
\newlabel{app:reproducibility}{{A}{24}{Reproducibility Details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Experiment Provenance}{24}{subsection.A.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces Experiment provenance for reproducibility. Model ID is the exact identifier used in API calls. Commit refers to the bAIs repository version.}}{24}{table.20}\protected@file@percent }
\newlabel{tab:provenance}{{20}{24}{Experiment provenance for reproducibility. Model ID is the exact identifier used in API calls. Commit refers to the bAIs repository version}{table.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Sampling Settings}{24}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Anchoring Experiment Prompt}{24}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Context Hygiene Prompt Addition}{25}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Premortem Prompt Addition}{25}{subsection.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}DeFrame Intervention}{25}{subsection.A.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Framing Effect Prompts}{25}{subsection.A.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Conjunction Fallacy Prompts}{26}{subsection.A.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.9}Sunk Cost Fallacy Prompts}{27}{subsection.A.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.10}Output Parsing and Retry Logic}{28}{subsection.A.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.11}Code Availability}{28}{subsection.A.11}\protected@file@percent }
\gdef \@abspage@last{28}
