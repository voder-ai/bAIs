% Add after "Reasoning Models Do Not Escape Bias" subsection

\subsection{Intuitive Debiasing Strategies Do Not Transfer}

Human debiasing research assumes interventions that engage deliberative reasoning (Kahneman's ``System 2'') will reduce bias. Sibony-style disclosure---explicitly labeling anchors as arbitrary---works for human judges because it triggers conscious discounting of irrelevant information \citep{sibony2016}.

This logic transfers imperfectly to LLMs. Our disclosure experiments reveal three distinct response patterns: RLHF-trained models (Anthropic, Hermes) show strong positive effects (+35--97.5\%), compliance-optimized models (GPT-4o, o3-mini) show null effects, and reasoning-optimized models (o1, GPT-5.2) show \emph{inverse} effects ($-$14\% to $-$28\%). The models most designed to ``think carefully''---o1's extended reasoning, GPT-5.2's deliberative chains---perform worst under disclosure.

This parallels recent findings that specialized AI personas degrade economic rationality despite appearing more rigorous \citep{chen2025specialization}. The common pattern: interventions that \emph{look like} deliberation may introduce systematic errors rather than correct them. LLMs generate tokens that resemble careful reasoning without the underlying cognitive mechanism that makes such reasoning debiasing in humans.

\textbf{Practical implication:} Do not assume human debiasing techniques transfer to LLMs. Test disclosure, SACD, and reasoning interventions on your specific deployment---what works for one model family may backfire on another.
