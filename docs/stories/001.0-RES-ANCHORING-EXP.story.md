# 001.0-RES-ANCHORING-EXP: Prosecutor Sentencing Recommendation Anchoring Experiment

## Theme Goal

**Theme**: Core Validation (Essential MVP)

Enable researchers to validate anchoring bias exists in one LLM model with basic statistical output, and automatically generate a human-readable “paper” from the results, using a judicial sentencing paradigm where a prosecutor’s recommended sentence serves as the anchor.

This implementation follows the “irrelevant anchor” variant where the prosecutor’s recommendation is determined by a die roll, so the anchor has no evidentiary connection to the case.

This is the first capability in the "Design Experiment" journey phase, providing the experimental framework that all subsequent Core Validation stories depend on.

## How This Story Contributes

This story is critical as the foundation of the entire toolkit - it defines the anchoring bias experiment that serves as the reference implementation for all future bias experiments. Without this experiment definition, researchers cannot validate whether LLMs exhibit human-like cognitive biases.

This enables:

- Story 003.0 (Configure conditions with high/low anchors)
- Story 005.0 (Parse numeric estimates from responses)
- Story 006.0 (Expand statistical analysis and reporting)
- The complete Core Validation theme MVP

It also provides immediate MVP value by running the experiment end-to-end, computing basic statistics, and generating a human-readable report suitable for quick review and iteration.

The anchoring experiment serves as the template and proof-of-concept that guides the implementation of other cognitive bias experiments in later themes.

## User Story

**Format**: So that I can validate LLMs exhibit human-like bias, as a Bias Researcher, I want to implement the anchoring experiment where a prosecutor’s recommended sentence anchors sentencing recommendations.

**INVEST Criteria Compliance**:

- **Independent**: Can be developed independently - defines experiment structure without requiring other stories
- **Negotiable**: Exact prompt wording and question structure can be refined during implementation
- **Valuable**: Delivers core research value - enables validation of cognitive bias in LLMs
- **Estimable**: Scope is clear - implement the two-question anchoring experiment structure
- **Small**: Not strictly small (it includes run + analysis + report generation), but is still a single coherent “vertical slice” of MVP researcher value
- **Testable**: Acceptance criteria provide clear verification of experiment structure and prompts

## Requirements and Acceptance Criteria

- **REQ-EXP-STRUCTURE**: Experiment implements a two-step sentencing anchoring format with an irrelevant die-roll anchor
  - [ ] **Step 1 (Irrelevant Anchor + Comparison)**: Presents a case vignette and (per condition) states that a die was rolled with a fixed outcome (low: 1; high: 6) to determine the prosecutor’s recommended sentence (anchor), then asks whether an appropriate sentence is higher/lower than that recommendation
  - [ ] **Step 2 (Estimate)**: Asks for the model’s sentencing recommendation as a numeric value in months

- **REQ-EXP-PROMPT**: Experiment prompts replicate the “irrelevant anchor” prosecutor-recommendation methodology
  - [ ] **Anchor Prompt**: Prompt states the die outcome and derived prosecutor recommendation (explicitly described as arbitrary/irrelevant) and asks for a higher/lower judgment relative to that number (it must not ask the model to roll a new die)
  - [ ] **Estimation Prompt**: Prompt requests a numeric sentencing recommendation in months
  - [ ] **Constant Case Facts**: The case vignette is identical across trials to isolate the anchor effect

- **REQ-EXP-VALIDATION**: Experiment produces statistically analyzable results
  - [ ] **Numeric Response**: Experiment expects and validates numeric sentence length responses (months)
  - [ ] **Condition Comparison**: Experiment structure enables comparison between low-anchor and high-anchor prosecutor recommendation conditions
  - [ ] **Statistical Output**: Experiment output supports computing a difference-in-means test (e.g., t-test) and effect size (it does not require a particular p-value)

- **REQ-EXP-RUNNER**: Researcher can run the experiment end-to-end via Codex CLI
  - [ ] **CLI Command**: Running `bais run anchoring-prosecutor-sentencing --runs N` executes N trials per condition (total trials = conditions × N)
  - [ ] **Codex Integration**: Each trial calls `codex exec --output-last-message …` (via `npx codex exec`) and prompts the model to return JSON only
  - [ ] **Incremental Output**: Results are written incrementally (JSONL to stdout or `--out results.jsonl`) as each trial completes
  - [ ] **Strict Schema Validation**: Each result must be JSON-only and contain exactly `diceRoll`, `prosecutorRecommendationMonths`, `higherOrLower`, and `sentenceMonths` (integer months in range)
  - [ ] **Consistency Validation**: `prosecutorRecommendationMonths` must be consistent with the die roll (e.g., `prosecutorRecommendationMonths = diceRoll * 10`)
  - [ ] **Condition Consistency**: For a given condition, the model’s returned `diceRoll` and `prosecutorRecommendationMonths` must match the values stated in that condition’s prompt
  - [ ] **Resilience**: If a trial returns invalid JSON, the runner retries and records an `error` field instead of aborting the full run

- **REQ-EXP-ANALYSIS**: Tool computes basic statistical analysis for the experiment run
  - [ ] **Trial Counts (Per Condition)**: Computes ok/error trial counts per condition
  - [ ] **Outcome Summaries (Per Condition)**: Computes $n$, mean, median, standard deviation, and standard error of `sentenceMonths` per condition (excluding errored trials)
  - [ ] **Distributions (Per Condition)**: Includes raw per-trial values and five-number summary (min, $Q_1$, median, $Q_3$, max) for `sentenceMonths` per condition
  - [ ] **Effect + Test (Direct Comparison)**: Compares the pre-specified low vs high conditions directly and computes a difference in means (high − low), a significance test (e.g., Welch’s t-test), and an effect size (e.g., Cohen’s $d$ or Hedges’ $g$)
  - [ ] **Confidence Interval**: Computes a 95% confidence interval for the mean difference (high − low), using a documented method (e.g., bootstrap percentile CI), and records method parameters (e.g., iterations and RNG seed) in the analysis JSON
  - [ ] **Reproducible Artifact**: Emits a deterministic machine-readable analysis summary (JSON) that includes the computed statistics and the software version/config used (model name, runs per condition, retry settings)

- **REQ-EXP-PAPER**: Tool uses an LLM to generate a human-readable report from the run + analysis
  - [ ] **LLM Inputs**: The prompt provided to the LLM includes (a) experiment metadata (ID/name/anchor derivation), (b) the fixed case vignette, (c) run configuration (model, trial count, retry rules), and (d) the computed analysis summary JSON
  - [ ] **Prompt Instructions**: The prompt instructs the LLM to write a concise, human-readable paper in Markdown with sections: Title, Abstract, Methods, Results (with the computed stats), Discussion, Limitations, and Conclusion
  - [ ] **Grounding Constraint**: The prompt instructs the LLM not to invent numbers, and to base all quantitative claims strictly on the provided analysis summary
  - [ ] **Output Artifact**: The report is written to a file (e.g., `--report report.md`) or printed to stdout in a clearly delimited block
  - [ ] **Resilience**: If report generation fails, the run and analysis artifacts are still produced (the overall command does not lose the collected data)

- **REQ-EXP-METADATA**: Experiment includes necessary metadata for execution
  - [ ] **Experiment ID**: Unique identifier for sentencing anchoring experiment
  - [ ] **Experiment Name**: Human-readable name ("Anchoring Bias - Prosecutor Sentencing Recommendation")
  - [ ] **Conditions**: Defines two conditions with fixed die outcomes (low: die=1; high: die=6) and how the prosecutor recommendation is derived from the die roll
  - [ ] **Expected Response Type**: Numeric months (integer)

## Dependencies

None - this is the foundational story for the Core Validation theme.

## Implementation Notes

**Historical Context**:

- Anchoring has been demonstrated in legal decision-making contexts where a prosecutor’s sentencing demand/recommendation shifts judges’ sentencing decisions
- The “irrelevant anchor” variant uses a random value (e.g., from a die roll) to set the prosecutor’s recommendation, making the anchor normatively irrelevant to the case
- Participants (or here, LLMs) then provide their own sentencing recommendation, which can be analyzed as a function of the anchor

Reference study to reproduce:

- Englich, B., Mussweiler, T., & Strack, F. (2006). _Playing dice with criminal sentences: The influence of irrelevant anchors on experts’ judicial decision making._ **Personality and Social Psychology Bulletin, 32**(2), 188–200. https://doi.org/10.1177/0146167205282152 (also available via ResearchGate: https://www.researchgate.net/publication/7389517_Playing_Dice_With_Criminal_Sentences_The_Influence_of_Irrelevant_Anchors_on_Experts'_Judicial_Decision_Making)

**Experiment Design Considerations**:

- Keep the case vignette constant across conditions to isolate anchoring
- Ensure the output is a single numeric sentence length (months) for reliable analysis
- Consider guardrails to avoid moralizing/lecturing responses that omit the numeric answer
- Structure should be reusable template for other anchoring-style experiments

**Technical Considerations**:

- Experiment definition should be data structure (JSON/TypeScript type)
- Prompts should support variable substitution for anchor values
- Design for extensibility - other experiments will follow similar pattern
- Consider validation of prompt structure during experiment creation

**Related Decisions**:

- [001: TypeScript with ESM](../decisions/001-use-typescript-with-esm.proposed.md) - Type safety for experiment definitions
- [009: TypeScript Strict Mode](../decisions/009-use-typescript-strict-mode.proposed.md) - Strict typing for experiment parameters
- [011: Codex CLI LLM Interaction](../decisions/011-use-codex-cli-for-llm-interaction.proposed.md) - Structured, schema-driven LLM responses for reliable numeric parsing
