# 001.0-RES-ANCHORING-EXP: Prosecutor Sentencing Recommendation Anchoring Experiment

## Theme Goal

**Theme**: Core Validation (Essential MVP)

Enable researchers to validate anchoring bias exists in one LLM model with basic statistical output, and automatically generate a human-readable “paper” from the results, using a judicial sentencing paradigm where a prosecutor’s recommended sentence serves as the anchor.

This implementation replicates Study 2 from Englich et al. (2006), where the prosecutor's recommendation is randomly determined (not based on judicial expertise), so the anchor has no evidentiary connection to the case. The null hypothesis we test is that the LLM exhibits the same anchoring bias as human legal professionals with 95% confidence.

This is the first capability in the "Design Experiment" journey phase, providing the experimental framework that all subsequent Core Validation stories depend on.

## How This Story Contributes

This story is critical as the foundation of the entire toolkit - it defines the anchoring bias experiment that serves as the reference implementation for all future bias experiments. Without this experiment definition, researchers cannot validate whether LLMs exhibit human-like cognitive biases.

This enables:

- Story 003.0 (Configure conditions with high/low anchors)
- Story 005.0 (Parse numeric estimates from responses)
- Story 006.0 (Expand statistical analysis and reporting)
- The complete Core Validation theme MVP

It also provides immediate MVP value by running the experiment end-to-end, computing basic statistics, and generating a human-readable report suitable for quick review and iteration.

The anchoring experiment serves as the template and proof-of-concept that guides the implementation of other cognitive bias experiments in later themes.

## User Story

**Format**: So that I can validate LLMs exhibit human-like bias, as a Bias Researcher, I want to implement the anchoring experiment where a prosecutor’s recommended sentence anchors sentencing recommendations.

**INVEST Criteria Compliance**:

- **Independent**: Can be developed independently - defines experiment structure without requiring other stories
- **Negotiable**: Exact prompt wording and question structure can be refined during implementation
- **Valuable**: Delivers core research value - enables validation of cognitive bias in LLMs
- **Estimable**: Scope is clear - implement the two-question anchoring experiment structure
- **Small**: Not strictly small (it includes run + analysis + report generation), but is still a single coherent “vertical slice” of MVP researcher value
- **Testable**: Acceptance criteria provide clear verification of experiment structure and prompts

## Requirements and Acceptance Criteria

- **REQ-EXP-STRUCTURE**: Experiment implements a three-step sentencing anchoring format matching Study 2 from Englich et al. (2006)
  - [ ] **Step 1 (Prosecutor Evaluation)**: Presents a shoplifting case vignette (Lena M., 12th offense) and states that the prosecutor's sentencing demand was randomly determined (low: 3 months; high: 9 months on probation), then asks whether that demand is too low, too high, or just right
  - [ ] **Step 2 (Defense Attorney Evaluation)**: Presents the defense attorney's demand (1 month on probation) and asks whether that demand is too low, too high, or just right
  - [ ] **Step 3 (Final Sentence)**: Asks for the judge's final sentencing decision as a numeric value in months on probation

- **REQ-EXP-PROMPT**: Experiment prompts replicate Study 2 methodology (randomly determined prosecutor demand)
  - [ ] **Anchor Prompt**: Prompt states that the prosecutor's demand was "randomly determined" and "does not reflect any judicial expertise," then asks for an evaluation (too low / too high / just right)
  - [ ] **Estimation Prompt**: Prompt presents defense attorney demand (1 month) and requests a final sentencing decision in months on probation
  - [ ] **Constant Case Facts**: The case vignette (shoplifting, Lena M., 12th offense) is identical across trials to isolate the anchor effect

- **REQ-EXP-VALIDATION**: Experiment produces statistically analyzable results matching Study 2 data characteristics
  - [ ] **Numeric Response**: Experiment expects and validates numeric sentence length responses (0-12 months on probation)
  - [ ] **Condition Comparison**: Experiment structure enables comparison between low-anchor (3 months) and high-anchor (9 months) prosecutor recommendation conditions
  - [ ] **Statistical Output**: Experiment output supports computing a difference-in-means test (e.g., Welch's t-test) and effect size to test whether LLM exhibits anchoring bias similar to human legal professionals in Study 2 (low: M=4.00, high: M=6.05, p<.05)

- **REQ-EXP-RUNNER**: Researcher can run the experiment end-to-end via Codex CLI
  - [ ] **CLI Command**: Running `bais run anchoring-prosecutor-sentencing --runs N` executes N trials per condition (total trials = conditions × N)
  - [ ] **Codex Integration**: Each trial calls `codex exec --output-last-message …` (via `npx codex exec`) and prompts the model to return JSON only
  - [ ] **Incremental Output**: Results are written incrementally (JSONL to stdout or `--out results.jsonl`) as each trial completes
  - [ ] **Strict Schema Validation**: Each result must be JSON-only and contain exactly `prosecutorRecommendationMonths`, `prosecutorEvaluation` (too low/too high/just right), `defenseAttorneyEvaluation` (too low/too high/just right), and `sentenceMonths` (integer 0-12)
  - [ ] **Condition Consistency**: For a given condition, the model's returned `prosecutorRecommendationMonths` must match the value stated in that condition's prompt
  - [ ] **Resilience**: If a trial returns invalid JSON, the runner retries and records an `error` field instead of aborting the full run

- **REQ-EXP-ANALYSIS**: Tool computes basic statistical analysis for the experiment run
  - [ ] **Trial Counts (Per Condition)**: Computes ok/error trial counts per condition
  - [ ] **Outcome Summaries (Per Condition)**: Computes $n$, mean, median, standard deviation, and standard error of `sentenceMonths` per condition (excluding errored trials)
  - [ ] **Distributions (Per Condition)**: Includes raw per-trial values and five-number summary (min, $Q_1$, median, $Q_3$, max) for `sentenceMonths` per condition
  - [ ] **Effect + Test (Direct Comparison)**: Compares the pre-specified low vs high conditions directly and computes a difference in means (high − low), a significance test (e.g., Welch’s t-test), and an effect size (e.g., Cohen’s $d$ or Hedges’ $g$)
  - [ ] **Confidence Interval**: Computes a 95% confidence interval for the mean difference (high − low), using a documented method (e.g., bootstrap percentile CI), and records method parameters (e.g., iterations and RNG seed) in the analysis JSON
  - [ ] **Reproducible Artifact**: Emits a deterministic machine-readable analysis summary (JSON) that includes the computed statistics and the software version/config used (model name, runs per condition, retry settings)

- **REQ-EXP-PAPER**: Tool uses an LLM to generate a human-readable report from the run + analysis
  - [ ] **LLM Inputs**: The prompt provided to the LLM includes (a) experiment metadata (ID/name/anchor derivation), (b) the fixed case vignette, (c) run configuration (model, trial count, retry rules), and (d) the computed analysis summary JSON (including human baseline comparison)
  - [ ] **Prompt Instructions**: The prompt instructs the LLM to write a concise, human-readable paper in Markdown with sections: Title, Abstract, Methods, Results (with the computed stats), Discussion (MUST include comparison to human baseline from Study 2 and cite source), Limitations, Conclusion, and References
  - [ ] **Human Baseline Discussion**: The Discussion section MUST compare LLM results to the Study 2 human baseline (low: 4.00 months, high: 6.05 months, diff: 2.05 months), state whether LLM bias is LESS, SIMILAR, or GREATER than human bias, interpret this finding relative to the null hypothesis, and cite Englich et al. (2006)
  - [ ] **References Section**: The report MUST include a References section with the full citation: Englich, B., Mussweiler, T., & Strack, F. (2006). Playing dice with criminal sentences: The influence of irrelevant anchors on experts' judicial decision making. Personality and Social Psychology Bulletin, 32(2), 188–200. https://doi.org/10.1177/0146167205282152
  - [ ] **Grounding Constraint**: The prompt instructs the LLM not to invent numbers, and to base all quantitative claims strictly on the provided analysis summary
  - [ ] **Output Artifact**: The report is written to a file (e.g., `--report report.md`) or printed to stdout in a clearly delimited block
  - [ ] **Resilience**: If report generation fails, the run and analysis artifacts are still produced (the overall command does not lose the collected data)

- **REQ-EXP-METADATA**: Experiment includes necessary metadata for execution
  - [ ] **Experiment ID**: Unique identifier for sentencing anchoring experiment
  - [ ] **Experiment Name**: Human-readable name ("Anchoring Bias - Prosecutor Sentencing Recommendation")
  - [ ] **Conditions**: Defines two conditions with randomly determined prosecutor demands (low: 3 months; high: 9 months on probation)
  - [ ] **Expected Response Type**: Numeric months on probation (integer, 0-12 range matching Study 2 data)

## Dependencies

None - this is the foundational story for the Core Validation theme.

## Implementation Notes

**Historical Context**:

- Anchoring has been demonstrated in legal decision-making contexts where a prosecutor’s sentencing demand/recommendation shifts judges’ sentencing decisions
- Study 2 from Englich et al. (2006) used shoplifting case (Lena M., 12th offense) with randomly determined prosecutor demands (3 vs 9 months on probation)
- Human legal professionals (N=39, mean 13 years experience) showed significant anchoring: low anchor M=4.00 months, high anchor M=6.05 months, t(37)=2.10, p<.05
- Participants (or here, LLMs) provide their own sentencing recommendation, which can be analyzed to test whether LLMs exhibit the same bias
- Null hypothesis: LLM sentencing decisions are influenced by irrelevant anchors in the same manner as human legal professionals

Reference study to reproduce:

- Englich, B., Mussweiler, T., & Strack, F. (2006). _Playing dice with criminal sentences: The influence of irrelevant anchors on experts’ judicial decision making._ **Personality and Social Psychology Bulletin, 32**(2), 188–200. https://doi.org/10.1177/0146167205282152 (also available via ResearchGate: https://www.researchgate.net/publication/7389517_Playing_Dice_With_Criminal_Sentences_The_Influence_of_Irrelevant_Anchors_on_Experts'_Judicial_Decision_Making)

**Experiment Design Considerations**:

- Case vignette: shoplifting (Lena M., 12th offense) held constant across conditions
- Prosecutor demands randomly determined (3 vs 9 months on probation)
- Defense attorney demand constant at 1 month on probation
- Ensure output is single numeric sentence length (months on probation) for reliable analysis
- Consider guardrails to avoid moralizing/lecturing responses that omit the numeric answer
- Structure should be reusable template for other anchoring-style experiments

**Known Limitations**:

- **Incomplete Case Materials**: The case vignette used in this implementation is a high-level summary based on the published description in Englich et al. (2006). The original study likely included more detailed case materials (witness statements, psycho-legal expert reports, etc.) that are not available in the published paper. This limits the exact replicability of the case context, though the core anchoring manipulation (randomly determined prosecutor demand) remains intact.
- **Participant Qualifications**: The original study used 39 experienced legal professionals (27 judges, 2 prosecutors, and others with mean 13 years judicial experience). LLMs have no formal legal qualifications, no professional experience, and no real-world judicial decision-making context. This fundamental difference means LLM responses cannot be directly equated to human legal professional judgments, though anchoring bias patterns can still be compared.

**Technical Considerations**:

- Experiment definition should be data structure (JSON/TypeScript type)
- Prompts should support variable substitution for anchor values
- Design for extensibility - other experiments will follow similar pattern
- Consider validation of prompt structure during experiment creation

**Related Decisions**:

- [001: TypeScript with ESM](../decisions/001-use-typescript-with-esm.proposed.md) - Type safety for experiment definitions
- [009: TypeScript Strict Mode](../decisions/009-use-typescript-strict-mode.proposed.md) - Strict typing for experiment parameters
- [011: Codex CLI LLM Interaction](../decisions/011-use-codex-cli-for-llm-interaction.proposed.md) - Structured, schema-driven LLM responses for reliable numeric parsing
